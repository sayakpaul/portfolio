{
  
    
        "post0": {
            "title": "A Battle of Text Detectors for Mobile Deployments: CRAFT vs. EAST",
            "content": "In the previous post, we saw how to convert the pre-trained CRAFT model from PyTorch to TensorFlow Lite (TFLite) and run inference with the converted TFLite model. In this post, we will be comparing the TFLite variants of the CRAFT model to another text detection model - EAST. The objective of this post is to provide a comparative study between these two models with respect to various deployment-specific pointers such as inference latency, model size, performance on dense text regions, and so on. Text detection continues to be a very important use-case across many verticals. So we hope this post will serve as a systematic guide for developers that are interested to explore on-device text detection models. . Precisely, we will be comparing the two models on the basis of the following pointers which we think are very crucial when it comes to deploying them out in the wild - . Visual Inspection of Performance | Model Size | Inference Latency | Memory Usage | . . Important: If you are interested to know about the conversion process and inference pipelines of the models, please refer to these notebooks - CRAFT and EAST. The pre-converted models are available on TensorFlow Hub - CRAFT and EAST. . Benchmark Setup . We used the TensorFlow Lite Benchmark tool in order to gather results on inference latency and memory usage of the models with Redmi K20 Pro as the target device. We chose a mobile device for this purpose because text detection is a pretty prevalent recipe of many mobile applications such as Google Lens. . In order to make the comparisons fair, we consider the two models with three different image resolutions - 320x320, 640x416, and 1200x800. For each of these resolutions, we consider two different post-training quantization schemes - dynamic-range and float16. The CRAFT model conversion is not yet supported in the integer variant, hence we do not consider integer quantization (but the EAST model does support it). . Visual Inspection of Performance . In this setting, we run both of the models and their different variants (dynamic-range and float16 quantized) on a sample image that has dense text regions, and then we visualize the results. We observed that both of these models perform fairly well on images having lighter text regions. Here‚Äôs the sample image we used for the purpose - . . Image is taken from the SROIE dataset. Time to detect some texts! . CRAFT - 320x320 Dynamic-Range &amp; float16 . In the dynamic-range quantization setting, we can see the model misses out on some text blocks. . . Inference results from the 320x320 dynamic-range and float16 quantized CRAFT models. With increased numerical precision i.e. float16, we can clearly see quite a bit of improvement in the results. It‚Äôs important to note that this improvement comes at the cost of increased model size. . Next up, we apply the same steps to the EAST model. . EAST - 320x320 Dynamic-Range &amp; float16 . EAST apparently performs better than CRAFT under dynamic-range quantization. If we look closely, it appears that the CRAFT model produces far fewer overlaps in the detections compared to EAST. On the other hand, the EAST model is able to detect more text blocks. When developing practical applications with text detectors, it often becomes a classic case of precision-recall trade-offs like the one we are currently seeing. So, you would want to consider the application-specific needs in order to decide the level of trade-off to be achieved there. . . Inference results from the 320x320 dynamic-range and float16 quantized EAST models. With increased precision, the above-mentioned points still hold, i.e. the number of overlaps being way higher for the EAST model than they are in the CRAFT equivalent. In this setting (float16 quantization), superiority in the performance of the CRAFT model is quite evident in regards to the EAST model. . As different applications may use different image resolutions we decided to test the performance of the models on larger dimensions as well. This is what we are going to see next. . CRAFT - 640x416 Dynamic-Range &amp; float16 . On an increased resolution, the CRAFT model performs pretty well - . . Inference results from the 640x416 dynamic-range and float16 quantized CRAFT models. The float16 version of this resolution is a slam dunk (rightfully leaving behind the barcode which is not a piece of text). . EAST - 640x416 Dynamic-Range &amp; float16 . The performance of the EAST model under these settings are very equivalent to CRAFT - . . Inference results from the 640x416 dynamic-range and float16 quantized EAST models. With float16 quantization and 640x416 as the resolution, the CRAFT model is a clear winner. Notice that the EAST model is still unable to discard the barcode part which might be an important point to note for some applications. . Time to inspect the results for our final and highest resolution - 1280x800. . CRAFT - 1280x800 Dynamic-Range &amp; float16 . Under dynamic-range quantization, the results look okayish. The model misses out on a number of text blocks but the only ones that it detects appear to be neat. . . Inference results from the 1280x800 dynamic-range and float16 quantized CRAFT models. The results from the float16 variant are tremendous (as you probably have guessed by now). . EAST - 1280x800 Dynamic-Range &amp; float16 . At this resolution, the EAST model seems to be performing well too - . . Inference results from the 1280x800 dynamic-range and float16 quantized EAST models. With float16 quantization as well, the CRAFT model beats EAST in terms of the detection quality. . Model Size . When it comes to deploying models to mobile devices model size becomes a really important factor. You may not want to have a heavy model that would, in turn, make your mobile application bulky. Moreover, Playstore and AppStore also have size restrictions on the applications one can host there. . On the other hand, heavier models tend to be slower. If your application cannot have increased inference latency then you would want to have the model size as low as possible. . The following figure shows the size of the CRAFT and EAST models - . . Model (TFLite variants) sizes of CRAFT and EAST. The dynamic-range quantized versions of both the models are in a well-acceptable range with respect to size. However, the float16 variants may still be a bit heavier for some applications. . Inference Latency . Inference latency is also one of the major factors for mobile-based deployments especially when your applications might require instantaneous predictions. We are going to show a comparison between all the settings we considered in the visual inspection section. . To reiterate we performed the benchmarks for this section on a Redmi K20 Pro using 4 threads. In the following figures, we present inference latency of different variants of the CRAFT and EAST models. . . Inference latency of different variants of the CRAFT model. . Inference latency of different variants of the EAST model. As expected, with increased resolution the inference latency also increases. Inference latency is also quite lower for all the variants of the EAST model compared to CRAFT. Earlier we saw how a quantization affects model performance under a particular resolution. As stated earlier, when using these models inside a mobile application, the ‚ÄúSize vs. Performance‚Äù trade-off becomes extremely vital. . Important: The results for the float16 1280x800 CRAFT model could not be obtained on our target device. . Memory Usage . In section, we shed light on the total memory allocated for the models while running the TensorFlow Lite Benchmark tool. Knowing about the memory usage of these models helps us plan application releases accordingly as not all the mobile phones may support extensive memory requirements. So based on this information, you may want to set some device requirements for your application using these models. On the other hand, if you would want your application to be as device-agnostic as possible then you may want to maintain separate models according to their size and memory usage. . In this case, also, we are going to consider all the settings we had considered in the previous sections. The following figures give us a sense of the memory footprint left behind by the models - . . Memory footprint of different variants of the CRAFT model. . Memory footprint of different variants of the EAST model. Detection performance-wise, CRAFT was a winner in many cases but if we factor in for inference latency and memory footprint the situation might need reconsideration. In other words, the best performing (with respect to a certain task, detection in this case) model may not always be the best candidate for deployments. . Important: The results for the float16 1280x800 CRAFT model could not be obtained on our target device. . Conclusion . In this post, we presented a comparative study between two text detection models - CRAFT and EAST. We went beyond their task-specific performance and considered various essential factors that one needs to consider when deploying these models. At this point, you might have felt the need to consider another important factor of these models - FPS information of the models on real-time videos. Well, a mobile application is already in the works in order to actually run these models in a real setting as well as to gather the FPS information. We will update this blog post as soon as the results are available. . Contribution . Tulasi worked on the CRAFT model while Sayak worked on the EAST model. For the purpose of this post, Tulasi focused on gathering all the relevant information for doing the comparisons while Sayak focused on the writing part. . Thanks to Khanh LeViet from the TFLite team for reviewing the post. .",
            "url": "https://sayak.dev/optimizing-text-detectors/",
            "relUrl": "/optimizing-text-detectors/",
            "date": " ‚Ä¢ Nov 27, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Optimizing MobileDet for Mobile Deployments",
            "content": "This year researchers from the University of Wisconsin-Madison and Google published their work on MobileDet. MobileDet presents an architectural philosophy for designing object detectors specifically targeted toward running on mobile accelerators like DSP, EdgeTPU, and so on. MobileDet yields significant improvement over architectures MobileNetV2+SSDLite and MobileNetV3+SSDLite on the COCO object detection task with the same accelerated inference time. Long story cut short, if you are planning to use object detection models in mobile applications MobileDets may be an extremely good choice. . One fantastic thing about modern-day research is most of the time, the code and essential artifacts (like the trained models) are available publicly. MobileDet is no exception; the authors released their code and pre-trained models in TensorFlow Object Detection (TFOD) API. The model files come in three different variants - . Optimized for mobile CPU | Optimized for EdgeTPU | Optimized for DSP | . Each of these variants includes the pre-trained checkpoints, a TensorFlow Lite (TFLite) compatible model graph, a TFLite model file, a configuration file, and a graph proto. The models were pre-trained on the COCO dataset. . In this post, I am going to be revisiting the TFLite conversion from the pre-trained model checkpoints along with some of the non-trivial things that come up during the process. It is basically an extension of Khanh LeViet and my findings we shared over this GitHub thread. . The code discussed throughout this post is available here as a Colab Notebook. . Important: If you want to train MobileDet models on your own dataset you may find these notebooks useful. They show you how to prepare the dataset, fine-tune a MobileDet model with the dataset, and optimize the fine-tuned model with TFLite. . Why yet another post on model conversion? . Fair question. After all, there are so many great examples and tutorials that show how to use the post-training quantization APIs in TFLite to perform the model conversion. MobileDet models in the TFOD API repository were trained in TensorFlow (TF) 1. If you ever wanted to use the latest TFLite converter to do the conversion, that may not be immediately approachable. . Besides, there are certain caveats to the EdgeTPU and DSP variants. They come in two precision formats - uint8 and float32. The models in uint8 precision were trained using quantization aware training (QAT) while the float32 models were not. During QAT fake quantization nodes get inserted into a model‚Äôs computation graph. So, the models trained using QAT usually require some extra care during the TFLite conversion process as we&#39;ll see in a moment. . If we wanted to convert a single shot detector (SSD) based model to TFLite then we first need to generate a frozen graph first that is compatible with the TFLite operator set (as per these guides - TF1 and TF2). The TFOD API team provides stock scripts (TF1, TF2) for this. Both of these scripts add optimized postprocessing operations to the model graph. Now, these operations are not yet supported in int8 precision. So, if you ever wanted to convert these pre-trained checkpoints using full integer quantization, what would have been your approach? . By now, hopefully, I have been able to convince you that this post is not just about regular model conversion in TFLite. The situations we&#39;ll be going through over the next sections may be helpful for your production TFLite models as well. . The hassle-free conversions . Before we build our way toward the fun stuff, let‚Äôs start with the conversions that won‚Äôt cost us a night‚Äôs sleep. Conversions based on dynamic-range and float16 quantization would come under this category. . Important: The EdgeTPU and DSP variants of MobileDet are meant to run on the respective hardware accelerators. These accelerators need a model to be in full integer precision. So converting the EdgeTPU and DSP variants with dynamic-range and float16 quantization does not have any practical usage. . So, for dynamic-range and float16 quantization based conversions, we will be using the CPU variant only. This variant is available here as ssd_mobiledet_cpu_coco. Once the model bundle is untar‚Äôd we get the following files - . ‚îú‚îÄ‚îÄ model.ckpt-400000.data-00000-of-00001 ‚îú‚îÄ‚îÄ model.ckpt-400000.index ‚îú‚îÄ‚îÄ model.ckpt-400000.meta ‚îú‚îÄ‚îÄ model.tflite ‚îú‚îÄ‚îÄ pipeline.config ‚îú‚îÄ‚îÄ tflite_graph.pb ‚îî‚îÄ‚îÄ tflite_graph.pbtxt . model.ckpt-* files are the pre-trained checkpoints on the COCO dataset. If you train a MobileDet object detection model on your own dataset, you will have your own model checkpoint files. The tflite_graph.pb file is a frozen inference graph that is compatible with the TFLite operator set, which was exported from the pre-trained model checkpoints. model.tflite file is a TFLite model that was converted from the tflite_graph.pb frozen graph. . In case if you ever train a MobileDet model on your dataset, here‚Äôs how you‚Äôd get the TFLite frozen graph file (based on this guide mentioned above) - . $ PIPELINE_CONFIG=&quot;checkpoint_name/pipeline.config&quot; $ CKPT_PREFIX=&quot;checkpoint_name/model.ckpt-400000&quot; $ OUTPUT_DIR=&quot;tflite_graph&quot; $ python models/research/object_detection/export_tflite_ssd_graph.py --pipeline_config_path=$PIPELINE_CONFIG --trained_checkpoint_prefix=$CKPT_PREFIX --output_directory=$OUTPUT_DIR --add_postprocessing_op=true . You can see a fully worked out example in the Colab Notebook mentioned above. If everything goes well, then you should have the frozen graph file exported in OUTPUT_DIR. Let‚Äôs now proceed to the TFLite model conversion part. . Here‚Äôs how the dynamic-range quantization would look like in TensorFlow 2 - . converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph( graph_def_file=model_to_be_quantized, input_arrays=[&#39;normalized_input_image_tensor&#39;], output_arrays=[&#39;TFLite_Detection_PostProcess&#39;, &#39;TFLite_Detection_PostProcess:1&#39;, &#39;TFLite_Detection_PostProcess:2&#39;, &#39;TFLite_Detection_PostProcess:3&#39;], input_shapes={&#39;normalized_input_image_tensor&#39;: [1, 320, 320, 3]} ) converter.optimizations = [tf.lite.Optimize.DEFAULT] tflite_model = converter.convert() . A note about some of the parameters and their values from the above code listing - . model_to_be_quantized corresponds to the frozen graph file. | input_arrays and input_shapes are set accordingly with respect to the frozen graph file. As we can see in the figure below that these values have been set correctly. . . | output_arrays is set according to the instructions provided in this guide. Those operations represent four arrays: detection_boxes, detection_classes, detection_scores, and num_detections, usually a mandate for any object detector out there. | . The rest of the parts in the code listing should be familiar to you if you already know about the typical post-training quantization process in TFLite. For float16 quantization, all the things would remain the same; we just need to add this line before calling convert() - converter.target_spec.supported_types = [tf.float16]. . The dynamic-range quantized model is 4.3 MB in size and float16 one is 8.2 MB. Later, we will see how fast this model would run on actual mobile devices with and without different accelerators. . The trickier TFLite conversions for MobileDet . In this section, we will be dealing with the full integer quantization for the three different variants of MobileDet. Full integer quantization is usually more involved than the other quantization formats supported by TFLite. . Representative dataset . Our first step toward doing full integer quantization is preparing a representative dataset. It is required to calibrate the activation ranges so that the quantized model is able to retain the original model performance as much as possible. For the purpose of this post, I sampled 100 images from the COCO training dataset (train2014 split). In my experience, 100 samples as the representative dataset have always been sufficient. I have hosted these images here in case you are interested to use them. . The following code listing denotes a generator function that produces a preprocessed image to the TFLite converter - . rep_ds = tf.data.Dataset.list_files(&quot;train_samples/*.jpg&quot;) HEIGHT, WIDTH = 320, 320 def representative_dataset_gen(): for image_path in rep_ds: img = tf.io.read_file(image_path) img = tf.io.decode_image(img, channels=3) img = tf.image.convert_image_dtype(img, tf.float32) resized_img = tf.image.resize(img, (HEIGHT, WIDTH)) resized_img = resized_img[tf.newaxis, :] yield [resized_img] . Note that these preprocessing steps should be in sync with the actual preprocessing steps that would apply before running inference with your TFLite model. In case if you are interested to know about more complex representative dataset generators you may find this notebook useful. . Also, note that dynamic-range and float16 quantization of the EdgeTPU and DSP variants don‚Äôt have much of practical usage. The next section is going to be solely about full integer quantization of these different variants and the nitty-gritty to take into consideration for the conversion process. . Dealing with fake quantization nodes during conversion . The figure below represents a portion of the uint8 EdgeTPU model computation graph. The nodes highlighted in red are inserted by the QAT mechanism. You would notice the same kind of nodes in the uint8 DSP model computation graph as well. . . Now, these nodes have some important implications that we need to consider during the conversion process - . During QAT the activation ranges are already approximated i.e. QAT resembles post-training quantization during training and adjusts the activation ranges accordingly. So, we don‚Äôt need to provide a representative dataset for a full integer quantization based conversion. | These fake nodes are generally in integer precision. So, setting an optimization option (converter.optimizations) might lead to inconsistencies. | In order to convert the uint8 models with full integer quantization, we need to set the input and output data type of the TFLite models to integer precision (typically uint8 or int8). As per this documentation, we also need to specify the quantized_input_stats parameter during the conversion process. This is needed in order for the converted TFLite model to map the quantized input values to real values. More details are available here. | . So, how do we realize all of these in code? . converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph( graph_def_file=model_to_be_quantized, input_arrays=[&#39;normalized_input_image_tensor&#39;], output_arrays=[&#39;TFLite_Detection_PostProcess&#39;, &#39;TFLite_Detection_PostProcess:1&#39;, &#39;TFLite_Detection_PostProcess:2&#39;, &#39;TFLite_Detection_PostProcess:3&#39;], input_shapes={&#39;normalized_input_image_tensor&#39;: [1, 320, 320, 3]} ) converter.inference_input_type = tf.uint8 converter.quantized_input_stats = {&quot;normalized_input_image_tensor&quot;: (128, 128)} tflite_model = converter.convert() . If you‚Äôre thinking this does not look all that gory compared to the above code listing - tt does not have to be! The tooling should help you do these things seamlessly. But catching these details during your project development may not be trivial. Note that we don‚Äôt specify converter.inference_output_type. Hold your breath, we will come to this in a moment. . After successful execution, we get two full integer quantized models - EdgeTPU one is 4.2 MB and the DSP one is 7.0 MB. . Integer quantization for CPU variants and float32 precision models . The variants that don‚Äôt contain fake quantization nodes (CPU and all the models in float32 precision) have a relatively simpler conversion process. Recollect that the EdgeTPU and DSP variants come in two different precisions - uint8 and float32. For example, here‚Äôs how it would be for the float32 precision models - . converter.representative_dataset = representative_dataset_gen converter.inference_input_type = tf.uint8 converter.optimizations = [tf.lite.Optimize.DEFAULT] . Note that we are specifying a representative dataset here because the float32 precision models weren‚Äôt trained using QAT. For the CPU variant model, the lines of code would slightly change - . converter.inference_input_type = tf.uint8 converter.quantized_input_stats = {&quot;normalized_input_image_tensor&quot;: (128, 128)} converter.optimizations = [tf.lite.Optimize.DEFAULT] . Honestly, I found this configuration by trial and error. I observed that if I specify a representative dataset then it hurts the predictions of the converted model. Also, I found out that specifying converter.quantized_input_stats helped improve the predictions of the converted model. . We don‚Äôt specify converter.inference_output_type in this case as well. Let‚Äôs get to it now. . Dealing with non-integer postprocessing ops during conversion . Remember that frozen graph exporter scripts provided by the TFOD API team add optimized postprocessing operations to the graph. These operations are not supported in integer precision yet. So, even if you wanted to specify converter.inference_output_type as tf.uint8 you‚Äôll likely get the following error - . RuntimeError: Unsupported output type UINT8 for output tensor &#39;TFLite_Detection_PostProcess&#39; of type FLOAT32. . This is why we did not set the converter.inference_output_type parameter. . This should resolve all the problems you may run into if you ever wanted to convert the MobileDet models offered by the TFOD API team. In the last two sections, we&#39;ll see these converted models in action and how fast they can perform on respective hardware accelerators. . Show me some results . For the CPU variant model, its float16 quantized TFLite provided decent results - . . On Colab, the inference time is about 92.36 ms for this particular model. I experimented with different threshold values for filtering out the weak predictions and a threshold of 0.3 yielded the best results. These results are pretty consistent across the several different models we talked about. . A major point to note here for the EdgeTPU and DSP variants, their converted counterparts would be much slower on Colab since they were specifically optimized for different hardware accelerators. . You are encouraged to play with the different converted models using the Colab Notebook mentioned above and see these results for yourself. . Model benchmarks . In this section, we‚Äôll address the question - ‚ÄúSo, how do I choose one among these many models?‚Äù Well, you could manually try them all out and see which performs the best on the runtime of your choice. But a more practical approach to this would be to first benchmark these models on a set of devices using the TFLite Benchmark Tool and then decide accordingly. . The following table provides a comprehensive summary of the important statistics about the runtime of different TFLite MobileDet models. These results were generated using the TFLite Benchmark Tool mentioned above. . . * Device used - Pixel 4 (Inference timings are reported in milliseconds) ** As reported here We can see that with the proper hardware accelerators, the DSP EdgeTPU variants can really shine. For the CPU variant, on a GPU accelerated runtime the float16 quantized TFLite model can bring in additional speed boosts. . A catch here is Pixel devices don‚Äôt allow third-party applications to use the Hexagon DSP therefore even if we instruct the Benchmark Tool to make use of that the model would fall back to the CPU for execution. This is why for fair benchmarking results for the DSP variants we should consider running the Benchmark Tool on a device (such as Samsung Galaxy S9+) that has Hexagon DSP and also allows third-party applications to use it. . . * Device used - Samsung Galaxy S9+ (Inference timings are reported in milliseconds) . Note: To train a custom MobileDet-based object detector you can refer to these notebooks. . Conclusion . In this post, we discussed some of the intricate problems one may run into while converting different variants of the MobileDet model in TFLite. One aspect about TFLite that I really like is how it provides the tooling needed to deal with practical problems like this. . I am thankful to Khanh for thoroughly guiding me while writing this post. Thanks to Martin Andrews for suggesting textual edits. .",
            "url": "https://sayak.dev/mobiledet-optimization/",
            "relUrl": "/mobiledet-optimization/",
            "date": " ‚Ä¢ Sep 29, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "The Maker Philosophy with ML APIs",
            "content": "In this post, I discuss how I used several Google Cloud Platform (GCP) APIs to turn two ideas into small prototypes. It includes my thought process, the problems I ran into while developing the prototypes, and my approach toward tackling them. All the code discussed in the post is available in this repository. . As a Machine Learning (ML) Practitioner, I advocate for having an understanding of the underlying principles of the models and other stuff that I use. This understanding has many extents. Sometimes, it involves minimally implementing models, and sometimes it may not involve the from-scratch implementation. When it does not involve the implementation part and when the model is readily available, I prefer to put such models directly to use and get a sense of their broader capabilities. . With libraries like TensorFlow, PyTorch, and Scikit-Learn, realizing this usage has never been easier. As all of these libraries are open-source, you could easily get access to the low-level primitives of their model APIs whenever you‚Äôd like. It may require you to have a sufficient amount of experience with the library you‚Äôd use. But as a Machine Learning Practitioner, one cannot skip this practice. It‚Äôs important to have a good grip over a particular Machine Learning library given the domain of choice (structured tabular dataset, images, texts, audios, for example). . On the other hand, APIs that offer ML as a service, allow non-ML folks to incorporate the power of Machine Learning in their applications very easily. This way developers can prototype ideas faster than ever. Some would argue that leaky abstractions can hit sooner than expected and it can be particularly very miserable in Machine Learning. Nonetheless, if you are more on the applied side of things and don‚Äôt want to worry about this aspect, that‚Äôs perfectly fine. . I wanted to revisit this idea through the lens of an ML Practitioner. More precisely, I wanted to build a series of short demos utilizing the Cloud ML APIs offered by Google Cloud Platform. The premise here is if I have an idea for an ML project, I wanted to see how quickly I can develop a PoC around it. . The ideation phase . Let me quote Emil Wallner from this interview - . It‚Äôs important to collect objective evidence that you can apply machine learning. . With regard to successful ML practice, this statement couldn‚Äôt have been more appropriate. Machine Learning has affected almost every industry in some way, it has changed the way we develop and perceive software. Coming up with an ML application idea that‚Äôs not already there or implemented is actually pretty hard. . So, I ideated the prototypes drawing inspiration from what is already available. For example, Dale and Kaz of Google built this uber-cool demo that lets you transform a PDF into an audiobook. I really wanted to build something similar but in a more minimal capacity -- something that could solely run on a Colab Notebook. I decided to revisit some of the GCP ML APIs that I already knew, Vision, Text-to-Speech APIs, for example. As someone that is already working in the field of Computer Vision, I was inclined to do something that involves it. So here are some initial ideas that came to mind after spending a considerable amount of time with the different API documentation available on GCP: . A pipeline that takes a short video clip, detects the entities present in the video and generates an audio clip dictating detected entity labels. This allowed me to spend some time with GCP‚Äôs Video Intelligence API. . | A pipeline that takes an arXiv paper and generates an audio clip of the paper abstract. This was inspired by the demo that Dale and Kaz had already built. . | . Note that if you are already experienced with the Vision and Text-to-Speech APIs then these may seem very trivial. . The mental model . After these ideas, I designed a bunch of visual workflows demonstrating the steps required to realize these ideas along with the right tooling. Here‚Äôs an example - . . I also like to refer to these workflows as mental models. Additionally, it helps me to figure out the major dependencies and steps that may be required for the work so that I can plan accordingly. I discuss the importance of developing mental models in this blog post. . (You might have noticed that the above model is a bit different from the first initial idea - I added a logo detection block in there as well.) . Here is another workflow I developed for the second idea I mentioned above: . . This is slightly different from the initial idea I had. In fact, it does not even incorporate anything related to the Vision API. If I only wanted to deal with arXiv papers, I thought using the arXiv API (I used the arXiv Python library) would be a far more reasonable option here since it already provides important information about an arXiv paper such as its categories, abstract, last updated date, and so on. . Finally, I wanted to combine the Vision and Text-to-Speech APIs for the second idea I had. In their demos, Dale and Kaz used AutoML Tables to train a model capable of classifying a paragraph of text into the following categories - ‚Äúbody&quot;, &quot;header&quot;, &quot;caption&quot; and &quot;others&quot;. But I wanted to see if I can bypass this additional training step to filter out the abstract block of a paper and perform optical character recognition (OCR) locally. So, I came up with the following workflow - . . As we can see I am using two Python libraries additionally - . pdf2image - as the name suggests, it is for converting a PDF file to PNG. . | pytesseract - this is for performing OCR locally on an image. . | . In the next sections, I&#39;ll discuss the problems I faced while implementing these workflows in code, and how I went about approaching the solutions. . Building a short video descriptor . In the following texts, we will go over the main ingredients that turned out to be important while developing the prototypes. This will include some code along with the motivation to justify their inclusion. . For the first two workflows, it was mostly about reading the documentation carefully and figuring out the right APIs to use. GCP provides first-class documentation for these APIs with bindings available in many different languages as you can see in the figure below - . . I repurposed these code snippets for the workflows. The Python binding of the Video Intelligence API is simple to use - . You first instantiate the client and instruct what all you are interested in performing - . video_client = videointelligence.VideoIntelligenceServiceClient() features = [videointelligence.enums.Feature.LABEL_DETECTION] . It provides a bag of different features like entity detection, logo recognition, text recognition, object tracking, and so on. Here I am only interested in performing entity detection on a per-segment basis. A user usually specifies segments if they are interested to only analyze a part of their videos. I didn‚Äôt specify any segments, and in that case, the Video Intelligence API handles the entire video as a segment. The API also allows you to perform label detection on more granular levels, i.e. on both shot and frame levels. . After the initialization, it was only a matter of a few keystrokes till I made my first video annotation request - . # Specify the mode in which label detection is to be performed mode = videointelligence.enums.LabelDetectionMode.SHOT_AND_FRAME_MODE config = videointelligence.types.LabelDetectionConfig(label_detection_mode=mode) context = videointelligence.types.VideoContext(label_detection_config=config) # Make the request operation = video_client.annotate_video( input_uri=gcs_path, features=features, video_context=context) . Here I am supplying a GCS bucket path of the video I wanted to infer on. Processing the results of the operation is also straightforward - . # Process video/segment level label annotations # Get the first response, since we sent only one video. segment_labels = operation.result.annotation_results[0].segment_label_annotations video_labels = [] for (i, segment_label) in enumerate(segment_labels): print(&quot;Video label description: {}&quot;.format(segment_label.entity.description)) video_labels.append(segment_label.entity.description) . After I got the entity labels on the entire video the next task was to use the Text-to-Speech API to generate an audio clip. For that, I simply followed the official tutorial and reused the code. . The logo detection pipeline is almost similar with some very minor changes. In case you want to catch all the details please follow this Colab Notebook. . I tested the entire workflow on the following video and you can see the outputs right below it - . Processing video for label annotations: Finished processing. Video label description: sidewalk Video label description: street Video label description: public space Video label description: pedestrian Processing video for logo detection: Finished processing. . As for the audio clip, it got came out pretty nice - . Speed-wise the entire pipeline executed pretty quickly. . I had some previous experience working with videos, so I was able to get an idea of what was going under the hood for the video-related activities but for speech, I plan to get to that probably in the next summer (?) . A potential extension of this demo could be developed to aid blind people to navigate their ways when they are outside. I developed this demo keeping this mind, hence you won&#39;t see any visual results. . Detecting, cropping, and reading an arXiv summary . I presented with two different workflows for the second idea i.e. get the abstract of an arXiv paper and generate an audio clip of it. The workflow involving the arxiv Python library wasn‚Äôt problematic at all, so I am not going to discuss it in detail. You can always check out this fully worked out Colab Notebook in case you are interested. . The other workflow is a bit more involved. In there, I wanted to take an arXiv paper in PDF format, use the Vision API to get blocks of texts from it, and then locate the abstract from there like so - . . But that‚Äôs not it. I also wanted to perform OCR locally on the text blocks. This essentially allowed me to reduce the number of calls to the Vision API and thereby saving me some $. The final piece of the puzzle was to take the local OCR results and generate an audio clip. If you saw the Text-to-Speech documentation, you probably noticed that it is really not a big deal. . So, to realize this workflow here‚Äôs what I did (Colab Notebook) - . As I am only interested in dealing with the abstract of a paper, I first converted the entire PDF-formatted paper to PNG and serialized only the first page. I used the pdf2png library for this. | Next, I used the Vision API to make a document_text_detection() request for getting the dense text blocks. The code for this is again, very straightforward - . client = vision.ImageAnnotatorClient() bounds = [] with io.open(image_file, &#39;rb&#39;) as image_file: content = image_file.read() image = types.Image(content=content) response = client.document_text_detection(image=image) document = response.full_text_annotation # Segregate the blocks for page in document.pages: for block in page.blocks: bounds.append(block.bounding_box) . | Then I used the example presented here to draw the bounding boxes on the input image which we saw earlier. I also reused these bounding boxes to segregate different blocks as inferred by the Vision API. . | I am not going to get into the gory details of how I did the segregation. The catch here is for dense text block detection, Vision API returns polygon coordinates and not rectangular coordinates. So, I had to take polygon crops to segregate the different text blocks. (Thanks to this StackOverflow thread.) . | After the segregation part, I used pytesseract to perform OCR on the segregated text blocks. In pytesseract it‚Äôs literally doable with text = pytesseract.image_to_string(image_block). | Now, an abstract cannot be just a single character (if the OCR was performed correctly). So I only considered those OCR‚Äôd texts where the character length is greater than 1000. | Even with this kind of thresholding, you‚Äôd end up with multiple text blocks where this criterion holds. To counter this, I first sorted the OCR‚Äôd text blocks with respect to their character lengths and checked if a text block contained only one or no reference to citations. If this criterion was matched then the text block is returned as the abstract. . Here‚Äôs how I coded it up: . texts_sorted = sorted(texts, key=len) for text in texts_sorted: if text.split()[0].isupper() &amp; text.count(&quot;[&quot;) &lt;= 1: abstract = text . The upper case criterion is there to ensure an abstract always starts with an uppercase letter. . I am aware that these handcrafted rules can get broken for many instances. But I wanted to explore this possibility anyway. . | To make sure the Text-to-Speech API does not account for any citation I filtered out the raw text to escape them - raw_lines = re.sub(&quot;[[ s* d* ,*]*]&quot;, &quot;&quot;, raw_lines). . | . And that‚Äôs it! After a number of trial and error rounds, I was able to get a decent output. . . Final thoughts . Throughout this post, we went over two different ideas that are good prototype candidates for Machine Learning. We saw how easy it is to see these ideas in actions with different ML APIs. We saw how to make these different APIs work together to solve a given problem. Now, if you are feeling excited enough, you can dive deeper into the different ML tasks we saw: detection and classification, for example. Also note that even if one is using these APIs, it‚Äôs important to be able to process the API responses properly for the project at hand. . I would like to leave you with this amazing resource provided by GCP. It includes detailed solution walkthroughs of real-world problem scenarios across a wide range of different industry verticals. They also show how to make the best use of different GCP services. . I would like to thank Karl Weinmeister for reviewing this post and for sharing his valuable feedback. Also, thanks to the GDE program for providing the GCP credit support which made these demos possible. .",
            "url": "https://sayak.dev/mlapis-maker/",
            "relUrl": "/mlapis-maker/",
            "date": " ‚Ä¢ Sep 25, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "A few favorite recipes in computer vision & deep learning",
            "content": "A few days ago from the time of writing this blog post I tweeted - Some recent favorite recipes (#CV &amp; #DL):üëâHave loads of labeled data? Try improving your image classifier with Supervised Contrastive Learning. üëâDon&#39;t have loads but loads of unlabeled data? Try SimCLRv2.üëâJust want to fine-tune? Try BigTransfer. 1/3 . &mdash; Sayak Paul (@RisingSayak) July 22, 2020 . In this blog post, I will expand on this tweet to convey why these are my favorite recipes among other things. . The training frameworks I mentioned can be classified into two broad categories - . supervised learning (Supervised Contrastive Learning [1] and BigTransfer [2]) | self-supervised learning (SimCLRv2 [only the SimCLR part]). | . . Note: that SimCLR [3] and SimCLRv2 [4] are two separate works. . So, why self-supervised learning anyway? . The field of self-supervised visual representation learning is progressing pretty fast. With recent advancements, the deep learning community has started to consider it as an alternative to the fully supervised models for tasks like image classification, object detection, image segmentation, etc. If you are unfamiliar with self-supervised learning check out this blog post by Jeremy Howard. So, why all the fuss around self-supervised visual representation learning? . This is because a self-supervised learning framework can benefit from unlabeled data. Essentially, you would frame a supervised learning task from a large unlabeled corpus and then train a model to learn that task. You see we are not using any explicit label information here. Instead, we are using the given data to form a supervised learning task, this is why it is self-supervised. You would then take the representations from the model (preferably from the encoder part of the model) and use them for downstream tasks. Representations learned using self-supervised learning frameworks like SimCLRv2, SwAV [5] transfer quite well to downstream (vision) tasks even with very less labeled data. . Colin Raffel beautifully summed up the recent progress in the field of self-supervised learning for computer vision - I just made this figure for a class I am teaching on &quot;learning from limited labeled data&quot;. The left plot represents 6 years of results; the right plot is ~1 year. Anyone else feel like our field is moving kinda fast? pic.twitter.com/Kneb96JU12 . &mdash; Colin Raffel (@colinraffel) July 31, 2020 . BYOL [6] and SwAV have even beaten SimCLR - . . One could argue that this is in comparison with SimCLR but not SimCLRv2. Well, that is because SimCLRv2 is not just about self-supervised learning, it is more than that - . . This blog post, however, would only focus on the SimCLR part i.e. the left-most part of the figure above. If you are interested to know more about SimCLRv2, feel free to watch this video by Yannic Kilcher. . Why SimCLR(v2)? . Among all of the techniques that have been mentioned in the post so far, SimCLR is by far the most favorite of mine. This is because of its simplicity and the promise to shine more as one would scale up the base architecture and introduce more (unlabeled data). Moreover, framework wise SimCLR is very consistent with the recipes I would want to see in an effective visual representation learning framework. One of these primary recipes is to maximize the agreement between the semantically similar images. SwAV is also capable of doing this, in fact, it is currently the state-of-the-art (as of August 2020) in this domain. But simplicity wise SimCLR beats SwAV big time. . At a very high-level, SimCLR takes two different views of the same image and tries to maximize the agreement between these two views while minimizing the agreement between the views coming from other images. These different views are obtained by applying augmentation operations like random-resized crops, horizontal flips, color distortions, etc. Representations learned using this framework (and any self-supervised visual representation learning framework in general) can be used in different flavors - . You may have loads of unlabeled data and limited labeled data for your problem. You could use the unlabeled data and incorporate SimCLR for obtaining effective representations and use them to a downstream task where the limited labeled data might be required. | Representations obtained from datasets like ImageNet using SimCLR can be used in regular transfer learning settings. | . . As we can see in SimCLR, the loss function (normalized temperature-scaled cross-entropy loss) operates directly on the features computed by the projection head (MLP part). This makes SimCLR a compute-intensive framework. . On the other hand, SwAV operates by assigning the encoded representations of different views of the same image to clusters. The clusters are being assigned by keeping a differentiable codebook for the prototypes of the different types of images present in the given dataset. Training wise, SwAV tries to maximize the agreement between the clusters of semantically similar images. Operating on the clusters rather than the encoded representations is a lesser compute-intensive task. . . SwAV might appear as a simpler framework than SimCLR but there are a number of different pieces to look after here: . Maintaining a differentiable prototype bank | Optimal transportation of the representations to form soft codes using the Sinkhorn-Knopp algorithm | Multi-crop data augmentation policy | Swapped prediction problem | . When working in practical scenarios we often need to maintain a trade-off between technical feasibility and performance. To me, SimCLR cuts it through in terms of technical feasibility. . Update: Along with Ayush and generous amount of help from Mathilde Caron (first author of SwAV) we were finally able to minimally implement SwAV after realizing the improvements it brings to the table. . Returning to supervised learning . Given the almightly prowess of the self-supervised learning frameworks why even bother about supervised regimes? . Labels + contrastive loss = win-win . Supervised Contrastive Learning addresses a very important point about the self-supervised learning frameworks like SimCLR. In SimCLR, the positive pairs are generated by taking different views of the same image and the negative pairs are then randomly sampled from the other images present in a batch. . . Here are some examples of negative pairs - . . As discussed in the Supervised Contrastive Learning paper, this method of contrasting two different views of the same image can result in false negatives i.e. the samples belonging to the same class might get mapped differently in the embedding space. There&#39;s no way for us to properly mitigate this issue without having access to the original labels. Hence, I mentioned if you have loads of labeled images, it&#39;s better to use Supervised Contrastive Learning to capture meaningful representations. . . Supervised Contrastive Learning extends how we train supervised models by introducing a two stage training framework. In the first stage, it uses the label information in the contrastive loss to learn to map the encoded representations effectively. In the second stage, it train a linear model on top of these encoded representations for the given supervised training objective. . In practice this works quite well - . . You might be feeling very tempted to try out this on your labeled dataset. Truth be told - Supervised Contrastive Learning is also compute-intensive even for relatively small datasets. So, if you don&#39;t have the training budget you might need to reconsider this option. . Maybe you have a relatively smaller labeled dataset and you just want to be able to fine-tune a good enough architecture which is still very useful for many practical scenarios. This is where BigTransfer can really shine. . Returning to supervised transfer learning . BigTransfer is from the family of classic supervised pre-training and transfer the learned representations to downstream tasks. ImageNet has been the choice for this kind of pre-training for a long time. But in BigTransfer, the authors use larger datasets such as ImageNet-21k, JFT along with ImageNet. In order to facilitate these larger datasets they scale up the model architectures along with longer pre-training. Their result speaks for itself - . . To eliminate the dependence on batch statistics the authors make use of Group Normalization and Weight Standardization. Personally, I really liked this recipe because using overall large batch sizes to train larger models at scale is a common choice and using Batch Normalization there could have easily affected the performance of the models during the downstream tasks. . For fine-tuning, the authors propose a heuristics-based BiT-HyperRule which provides instructions on what augmentation policy to use, how many steps to train for, what learning rate schedule to use, etc. . . I found this strategy to be simple enough to be implemented and practised. . Resources . If you are interested to apply these techniques in your own works you may find the following resources to be helpful - . A Colab Notebook by the authors of SimCLRv2 that shows how to fine-tune with SimCLRv2. | A report by Sweta Shaw and myself that walks through Supervised Contrastive Learning along with Colab Notebooks. | A tutorial on BigTransfer by the authors of BigTransfer. | . Here is a list of some other interesting works on transfer learning for computer vision - Here&#39;s a list of my favorite recent papers on transfer learning for vision:- BigTransfer: https://t.co/saYoJM3A8e- VirTex: https://t.co/R59KohXwec- SimCLRv2: https://t.co/6Hcws2v5dx- Self-training: https://t.co/ufLvXD4kqlWould love to see a T5-like paper for vision. . &mdash; Sayak Paul (@RisingSayak) July 20, 2020 . Acknowledgments . Amit&#39;s visual guide on SimCLR and Thalles&#39;s exploratory analysis on SimCLR helped me in developing a thorough understanding of SimCLR. . Thanks to Yannic Kilcher for his explanation videos on SimCLRv2 and Supervised Contrastive Learning. Those made the learning process smoother. . Thanks to Ting Chen (SimCLR author) for providing me with additional pointers on self-supervised learning in general. . Thanks to Jeremy Howard for his fast.ai lectures that continue to help me in approaching deep learning recipes with more common sense and practicality. . References . Khosla, Prannay, et al. ‚ÄúSupervised Contrastive Learning.‚Äù ArXiv:2004.11362 [Cs, Stat], Apr. 2020. arXiv.org, http://arxiv.org/abs/2004.11362. | Kolesnikov, Alexander, et al. ‚ÄúBig Transfer (BiT): General Visual Representation Learning.‚Äù ArXiv:1912.11370 [Cs], May 2020. arXiv.org, http://arxiv.org/abs/1912.11370. | Chen, Ting, Simon Kornblith, Mohammad Norouzi, et al. ‚ÄúA Simple Framework for Contrastive Learning of Visual Representations.‚Äù ArXiv:2002.05709 [Cs, Stat], June 2020. arXiv.org, http://arxiv.org/abs/2002.05709. | Chen, Ting, Simon Kornblith, Kevin Swersky, et al. ‚ÄúBig Self-Supervised Models Are Strong Semi-Supervised Learners.‚Äù ArXiv:2006.10029 [Cs, Stat], June 2020. arXiv.org, http://arxiv.org/abs/2006.10029. | Caron, Mathilde, et al. ‚ÄúUnsupervised Learning of Visual Features by Contrasting Cluster Assignments.‚Äù ArXiv:2006.09882 [Cs], July 2020. arXiv.org, http://arxiv.org/abs/2006.09882. | Grill, Jean-Bastien, et al. ‚ÄúBootstrap Your Own Latent: A New Approach to Self-Supervised Learning.‚Äù ArXiv:2006.07733 [Cs, Stat], June 2020. arXiv.org, http://arxiv.org/abs/2006.07733. | ‚ÄúAdvancing Self-Supervised and Semi-Supervised Learning with SimCLR.‚Äù Google AI Blog, http://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html. | Facebookresearch/Swav. 2020. Facebook Research, 2020. GitHub, https://github.com/facebookresearch/swav. | Exploring SimCLR: A Simple Framework for Contrastive Learning of Visual Representations - Thalles‚Äô Blog. https://sthalles.github.io/simple-self-supervised-learning/. | BigTransfer (BiT): State-of-the-Art Transfer Learning for Computer Vision. https://blog.tensorflow.org/2020/05/bigtransfer-bit-state-of-art-transfer-learning-computer-vision.html. |",
            "url": "https://sayak.dev/visual-representation-learning/self-supervised-learning/computer-vision/2020/08/02/favorite-recipes-vision.html",
            "relUrl": "/visual-representation-learning/self-supervised-learning/computer-vision/2020/08/02/favorite-recipes-vision.html",
            "date": " ‚Ä¢ Aug 2, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Using TensorRT for accelerated deep learning inference",
            "content": "If you see the way deep learning research has progressed over the years, it has always been guided by the need of the hour. If I were to develop a chronology out of it, it would be something like - train better model -&gt; train them faster -&gt; get them good at generalizing well, and so on. With a stern increase in the demand for using deep learning more as just another technology stack, there could not have been a better time to think about how do we make our models infer faster. In this post, we are going to see how to use TensorRT to perform accelerated inference with TensorFlow (2) models. After all, making predictions with deep learning models is what makes you real üí∞ and we would want to make sure that our bucks burned judiciously. . Performing inference with a pre-trained image classification model . Let‚Äôs first do the groundwork. We will be using a pre-trained (on ImageNet) MobileNetV2 model throughout the post. For this section, we will be using tf.keras to load this model and we will then use it to perform inference. We will time the performance of this model. . The code to download the weights of the pre-trained MobileNetV2 model is straightforward - . mobilenet_v2 = tf.keras.applications.MobileNetV2(weights=&#39;imagenet&#39;) mobilenet_v2.save(&#39;mobilenet_v2&#39;) . Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5 14540800/14536120 [==============================] - 0s 0us/step WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating: If using Keras pass *_constraint arguments to layers. INFO:tensorflow:Assets written to: mobilenet_v2/assets . Here, mobilenet_v2 is a directory and when you pass a directory to the save function, it serializes the model in the TensorFlow SavedModel format. This format makes it easier for us to be able to use it on different platforms - be it on GCP‚Äôs AI Platform, be it on TensorFlow JS, be it on TensorFlow Serving, and so on. . Now, there are some basic preprocessing steps to be followed before we can actually feed an image to this model - . # Prepare the image for prediction img = tf.keras.preprocessing.image.load_img(&#39;elephant.jpg&#39;, target_size=(224, 224)) x = tf.keras.preprocessing.image.img_to_array(img) x = np.expand_dims(x, axis=0) x = tf.keras.applications.mobilenet_v2.preprocess_input(x) . Here‚Äôs how elephant.jpg looks like in case if anyone‚Äôs curious - . Now, running the prediction and decoding it is just a matter of two lines of code - . # Run inference preds = mobilenet_v2.predict(x) print(&#39;Predicted:&#39;, tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=3)[0]) . Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json 40960/35363 [==================================] - 0s 0us/step Predicted: [(&#39;n02504013&#39;, &#39;Indian_elephant&#39;, 0.70024925), (&#39;n01871265&#39;, &#39;tusker&#39;, 0.2549572), (&#39;n02504458&#39;, &#39;African_elephant&#39;, 0.0033761878)] . To find out how much time does this model take to predict a given image? Let‚Äôs write a short utility function to handle that - . def time_my_model(model, data): times = [] for i in range(20): start_time = time.time() one_prediction = model.predict(data) delta = (time.time() - start_time) times.append(delta) mean_delta = np.array(times).mean() fps = 1 / mean_delta print(&#39;average(sec):{:.2f},fps:{:.2f}&#39;.format(mean_delta, fps)) . If we run time_my_model fives times the output would look like so - . average(sec):0.06,fps:15.48 average(sec):0.03,fps:32.26 average(sec):0.03,fps:32.48 average(sec):0.03,fps:31.14 average(sec):0.03,fps:31.67 . Can we further optimize this? We will start the next section with this question. . Optimizing the pre-trained image classification model . Note that explaining the different means of optimizing a deep learning model is out of the scop for this post. If you are interested, the following posts are great starting points - . High performance inference with TensorRT Integration . | Optimizing TensorFlow Models for Serving . | . Let‚Äôs now introduce the big elephant in the room - TensorRT. TensorRT is an SDK by NVIDIA for performing accelerated deep learning inference. It utilizes Tensor Cores of an NVIDIA GPU (for example V100, P4, etc.) and performs a number of model optimization steps for including parameter quantization, constant folding, model pruning, layer fusion, etc. You can know more about this SDK from here. . Note that TensorRT will only be able to achieve acceleration when it‚Äôs used on supported hardware. For more on this, check out the aforementioned link. . Optimizing the MobileNetV2 model is a three-step process - . Setting up the optimization configuration - . params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace( precision_mode=&#39;FP16&#39;, is_dynamic_op=True) . We use the precision_mode argument to specify the numerical precision of the model parameters we would want. In this case it is FP16 (float16). is_dynamic_op argument is set to True so that the shapes would be determined during runtime. Onto the next step. . (trt is aliased as from tensorflow.python.compiler.tensorrt import trt_convert as trt.) . | Performing the model conversion for optimization - . As the headline suggests, in this step we actually perform the conversion with the configurations we specified in the previous step to optimize our model. . converter = trt.TrtGraphConverterV2( input_saved_model_dir=&#39;mobilenet_v2&#39;, conversion_params=params) converter.convert() . For the conversion to take place, we are supplying the pre-trained MobileNetV2 model in the SavedModel format. It‚Äôs really nice to see how this format comes to unify different platforms. . | Serializing the optimized model - . Serializing this optimized model is similar to how we did it for the pre-trained model - . saved_model_dir_trt = &#39;mobilenet_v2.trt&#39; converter.save(saved_model_dir_trt) . | . Now, how good is this new variant of the model? How accurate will it be? How much faster will it be? We will find those out in a moment. Before that let‚Äôs see how to run inference with this optimized model in the next section. . Running inference with the optimized model . TensorFlow 2.x provides a convenient function tf.saved_model.load to load the models saved in SavedModel. We are only interested in performing inference with the model so we will load the respective signature from the model as a concrete function - . # Load the particular signature from the TRT graph root = tf.saved_model.load(saved_model_dir_trt) concrete_func = root.signatures[&#39;serving_default&#39;] . You can inspect the structure of this function by running concrete_func.structured_outputs and the output would be - . {&#39;predictions&#39;: TensorSpec(shape=(None, 1000), dtype=tf.float32, name=&#39;predictions&#39;)} . As we would expect, this function will yield a 1000-d vector which is nothing but probabilities distributed across the 1000 different classes of the ImageNet dataset. Also note the key of the above dictionary, it might not be ‚ÄòLogits‚Äô always. . Now, to be able to run the inference and decode them in a human-interpretable way, we first need to get the ImageNet dataset labels - . #hide_output # Gather the ImageNet labels first and prepare them labels_path = tf.keras.utils.get_file(&#39;ImageNetLabels.txt&#39;, &#39;https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt&#39;) imagenet_labels = np.array(open(labels_path).read().splitlines()) . Time for performing the inference - . # Perform inference labeling = concrete_func(tf.constant(x.astype(&#39;float32&#39;))) activations = tf.nn.softmax(labeling[&#39;predictions&#39;]) imagenet_labels[np.argsort(activations)[0,::-1][:5]+1] . array([&#39;Indian elephant&#39;, &#39;tusker&#39;, &#39;African elephant&#39;, &#39;bull mastiff&#39;, &#39;Great Dane&#39;], dtype=&#39;&lt;U30&#39;) . Looks like our optimized model got it right! . While parsing the predictions, we would need to put focus on the key in this case which is &#39;predictions&#39;. . Battle of performance . Let‚Äôs first recall where we were with our pre-trained MobileNetV2 - . average(sec):0.03,fps:37.22 average(sec):0.03,fps:36.54 average(sec):0.03,fps:36.54 average(sec):0.03,fps:38.93 average(sec):0.03,fps:37.24 . Now, to time the performance of our optimized model, we will need to make little adjustments to the utility function we previously wrote. This is mainly because now, we will now be using a concrete function which takes a tf.constant. . def time_trt_model(): image_input = tf.constant(x.astype(&#39;float32&#39;)) times = [] for i in range(20): start_time = time.time() one_prediction = concrete_func(input_1=image_input) delta = (time.time() - start_time) times.append(delta) mean_delta = np.array(times).mean() fps = 1 / mean_delta print(&#39;average(sec):{:.2f},fps:{:.2f}&#39;.format(mean_delta, fps)) . For convenience, here‚Äôs our x - . img = tf.keras.preprocessing.image.load_img(&#39;elephant.jpg&#39;, target_size=(224, 224)) x = tf.keras.preprocessing.image.img_to_array(img) x = np.expand_dims(x, axis=0) x = tf.keras.applications.mobilenet_v2.preprocess_input(x) . Let‚Äôs now run time_trt_model() for five times - . average(sec):0.00,fps:227.01 average(sec):0.00,fps:279.10 average(sec):0.00,fps:269.89 average(sec):0.00,fps:277.14 average(sec):0.00,fps:219.98 . That‚Äôs quite a bit of improvement, isn‚Äôt it? Note that you may experience slowed up inference in the first call to time_trt_model(). It may happen because of the warm-up of a GPU. When running comparisons like this, it‚Äôs a good practice to first warm up the base hardware by running a few test iterations on it and then run the actual iterations for comparison. Depending on the GPU you&#39;re using, these numbers can vary (these experiments are from a Tesla P100). . Talking about the memory footprints of both the models, for the pre-trained model we have - . # Size of the model files !du --all -h mobilenet_v2 . 96K mobilenet_v2/variables/variables.data-00000-of-00002 14M mobilenet_v2/variables/variables.data-00001-of-00002 20K mobilenet_v2/variables/variables.index 14M mobilenet_v2/variables 3.9M mobilenet_v2/saved_model.pb 4.0K mobilenet_v2/assets 18M mobilenet_v2 . We have a total of 18 MB here. For the optimized model, we have - . !du --all -h mobilenet_v2.trt . 96K mobilenet_v2.trt/variables/variables.data-00000-of-00002 14M mobilenet_v2.trt/variables/variables.data-00001-of-00002 20K mobilenet_v2.trt/variables/variables.index 14M mobilenet_v2.trt/variables 31M mobilenet_v2.trt/saved_model.pb 4.0K mobilenet_v2.trt/assets 45M mobilenet_v2.trt . Woah! The size has increased in this case! This is because the computational graph of our optimized model has been changed. However, the size of the parameters (mobilenet_v2/variables and mobilenet_v2.trt/variables) of both models is the same. . In real-life situations, as a machine learning practitioner, you will often have to make trade-offs between memory footprints, performance both in terms of accuracy and inference time. So having the knowledge of employing the right tools at the right moment will help you a long way. So, if you are looking for reducing the memory footprint of the model as well as accelerating the inference time, TensorFlow Lite is a good choice. In the bonus section of the post, we are going to discuss it. . Using TensorRT on your custom models . This section is for you to pick up. It will be a nice weekend project to train a simple model on a custom dataset and compare the performances as we saw in this post. Additionally, it will be interesting to compare different evaluation metrics like accuracy, precision, and recall for the different models (a custom trained mode, its optimized variants). We barely scratched the surface of TensorRT in this post. You are encouraged to experiment with the different arguments that come with the functions we saw in the post and figure out what works the best for your use-case. If your use-case involves embedded devices and mobile phones then TensorFlow Lite will be another great framework for you to explore. .",
            "url": "https://sayak.dev/tf.keras/tensorrt/tensorflow/2020/07/01/accelerated-inference-trt.html",
            "relUrl": "/tf.keras/tensorrt/tensorflow/2020/07/01/accelerated-inference-trt.html",
            "date": " ‚Ä¢ Jul 1, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Different data augmentation recipes in `tf.keras` for image classification",
            "content": "Data augmentation is a favorite recipe among deep learning practitioners especially for the ones working in the field of computer vision. Data augmentation is a technique used for introducing variety in training data thereby helping to mitigate overfitting. . When using Keras for training image classification models, using the ImageDataGenerator class for handling data augmentation is pretty much a standard choice. However, with TensorFlow, we get a number of different ways we can apply data augmentation to image datasets. In this tutorial, we are going to discuss three such ways. Knowing about these different ways of plugging in data augmentation in your image classification training pipelines will help you decide the best way for a given scenario. . Here‚Äôs a brief overview of the different ways we are going to cover: . Using the standard ImageDataGenerator class | Using TensorFlow image ops with a TensorFlow dataset | Using Keras‚Äôs (experimental) image processing layers | Mix-matching different image ops &amp; image processing layers | . Let‚Äôs get started! . Experimental setup . We are going to use the flowers dataset to demonstrate the experiments. Downloading the dataset is just as easy as executing the following line of code: . flowers contains the path (which in my case is - /root/.keras/datasets/flower_photos) where the dataset got downloaded. The structure of the dataset looks like so - . ‚îú‚îÄ‚îÄ daisy [633 entries] ‚îú‚îÄ‚îÄ dandelion [898] ‚îú‚îÄ‚îÄ roses [641] ‚îú‚îÄ‚îÄ sunflowers [699 entries] ‚îú‚îÄ‚îÄ tulips [799 entries] ‚îî‚îÄ‚îÄ LICENSE.txt . # Get the flowers dataset flowers = tf.keras.utils.get_file( &#39;flower_photos&#39;, &#39;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&#39;, untar=True) . Using the standard ImageDataGenerator class For most of the scenarios, the ImageDataGenerator should be good enough. Its flexible API design is really to follow and it makes it easier to work with custom image datasets by providing meaningful high-level abstractions. . We instantiate the ImageDataGenerator class like so - . img_gen = tf.keras.preprocessing.image.ImageDataGenerator( rescale=1./255, rotation_range=30, horizontal_flip=True) . We specify two augmentation operations and a pixel rescaling operation in there. ImageDataGenerator comes with a handy flow_from_directory method that allows us to read images from a directory and apply the specified operations on the fly during the time of training. Here‚Äôs how to instruct the img_gen object to read images from a directory - . IMG_SHAPE = 224 BATCH_SIZE = 32 img_flow = img_gen.flow_from_directory(flowers, shuffle=True, batch_size=BATCH_SIZE, target_size=(IMG_SHAPE, IMG_SHAPE)) . Found 3670 images belonging to 5 classes. . We then verify the images and the labels and they are indeed parsed right - . images, labels = next(img_flow) print(images.shape, labels.shape) show_batch(images, labels) . (32, 224, 224, 3) (32, 5) . Training with an ImageDataGenerator instance is extremely straight-forward - . model = get_training_model() model.fit(img_flow, ...) . For a fully worked out example, refer to this tutorial. . As can be seen in this blog post, ImageDataGenerator‚Äôs overall data loading performance can have a significant effect on how fast your model trains. To tackle situations, where you need to maximize the hardware utilization without burning unnecessary bucks, TensorFlow‚Äôs data module can be really helpful (comes at some costs). . TensorFlow image ops with tf.data APIs . The blog post I mentioned in the previous section shows the kind of performance boost achievable with tf.data APIs. But it‚Äôs important to note that boost comes at the cost of writing boilerplate code which makes the overall process more involved. For example, here‚Äôs how you would load and preprocess your images and labels - . def parse_images(image_path): # Load and preprocess the image img = tf.io.read_file(image_path) # read the raw image img = tf.image.decode_jpeg(img, channels=3) # decode the image back to proper format img = tf.image.convert_image_dtype(img, tf.float32) # scale the pixel values to [0, 1] img = tf.image.resize(img, [IMG_SHAPE, IMG_SHAPE]) # resize the image # Parse the labels label = tf.strings.split(image_path, os.path.sep)[5] return (img, label) . You would then write a separate augmentation policy with the TensorFlow Image ops - . def augment(image, label): img = tf.image.rot90(image) img = tf.image.flip_left_right(img) img = tf.clip_by_value(img, 0.0, 1.0) return (img, label) . To chain the above two together you would first create an initial dataset consisting of only the image paths - . image_paths = list(paths.list_images(flowers)) list_ds = tf.data.Dataset.from_tensor_slices((image_paths)) . Now, you would read, preprocess, shuffle, augment, and batch your dataset - . AUTO = tf.data.experimental.AUTOTUNE train_ds = ( list_ds .map(parse_images, num_parallel_calls=AUTO) .shuffle(1024) .map(augment, num_parallel_calls=AUTO) # augmentation call .batch(BATCH_SIZE) .prefetch(AUTO) ) . num_parallel_calls allows you to parallelize the mapping function and tf.data.experimental.AUTOTUNE lets TensorFlow decide the level of parallelism to use dynamically (how cool is that?). prefetch allows loading in the next batch of data well before your model finishes the current epoch of training. It is evident that this process is more involved than the previous one. . Verifying if we constructed the data input pipeline correctly is a vital step before you feed your data to the model - . image_batch, label_batch = next(iter(train_ds)) print(image_batch.shape, label_batch.shape) show_batch(image_batch.numpy(), label_batch.numpy(), image_data_gen=False) . (32, 224, 224, 3) (32,) . The ‚Äúb‚Äùs appear before the class labels because TensorFlow parses the strings as byte-strings. Using train_ds with your model is also just about executing - . model = get_training_model() model.fit(train_ds, ...) . Here you can find a fully worked out example. Here you can know more about the different performance considerations when using tf.data. There are more image ops available with TensorFlow Addons which can found here. . Recently, Keras introduced image_dataset_from_directory function (only available in tf-nightly at the time of writing this) which takes care of many of the boilerplate code we saw above and still yields pretty good performance. Here‚Äôs a tutorial that shows how to use it. . Keras has also introduced a number of image processing layers which can be very useful to build flexible augmentation pipelines using the Sequential API. In the next section, let‚Äôs see how. . Using Keras&#8217;s (experimental) image processing layers . Just like you would construct an entire model using the Sequential API, you can now construct very flexible data augmentation pipelines using the newly introduced (although experimental at the time of writing this) image processing layers. If we were to convert the data augmentation operations we have been following in the tutorial so far, building a data augmentation pipeline using this approach would be like so - . data_augmentation = tf.keras.Sequential([ tf.keras.layers.experimental.preprocessing.RandomFlip(&#39;horizontal&#39;), tf.keras.layers.experimental.preprocessing.RandomRotation(0.3) ]) . Before passing your data through this stack of layers makes sure you haven‚Äôt applied any augmentation already. So, it‚Äôs safe to create a separate TensorFlow dataset without mapping the augmentation function like we previously did - . # Create TensorFlow dataset without any augmentation train_ds = ( list_ds .map(parse_images, num_parallel_calls=AUTO) .shuffle(1024) .batch(BATCH_SIZE) .prefetch(AUTO) ) . Now, we can see how to examine some of the augmented images that would come out of this mini pipeline - . image_batch, label_batch = next(iter(train_ds)) plt.figure(figsize=(10, 10)) for n in range(25): ax = plt.subplot(5, 5, n+1) augmented_image = data_augmentation(tf.expand_dims(image_batch[n], 0)) plt.imshow(augmented_image[0].numpy()) plt.title(label_batch[n].numpy()) plt.axis(&#39;off&#39;) . Note that these layers can be also added as a part of your model definition which makes them ideal candidates when doing test-time augmentation. Training a model with this pipeline baked in requires a different approach to follow since we cannot map it directly as we did in the previous section. This is why this approach needs a different treatment. A sample training pipeline using this approach may look like so - . # You define an input layer with pre-defined shapes inputs = keras.Input(shape=(IMG_SHAPE, IMG_SHAPE, 3)) x = data_augmentation(inputs) # apply random data augmentation x = feature_extractor_model(x, training=False) x = GlobalAveragePooling2D()(x) x = Dropout(0.2)(x) outputs = Dense(1)(x) model = Model(inputs, outputs) . Now, model should be good to go with - model.fit(train_ds, ...). A fully worked out example is available here. Note that, performance might get slightly affected when going with this approach. . Let‚Äôs now think about situations where we may need to use a combination of the image ops of TensorFlow and the layers we just saw. What if we need to plug in custom augmentation operations in the augmentation pipeline? Added on top of it, what if we need to fix the probability at which the augmentation operations would get applied? Data augmentation pipelines are quite central behind the success of recent works like SimCLR, Augmix, etc. . Towards more complex augmentation pipelines . In this final approach, we will see how to mix and match between the different stock image ops, and stock image processing layers. Let‚Äôs first define a class utilizing the stock image ops with a utility function to apply them at random with a pre-defined probability. . class CustomAugment(object): def __call__(self, image): # Random flips and grayscale with some stochasticity img = self._random_apply(tf.image.flip_left_right, image, p=0.5) img = self._random_apply(self._color_drop, img, p=0.8) return img def _color_drop(self, x): image = tf.image.rgb_to_grayscale(x) image = tf.tile(x, [1, 1, 1, 3]) return x def _random_apply(self, func, x, p): return tf.cond( tf.less(tf.random.uniform([], minval=0, maxval=1, dtype=tf.float32), tf.cast(p, tf.float32)), lambda: func(x), lambda: x) . _random_apply is taken from the official SimCLR repository. Now, in order to tie it together with the stock image processing layers, we can still use the Sequential API with a Lambda layer - . # Build the augmentation pipeline data_augmentation = tf.keras.Sequential([ tf.keras.layers.Lambda(CustomAugment()), tf.keras.layers.experimental.preprocessing.RandomRotation(0.1) ]) . When we verify if it‚Äôs indeed correct, we get desired outputs - . image_batch, label_batch = next(iter(train_ds)) plt.figure(figsize=(10, 10)) for n in range(25): ax = plt.subplot(5, 5, n+1) augmented_image = data_augmentation(tf.expand_dims(image_batch[n], 0)) plt.imshow(augmented_image[0].numpy()) plt.title(label_batch[n].numpy()) plt.axis(&#39;off&#39;) . Training models when using this approach remains the same as the previous one. Keep in mind that performance can get affected when using this approach. . References . Fine-tuning with Keras and Deep Learning | Transfer learning &amp; fine-tuning | Image classification from scratch | Data augmentation | .",
            "url": "https://sayak.dev/tf.keras/data_augmentation/image/2020/05/10/augmemtation-recipes.html",
            "relUrl": "/tf.keras/data_augmentation/image/2020/05/10/augmemtation-recipes.html",
            "date": " ‚Ä¢ May 10, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Sharing your work online effectively",
            "content": "Well, you have put a lot of blood and sweat into writing your latest blog post on Machine Learning. Don&#39;t let your struggle go in vain and let the world know about it. Sharing your blog posts across different channels not only gives you exposure but also may get you tremendous feedback on your work. In my personal experience, the feedback has been super useful for me to improve myself not only as a writer but also as a communicator. There can be times you might have missed out on a super important detail, or you might have unknowingly introduced a snazzy bug in the code listings of your blog -- those things could have been caught in the process of feedback interchange. . In this short article, I am going to enlist a few different ways to share your work and get feedback. Note your work can be anything starting from a crucial GitHub PR, to a weekend project. Although the following platforms and communities are mostly limited to Machine Learning, I hope this guide will be useful for tech bloggers in general. . Sharing on platforms/communities . Before I start the sharing process, I generally create a Google Doc to effectively keep track of where I am sharing my work. This essentially acts as a checklist for all the places I want to share my work on. Here&#39;s the template I follow for creating the Google Doc - . Link to where the work has been posted. | Brief description of the work. | Post table: . . | . I generally keep the description to a maximum of 280 characters so that I can use it on Twitter as well. . Now, turning to the platforms and communities, here are some recommendations (in no particular order): . HackerNews (https://news.ycombinator.com/newest) | Made With ML (https://madewithml.com/) | Reddit r/MachineLearning | r/MachinesLearn | r/learnmachinelearning | r/deeplearning | . | Twitter | Facebook AIDL | Montreal AI | Deep Learning | . | Fast.ai Forum (https://forums.fast.ai/) | LinkedIn | Google Groups (depends on the framework used in the work) discuss@tensorflow.org | tflite@tensorflow.org | tfjs@tensorflow.org | tfx@tensorflow.org | . | . While sharing my work, I find it to be important to always attach a brief description. Additionally, if your work is related to implementing research work, you should definitely include it on Papers with Code. . Sharing to aid discussions . You might be active on online forums like Quora, StackOverflow, and so on. While participating in a discussion in those forums you can make effective use of your work if it is relevant. In these cases, the approach is to not just supply a link to your work, but also to first write about any important pointers relevant to the discussion first, and then supply the link to your work to better aid it. Let&#39;s say there&#39;s a discussion going on the topic of &quot;What is Weight Initialization in Neural Nets?&quot; Here&#39;s how I would approach my comment: . A neural net can be viewed as a function with learnable parameters and those parameters are often referred to as weights and biases. Now, while starting the training of neural nets these parameters (typically the weights) are initialized in a number of different ways - sometimes, using constant values like 0‚Äôs and 1‚Äôs, sometimes with values sampled from some distribution (typically a uniform distribution or normal distribution), sometimes with other sophisticated schemes like Xavier Initialization. The performance of a neural net depends a lot on how its parameters are initialized when it is starting to train. Moreover, if we initialize it randomly for each run, it‚Äôs bound to be non-reproducible (almost) and even not-so-performant too. On the other hand, if we initialize it with constant values, it might take way too long to converge. With that, we also eliminate the beauty of randomness which in turn gives a neural net the power to reach covergence quicker using gradient-based learning. We clearly need a better way to initialize it. Careful initialization of weights helps us to train them better. To know more, please follow this article of mine. . Well, that&#39;s it for now. I hope it proves to be useful for you. Please provide any suggestions you may have via the comments. I am thankful to Alessio of FloydHub for sharing these tips with me. .",
            "url": "https://sayak.dev/blogs/sharing/2020/04/20/sharing-work-effectively.html",
            "relUrl": "/blogs/sharing/2020/04/20/sharing-work-effectively.html",
            "date": " ‚Ä¢ Apr 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Embedding an image preprocessing function in a `tf.keras` model",
            "content": "In this tutorial, we are going to see how to embed a simple image preprocessing function within a trained model (tf.keras) while exporting it for serving. This is a useful feature to have because it can help us reduce a lot of boilerplate code needed while using any model for serving purposes. With this capability, you get a lot more flexibility and modularity to your model. . Data loading, preprocessing, and visualization . To keep things simple we will be using the FashionMNIST dataset. Note that these techniques can easily be applied to more complex models as well (with some limitation). . We are not going to preprocess the images before hand. We will let the model do it. . # Load data (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() . # Class labels (don&#39;t change the order) CLASSES = [&quot;T-shirt/top&quot;, &quot;Trouser&quot;, &quot;Pullover&quot;, &quot;Dress&quot;, &quot;Coat&quot;, &quot;Sandal&quot;, &quot;Shirt&quot;, &quot;Sneaker&quot;, &quot;Bag&quot;, &quot;Ankle boot&quot;] . # Show a few examples from the train set plt.figure(figsize=(10,10)) for i in range(25): plt.subplot(5,5,i+1) plt.xticks([]) plt.yticks([]) plt.grid(True) plt.imshow(x_train[i], cmap=plt.cm.binary) plt.xlabel(CLASSES[y_train[i]]) plt.show() . Model building and training . We are good to proceed towards building and training a neural network. We will first define a simple preprocessing function to scale the pixel values and then we will embed it into the model using a Lambda layer. You can replace this anything fancy you would want. . We will use a shallow network architecture so that we can train it quickly. . # Define the preprocessing function # We will embed it in the model later def preprocess_image(image_pixels): img = image_pixels / 255 return img # A humble model def get_training_model(): # Construct the model using the Functional API input_layer = tf.keras.layers.Input(shape=(28, 28), name=&quot;input_layer&quot;) preproc_layer = tf.keras.layers.Lambda(preprocess_image, name=&quot;lambda_layer&quot;)(input_layer) # Preprocessing function flatten = tf.keras.layers.Flatten()(preproc_layer) dense_1 = tf.keras.layers.Dense(128, activation=&quot;relu&quot;)(flatten) dropout = tf.keras.layers.Dropout(0.2)(dense_1) outputs = tf.keras.layers.Dense(len(CLASSES), activation=&quot;softmax&quot;)(dropout) # Create the model model = tf.keras.models.Model(input_layer, outputs) # Compile the model and return it model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) return model . # Topology of the model tf.keras.utils.plot_model(get_training_model(), show_shapes=True) . The Lambda layer is our preprocessing layer. . # Train the model for 10 epochs apparel_model = get_training_model() history = apparel_model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=128) . Epoch 1/10 469/469 [==============================] - 2s 4ms/step - loss: 0.6004 - accuracy: 0.7937 - val_loss: 0.4682 - val_accuracy: 0.8347 Epoch 2/10 469/469 [==============================] - 2s 4ms/step - loss: 0.4246 - accuracy: 0.8495 - val_loss: 0.4089 - val_accuracy: 0.8521 Epoch 3/10 469/469 [==============================] - 2s 4ms/step - loss: 0.3795 - accuracy: 0.8642 - val_loss: 0.3928 - val_accuracy: 0.8564 Epoch 4/10 469/469 [==============================] - 2s 4ms/step - loss: 0.3576 - accuracy: 0.8711 - val_loss: 0.3632 - val_accuracy: 0.8687 Epoch 5/10 469/469 [==============================] - 2s 4ms/step - loss: 0.3407 - accuracy: 0.8762 - val_loss: 0.3593 - val_accuracy: 0.8688 Epoch 6/10 469/469 [==============================] - 2s 4ms/step - loss: 0.3294 - accuracy: 0.8788 - val_loss: 0.3532 - val_accuracy: 0.8721 Epoch 7/10 469/469 [==============================] - 2s 4ms/step - loss: 0.3165 - accuracy: 0.8846 - val_loss: 0.3609 - val_accuracy: 0.8685 Epoch 8/10 469/469 [==============================] - 2s 4ms/step - loss: 0.3084 - accuracy: 0.8859 - val_loss: 0.3503 - val_accuracy: 0.8701 Epoch 9/10 469/469 [==============================] - 2s 4ms/step - loss: 0.2982 - accuracy: 0.8915 - val_loss: 0.3560 - val_accuracy: 0.8713 Epoch 10/10 469/469 [==============================] - 2s 4ms/step - loss: 0.2886 - accuracy: 0.8929 - val_loss: 0.3381 - val_accuracy: 0.8776 . Now that we have a trained model, we can go ahead and export it and then we will see how to use it on new images for inference. . Sample test image and model export . We are getting close. Now that we have a trained model here are the things we would do from here: . Serialize a randomly selected image from the test set. | Export the model and parse model predictions. | . Let&#39;s go. . Step 1: Serializing a randomly selected image from the test set . # Select a random image from the test set for serialization sampe_test_img_id = np.random.choice(x_test.shape[0], 1) sampe_test_img = x_test[sampe_test_img_id].squeeze() # Remove the batch dimension sampe_test_img = (sampe_test_img * 255).astype(&quot;int32&quot;) # Scale back to integer # Verify image label and shape print(&quot;Image class: &quot;,CLASSES[y_test[int(sampe_test_img_id)]]) print(sampe_test_img.shape) . Image class: Ankle boot (28, 28) . #hide_output # Serialize the image cv2.imwrite(&quot;sample_image.png&quot;, sampe_test_img) . Note that while writing a grayscale image, OpenCV adds the channel dimension of 3 to it. We will need to handle carefully. . # Make sure the serialized image is good to go plt.imshow(plt.imread(&quot;sample_image.png&quot;), cmap=plt.cm.binary) plt.show() . Step 2: Model exporting and parsing predictions . Let&#39;s first serialize our model and load it. . # Serialize the model and load it apparel_model.save(&quot;apparel_model.h5&quot;) restored_model = tf.keras.models.load_model(&quot;apparel_model.h5&quot;) . WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer. . This warning is not desirable. When the optimizer is loaded with a fresh state, the model predictions can be erroneous. So, to resolve this problem we will only be serializing the weights of the model with the save_weights() function. There can be other nuances like this when you work with Lambda layers and you can check this article out to know about them. . apparel_model.save_weights(&quot;apparel_model.h5&quot;) . We will now initialize a dummy model with the same architecture as the one we trained and we will then load the weights of our trained model into it. . restored_model = get_training_model() restored_model.load_weights(&quot;apparel_model.h5&quot;) . Now we should be good to go with the predictions part. First, let&#39;s load the image we serialized in step 1. As mentioned before, OpenCV adds 3-channels to grayscale images while saving them. We can take care of this issue with cv2.cvtColor(image_pixels, cv2.COLOR_BGR2GRAY). . # Load the image image_pixels = cv2.imread(&quot;sample_image.png&quot;) image_pixels = cv2.cvtColor(image_pixels, cv2.COLOR_BGR2GRAY) # Preview the image plt.imshow(image_pixels, cmap=plt.cm.binary) plt.show() . # Run inference and parse the prediction class_probabilities = restored_model.predict(np.expand_dims(image_pixels, 0))[0] print(&quot;Predicted &quot;,CLASSES[np.argmax(class_probabilities)]) . Predicted Ankle boot . We can see that it is working as expected. . # Load and *preprocess* data (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() x_train = x_train / 255 x_test = x_test / 255 . Taking it a step further with concrete functions and SavedModel . The SavedModel format is the standard serialization format in TensorFlow 2.x since it communicates very well with the entire TensorFlow ecosystem. Be it GCP AI Platform, be it tf.keras, be it TFLite, etc,, SavedModel format unifies the entire ecosystem. For serializing custom models (developed using subclassing) SavedModel would be needed as well. . In this section, let&#39;s see how can we do the same i.e. embed a preprocessing function inside a model so that it can be serialized in the SavedModel format. . Step 1: Create a sequential model without any preprocessing layer . def get_training_model_v2(): # Construct the model using the Functional API input_layer = tf.keras.layers.Input(shape=(28, 28), name=&quot;input_layer&quot;) flatten = tf.keras.layers.Flatten()(input_layer) dense_1 = tf.keras.layers.Dense(128, activation=&quot;relu&quot;)(flatten) dropout = tf.keras.layers.Dropout(0.2)(dense_1) outputs = tf.keras.layers.Dense(len(CLASSES), activation=&quot;softmax&quot;)(dropout) # Create the model model = tf.keras.models.Model(input_layer, outputs) # Compile the model and return it model.compile(optimizer=&quot;adam&quot;, loss=&quot;sparse_categorical_crossentropy&quot;, metrics=[&quot;accuracy&quot;]) return model . Step 2: Train it! . # Train the model for 10 epochs apparel_model_v2 = get_training_model_v2() history = apparel_model_v2.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=128) . Epoch 1/10 469/469 [==============================] - 2s 4ms/step - loss: 0.5995 - accuracy: 0.7914 - val_loss: 0.4549 - val_accuracy: 0.8347 Epoch 2/10 469/469 [==============================] - 2s 4ms/step - loss: 0.4200 - accuracy: 0.8501 - val_loss: 0.4094 - val_accuracy: 0.8520 Epoch 3/10 469/469 [==============================] - 2s 4ms/step - loss: 0.3823 - accuracy: 0.8616 - val_loss: 0.3831 - val_accuracy: 0.8635 Epoch 4/10 469/469 [==============================] - 2s 4ms/step - loss: 0.3575 - accuracy: 0.8713 - val_loss: 0.3896 - val_accuracy: 0.8563 Epoch 5/10 469/469 [==============================] - 2s 4ms/step - loss: 0.3405 - accuracy: 0.8758 - val_loss: 0.3569 - val_accuracy: 0.8720 Epoch 6/10 469/469 [==============================] - 2s 4ms/step - loss: 0.3249 - accuracy: 0.8813 - val_loss: 0.3490 - val_accuracy: 0.8733 Epoch 7/10 469/469 [==============================] - 2s 4ms/step - loss: 0.3176 - accuracy: 0.8840 - val_loss: 0.3480 - val_accuracy: 0.8735 Epoch 8/10 469/469 [==============================] - 2s 4ms/step - loss: 0.3055 - accuracy: 0.8878 - val_loss: 0.3355 - val_accuracy: 0.8809 Epoch 9/10 469/469 [==============================] - 2s 4ms/step - loss: 0.2971 - accuracy: 0.8914 - val_loss: 0.3331 - val_accuracy: 0.8792 Epoch 10/10 469/469 [==============================] - 2s 4ms/step - loss: 0.2905 - accuracy: 0.8920 - val_loss: 0.3344 - val_accuracy: 0.8808 . Step 3: SavedModel plunge . Okay! Now we are ready to the crux of the section. We will first create a custom model class (inherited from tf.keras.Model) and it will contain two things: . A model that is loaded with the weights of a trained model | A serving function that will contain the preprocessing function along with the necessary signature. | . # A custom class for serving class ExportModel(tf.keras.Model): def __init__(self, model): super().__init__(self) self.model = model @tf.function(input_signature=[tf.TensorSpec([None, 28, 28], dtype=tf.uint8)]) def my_serve(self, images): images = tf.cast(images, tf.float32) / 255 # pre-processing probabilities = self.model(images) # prediction from model class_index = tf.argmax(probabilities, axis=-1) # post-processing return {&quot;class_index&quot;: class_index} . my_serve is our serving function. You can see that is decorated with tf.function and the reason behind doing so is it allows us to embed an arbitrary function in a model&#39;s graph which can later be exported using the SavedModel format. . We can also see - input_signature=[tf.TensorSpec([None, 28, 28], dtype=tf.uint8)]. This is needed in order to indicate which part of the model&#39;s graph would be needed while serving. By specifying tf.TensorSpec([None, 28, 28], we instruct the function that the inputs should respect this shape - [None, 28, 28] and the dtype argument is self-explanatory. . We will get to why the return type of the function is done in such a way - {&quot;class_index&quot;: class_index} in a moment. . If you are interested to know more using SavedModel and different serialization options that come with it, be sure to check this tutorial out. . Step 4: Instantiate a dummy model and set its weights . # Set the weights of this dummy model to the weights of the model we trained restored_model = get_training_model_v2() restored_model.set_weights(apparel_model_v2.get_weights()) . Step 5: Export the model and run inference . Now, to serialize the model in the SavedModel format we will make use of tf.saved_model.save. It can automatically determine which input signature to use for serving for most of the models if the details are available. However, in our case, it won&#39;t be able to do so. So, we will need to explicitly indicate which function to use as the signature while serving. . export_path = &quot;/content/saved_model/1/&quot; tf.keras.backend.set_learning_phase(0) # Make sure no weight update happens serving_model = ExportModel(restored_model) # Instantiate a model with the preprocessing function tf.saved_model.save(serving_model, export_path, signatures={&#39;serving_default&#39;: serving_model.my_serve}) . WARNING:tensorflow:Skipping full serialization of Keras layer &lt;__main__.ExportModel object at 0x7f4096b7b358&gt;, because it is not built. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating: If using Keras pass *_constraint arguments to layers. INFO:tensorflow:Assets written to: /content/saved_model/1/assets . By specifying &#39;serving_default&#39;: serving_model.my_serve we instructed tf.saved_model.save about which signature to use for serving. Now if we inspect what all were saved, things should seem consistent. For this we are going to use the saved_model_cli command-line interpreter. . !saved_model_cli show --dir /content/saved_model/1 --tag_set serve --signature_def serving_default . The given SavedModel SignatureDef contains the following input(s): inputs[&#39;images&#39;] tensor_info: dtype: DT_UINT8 shape: (-1, 28, 28) name: serving_default_images:0 The given SavedModel SignatureDef contains the following output(s): outputs[&#39;class_index&#39;] tensor_info: dtype: DT_INT64 shape: (-1) name: StatefulPartitionedCall:0 Method name is: tensorflow/serving/predict . So, we can see that the configuration that is expected from the inputs and the outputs of the serialized model is consistent with what we had instructed. We returned the outputs in form a dictionary (namely class_index) in my_serve and we can see that as well. . We can also do the inspection in Pythonic ways. . loaded = tf.saved_model.load(&quot;/content/saved_model/1/&quot;) print(list(loaded.signatures.keys())) # This signature will be used while serving . [&#39;serving_default&#39;] . # Output configuration infer = loaded.signatures[&quot;serving_default&quot;] print(infer.structured_outputs) . {&#39;class_index&#39;: TensorSpec(shape=(None,), dtype=tf.int64, name=&#39;class_index&#39;)} . Let&#39;s finally run the inference! . # Load the sample image image_pixels = cv2.imread(&quot;sample_image.png&quot;) image_pixels = cv2.cvtColor(image_pixels, cv2.COLOR_BGR2GRAY) . # Run inference CLASSES[infer(tf.constant(image_pixels))[&quot;class_index&quot;].numpy()[0]] . &#39;Ankle boot&#39; . We can see that the prediction is correct in this case as well. So, when we ran infer = loaded.signatures[&quot;serving_default&quot;] we essentially loaded a concrete function i.e. we loaded my_serve. Remember we assigned the value of serving_default in the beginning of this section? . With infer(tf.constant(image_pixels)) we are simply running our input image through the concrete function and we are parsing the output from the dictionary (class_index being the key) it returns . . References . MNIST on TPU (Tensor Processing Unit) or GPU using tf.Keras and tf.data.Dataset | Using the SavedModel format | .",
            "url": "https://sayak.dev/tf.keras/preprocessing/2020/04/13/embedding-image-preprocessing-functions.html",
            "relUrl": "/tf.keras/preprocessing/2020/04/13/embedding-image-preprocessing-functions.html",
            "date": " ‚Ä¢ Apr 13, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am currently working at PyImageSearch on Computer Vision and Deep Learning. My projects span across a wide variety of topics including model optimization, generative modeling, CRNN architectures, and so on. . Previously at DataCamp, I developed projects (Predicting Credit Card Approvals and Analyze International Debt Statistics), and practice pools (Advanced Deep Learning with Keras (requires a login to see)). Prior to DataCamp, I worked at TCS Research and Innovation (TRDDC) on Data Privacy. There, I was a part of the cybersecurity research team working on TCS‚Äôs critically acclaimed GDPR solution called Crystal Ball. . My subject of interest broadly lies in the area of visual representation learning. I love open-source initiatives and currently, I am actively contributing to TensorFlow Hub. Off the work, I like writing technical articles, working on applied Machine Learning ideas, and giving talks at developer meetups and conferences. . | | | | . Timeline: . Deep Learning Associate, PyImageSearch (June, 2019 - present) | Data Science Instructor, DataCamp (August 2018 - June 2019) (on contract) | Software Engineer, TCS Research and Innovation (January 2018 - August 2018) | Software Engineer, Tata Consultancy Services Limited (July 2017 - January 2018) | Intern, CareerIn (Dec, 2016 - Feb, 2017) | . Badges I proudly endorse: . . An honour to be their son üôÇ . Tapas Kumar Paul | Baby Paul | .",
          "url": "https://sayak.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Authoring",
          "content": "Co-authored a book Hands-On Python Deep Learning for the Web with Anubhav Singh (Packt Publishers). . Authored the following liveProjects with Manning: . Use Machine Learning to Detect Phishing Websites | Summarize News Articles with NLP, Deep Learning, and Python (with Souradip Chakraborty) | . Authored two DataCamp Projects (Predicting Credit Card Approvals and Analyze International Debt Statistics) and a DataCamp Practice Pool on Advanced Deep Learning with Keras. . Below are the blogs, articles, and tutorials I have written on Data Science, Machine Learning and more. . Datacamp . KMeans clustering with scikit-learn - https://goo.gl/dT7kYq | Hyperparameter Optimization in Machine Learning Models - https://goo.gl/5C6ouV | Towards Preventing Overfitting: Regularization - https://goo.gl/B9vxia | Ensemble Learning in Python - https://goo.gl/dmH9db | Investigating Tensors with PyTorch - https://goo.gl/yoYsVL | Introduction to Feature Selection - https://goo.gl/gY8rwy | Demystifying crucial Statistics in Python - https://goo.gl/i2Wm5v 1 | Diving Deep with Imbalanced Data - https://goo.gl/fZnYmV | Introduction to Cyclical Learning Rates - https://goo.gl/2fpkQQ | Turning Machine Learning Models into APIs in Python2 - https://goo.gl/vwzqtA | Essentials of Linear Regression in Python3 - https://goo.gl/5nuVmt | Simplifying Sentiment Analysis in Python - https://goo.gl/62mEJo | Automated Machine Learning with Auto-Keras - https://goo.gl/XEjea4 | Introduction to Indexing in SQL - https://goo.gl/7dcnE7 | Understanding Recursive Functions in Python - https://goo.gl/u1U2eH | Beginner‚Äôs Guide to Google‚Äôs Vision API in Python - https://goo.gl/VCwZa8 | Beginner‚Äôs Guide to PostgreSQL - https://goo.gl/DV1rhY | Managing Databases in PostgreSQL - https://goo.gl/YA9fAy | Working with Spreadsheets in SQL - https://goo.gl/PYUb2v | Installing PostgreSQL on Windows and Mac OS X - https://goo.gl/CyF8T4 | Using Order By Keyword in SQL - https://goo.gl/i7mD8f | Introduction to Alter Table Statement in SQL - https://goo.gl/qWi3km | SQLite in Python - https://goo.gl/wYCr4e | Introduction to Where Clause in SQL - https://goo.gl/VB3CdX | Introduction to SQL Joins - https://goo.gl/2w342W | 10 command-line utilities in PostgreSQL - https://goo.gl/xFWbRS | CASE Statements in PostgreSQL - https://bit.ly/2HWBSwu | Aggregate Functions in SQL - https://bit.ly/2GnDqg9 | Cleaning Data in SQL - http://bit.ly/2GyPdrL | Materialized Views in PostgreSQL - http://bit.ly/2VFz11x | Argument Parsing in Python - http://bit.ly/2LOWGsJ | Ten Important Updates from TensorFlow 2.0 - http://bit.ly/2EHENWL | Implementing Neural Style Transfer using TensorFlow 2.0 - http://bit.ly/2J1mmxv | TensorFlow 2.0 Case Study - http://bit.ly/2U0yjZA | . FloydHub . Introduction to Anomaly Detection in Python - https://bit.ly/2TZLg4d | Introduction to K-Means Clustering in Python with scikit-learn - https://bit.ly/2IZev5a | An introduction to Q-Learning: Reinforcement Learning - http://bit.ly/2HxuVzo | How to plan and execute your ML and DL projects - http://bit.ly/2XtMCeh | Becoming One With the Data - [http://bit.ly/30N3bPA] 4 | Training Neural Nets: a Hacker‚Äôs Perspective - http://bit.ly/training-neural-nets | . Weights and Biases . Running Hyperparameter Sweeps to Pick the Best Model - http://bit.ly/2MKHR7K | arXiv Search: Generating Tags from Paper Titles - http://bit.ly/2WQ8sFh | How to Use GCP with Weights &amp; Biases - http://bit.ly/399Fd60 | Mixed precision training with tf.keras - http://bit.ly/2RIkQ9z | Customizing Training Loops in TensorFlow 2.0 - http://bit.ly/39DOmEf | Bayesian Hyperparameter Optimization - A Primer - http://bit.ly/38SqXgR | Visualize models in TensorBoard with Weights and Biases - http://bit.ly/3cCP5qq | The effects of weight initialization on neural nets - https://bit.ly/3bk34Qu | Introduction to image inpainting with deep learning - https://bit.ly/39CqTBK (joint work with Ayush Thakur) | Kaggle Starter Kernel - Jigsaw Multilingual Toxic Comment Classification - https://bit.ly/2UQtnbB | Distributed training in tf.keras with W&amp;B - https://bit.ly/2JZkQwJ | Reproducible Models with W&amp;B - https://bit.ly/34V5ZNz | EvoNorm layers in TensorFlow 2 - https://bit.ly/3arUw9q | Transfer Learning with EfficientNet family of models - https://bit.ly/2zMJVcE | A Tale of Model Quantization in TF Lite - https://bit.ly/3dlCRSI | Towards self-supervised image understanding with SimCLR - https://bit.ly/2LHq6WR | The Power of Random Features of a CNN - https://bit.ly/2XKv76A | Plotting top loss images while training models - https://bit.ly/30Xzs9E (joint work with Tulasi) | Improving Image Classifiers with Supervised Contrastive Learning - https://bit.ly/2UVZtm7 (joint work with Sweta Shaw) | Plunging into Model Pruning in Deep Learning - https://bit.ly/2AJ67W4 | Understanding the Effectivity of Ensembles in Deep Learning - https://bit.ly/deep-ensembles (joint work with Ayush Thakur) | An Introduction to Adversarial Examples in Deep Learning - https://bit.ly/3b2zss2 | Unsupervised Visual Representation Learning with SwAV - http://bit.ly/swav-tf (joint work with Ayush Thakur) | Distilling Knowledge in Neural Networks - https://bit.ly/3mvsbGn | . Other . Your First Machine Learning Project: Q and A with Sayak Paul, Google Developer Expert (GDE) in Machine Learning (Ep. 4) | AMA with Sayak Paul - Hacktoberfest‚Äô19 - https://fossassam.tech/post/ama-sayak/ | Predicting the publisher‚Äôs name from an article: A case study (for Google Developers Experts‚Äô Medium Channel) - http://bit.ly/2K9TpS8 5 | GDE Journey ‚Äî Sayak Paul (for Google Developers Experts‚Äô Medium Channel) - https://bit.ly/2WVQsLd | Multi-part tutorial series on Selfie2Anime with TFLite (joint work with ML-GDE Margaret Maynard-Reid) - Part I, Part II, Part III | How to Create a Cartoonizer with TensorFlow Lite (joint work with ML-GDE Margaret Maynard-Reid) - http://bit.ly/cartoonizer-app 6 | Multi-part tutorial series on Create Artistic Effect by Stylizing Image Background (joint work with ML-GDE Margaret Maynard-Reid and George Soloupis) - Part I, Part II, Part III | A comprehensive list of data science resources for developers (for Intel DevMesh) - https://intel.ly/2J2UYSs | Detecting phishing websites using machine learning (Intel Software Innovators‚Äô Medium Channel) - http://bit.ly/2YBvaAs | Lessons learned from a Deep Learning Hackathon (Intel Software Innovators‚Äô Medium Channel) - http://bit.ly/2YfnZhI | Introduction to procedures and cursors in SQL (Towards Data Science) - https://bit.ly/2OTd8WF | ‚ÄúReparameterization‚Äù trick in Variational Autoencoders (Towards Data Science) - https://bit.ly/2RjoWnM | . This article got featured in ‚ÄúPython Top 10 Articles for the Past Month (v.Oct 2018)‚Äù and secured a rank of 4.¬†&#8617; . | This article got featured in ‚ÄúMachine Learning Top 10 Articles for the Past Month (v.Nov 2018)‚Äù and secured a rank of 9.¬†&#8617; . | This article got featured in ‚ÄúPython Top 10 Articles for the Past Month (v.Dec 2018)‚Äù and secured a rank of 10.¬†&#8617; . | Featured in Sebastian Ruder‚Äôs monthly newsletter.¬†&#8617; . | This one won the ML GDE Dev Challenge¬†&#8617; . | Published from the official TensorFlow blog¬†&#8617; . |",
          "url": "https://sayak.dev/authoring/",
          "relUrl": "/authoring/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "Education",
          "content": "(The formal ones may be) . B.Tech in IT from Netaji Subhash Engineering College (2013 - 17) (Final year dissertation: A CFS‚ÄìDNN-Based Intrusion Detection System) | High School from Jadavpur Vidyapith (PCMC) (2005 - 13) | Courses relevant to my subject of interest: Data Scientist with Python Track (DataCamp) | Data Analyst with Python Track (DataCamp) | Deep Learning Specialization (deeplearning.ai) | Advanced Machine Learning with TensorFlow on Google Cloud Platform | TensorFlow in Practice Specialization (deeplearning.ai) | TensorFlow: Data and Deployment Specialization (deeplearning.ai) | Generative Adversarial Networks (GANs) (deeplearning.ai) | Natural Language Processing (deeplearning.ai) | TensorFlow Developer Certification | Mathematics for Machine Learning (deeplearning.ai) | . | .",
          "url": "https://sayak.dev/education/",
          "relUrl": "/education/",
          "date": ""
      }
      
  

  

  
      ,"page5": {
          "title": "Interviews",
          "content": "The purpose of conducting these interviews is to mainly get insights about the real-world project experiences, perspectives on learning new things, some fun facts and thereby enriching the communities in the process. I sincerely thank the interviewees for taking the time out from their busy schedules and for agreeing to do these interviews. Here are the interviews I have done so far - . An interview with Robert Crowe, Developer Advocate (TensorFlow) at Google | An interview with Snehasis Banerjee, Scientist at TCS Research and Innovation | An interview with Abhishek Kumar, Senior Manager, Data Science at Publicis Sapient | An interview with Laurence Moroney, Developer Advocate at Google | An interview with Karl Fezer, AI Ecosystem Evangelist at Arm | An interview with Dan Becker, Team Lead of Kaggle Learn &amp; Product Lead of Kaggle Kernels | An interview with Rajarshee Mitra, Data Scientist at Microsoft | An interview with Alessio, Lead Data Scientist at FloydHub | An interview with Joel Grus, Research Engineer at Allen Institute for Artificial Intelligence | An interview with Josh Tobin, Research Scientist at OpenAI | An interview with Andrew Ferlitsch, Developer Program Engineer at Google | An interview with Shalini De Mello, Principal Research Scientist at NVIDIA | An interview with Rahul Agrawal, Principal Machine Learning Manager at AI and Research, Microsoft | An interview with Aakash Nain, Research Engineer at Ola | An interview with Xander Steenbrugge, Machine Learning Researcher &amp; YouTuber at ‚ÄúArxiv Insights‚Äù | An interview with Ines Montani, Co-founder at Explosion | An interview with Girish Palshikar, Principal Scientist at TCS Research and Innovation | An interview with Christoph Molnar, Interpretable Machine Learning Researcher | An interview with Leslie Smith, Senior Research Scientist at U.S. Naval Research Laboratory | An interview with Arindam Pal, Senior Research Scientist at CSIRO | An interview with Ankur Patel, Vice President of Data Science at 7Park Data | An interview with Max Pumperla, Deep Learning Engineer at Skymind | An interview with Abhishek Thakur, Data Scientist, and Kaggle 3x Grandmaster | An interview with Dmytro Mishkin, Computer Vision Researcher | An interview with Ellick Chan, Head of University Relations and Research ‚Äî Intel AI Academy | An interview with Thomas Wolf, Chief Science Officer at Hugging Face | An interview with Dat Tran, Head of AI at Axel Springer AI | An interview with Daniel Seita, Ph.D. student at UC, Berkeley | An interview with Vladimir Iglovikov, Senior Computer Vision Engineer at Lyft | An interview with Hamel Husain, Staff Machine Learning Engineer at GitHub | An interview with Patrick Hall, Principal Scientist at bnh.ai and Advisor to H2O.ai | An interview with Colin Raffel, Research Scientist at Google | An interview with Niki Parmar, Senior Research Scientist at Google Brain | An interview with Alexander (Sasha) Rush, Associate Professor at Cornell University | An interview with Vincent Sitzmann, Postdoctoral Researcher at MIT | An interview with Dan Hendrycks, Ph.D. student at UC Berkeley | .",
          "url": "https://sayak.dev/interviews/",
          "relUrl": "/interviews/",
          "date": ""
      }
      
  

  
      ,"page6": {
          "title": "Research",
          "content": "This page enlists the publications I have been a part of. . Publications: . Paul S., Banerjee C., Ghoshal M. (2018) A CFS‚ÄìDNN-Based Intrusion Detection System. In: Bera R., Sarkar S., Chakraborty S. (eds) Advances in Communication, Devices and Networking. Lecture Notes in Electrical Engineering, vol 462. Springer, Singapore. | Gupta J., Paul S., Ghosh A. (2019) A Novel Transfer Learning-Based Missing Value Imputation on Discipline Diverse Real Test Datasets‚ÄîA Comparative Study with Different Machine Learning Algorithms. In: Abraham A., Dutta P., Mandal J., Bhattacharya A., Dutta S. (eds) Emerging Technologies in Data Mining and Information Security. Advances in Intelligent Systems and Computing, vol 814. Springer, Singapore. | Saptarshi Sengupta, Sanchita Basak, Pallabi Saikia, Sayak Paul, Vasilios Tsalavoutis, Frederick Atiah, Vadlamani Ravi, Alan Peters, A review of deep learning with special emphasis on architectures, applications and recent trends, Knowledge-Based Systems, Volume 194, 2020, 105596, ISSN 0950-7051. | Souradip Chakraborty 1, Aritra Roy Gosthipaty 1, Sayak Paul 1; ‚ÄúG-SimCLR‚ÄØ: Self-Supervised Contrastive Learning with Guided Projection via Pseudo Labelling.‚Äù ArXiv:2009.12007 [Cs, Stat], Sept. 2020. arXiv.org, https://arxiv.org/abs/2009.12007. The code is available here. This work is accepted at ICDM 2020 for the Deep Learning for Knowledge Transfer (DLKT) workshop. | . Equal contribution¬†&#8617;¬†&#8617;2¬†&#8617;3 . |",
          "url": "https://sayak.dev/research/",
          "relUrl": "/research/",
          "date": ""
      }
      
  

  
  

  
  

  

  
      ,"page10": {
          "title": "Talks/Seminars/Workshops",
          "content": "I love to attend developer meetups, conferences, workshops and learn from them as much as I can. I sometimes talk on a range of topics that I love the most. All the slides of my talks/sessions can be found below. . Given by me: . Presented our paper A CFS‚ÄìDNN-Based Intrusion Detection System at International Conference on Communication Devices and Networking, Sikkim Manipal Institute of Technology, Sikkim, June 3, 2017. | Presented our paper A Comparative Study of Different Ensemble Learning Techniques Using Wisconsin Breast Cancer Dataset, at International Conference on Computer, Electrical &amp; Communication Engineering, Techno India University, Kolkata, December 23, 2017. | Co-presented our paper A Novel Transfer Learning-Based Missing Value Imputation on Discipline Diverse Real Test Datasets‚ÄîA Comparative Study with Different Machine Learning Algorithms at International Conference on Emerging Technologies in Data Mining and Information Security, University of Engineering and Management, Kolkata, February 23, 2018. | Spoke on Cyclical Learning Rates for training Neural Nets at DevFest Kolkata, November 3, 2018. | Conducted a hack-session on Cyclical Learning Rates at DataHack Summit (organized by Analytics Vidhya), Bangalore, November 23, 2018. | Delivered talks on Introduction to BigQuery at GDG Kolkata Cloud Study Jam (Academy of Technology), Google Cloud Next ‚Äò19 Extended - Kolkata on April 12 and April 19, 2019 respectively. | Conducted a session on Ten Updates Introduced in TensorFlow 2.0 along with a short quiz at Google I/O Extended 2019, Kolkata, May 11, 2019. | Conducted a session on Training neural nets: A methodical approach at ML/AR Developer Day organized by GDG Kolkata and DSC HIT (May 30, 2019). Conducted the same session but in a more detailed manner at ML With The Experts - GDG Kolkata Meetup (July 7, 2019). | . | Spoke at Google I/O Extended 2019, Bhubaneswar on Ten Updates Introduced in TensorFlow 2.0, June 9, 2019. Also shared a few opportunities with the students (link to the Opportunities‚Äô deck). | Spoke at DevFest Kolkata 2019 about how to approach the process of model deployment, August 3, 2019. My talk was titled Connecting Flutter with TensorFlow 2.0. Link to the slides, video and the GitHub repository. | Spoke at DevFest Jaipur 2019 (September 08, 2019) on Structuring Machine Learning Projects. Remotely presented on this topic at DevFest Izmir 2019 (November 23, 2019). Here‚Äôs the modified deck. Here‚Äôs a recording of the session. | . | Spoke at Explore ML Academy on Problem Framing and How to find data set and fairness practices, September 14, 2019, Hyderabad. | Spoke at DevFest Bhubaneswar 2019 (September 22, 2019) on The Human Loop in Machine Learning. | Spoke at DevFest Goa 2019 (September 29, 2019) on Training Neural Nets: a Hacker‚Äôs Perspective. Spoke at Class III of Launchpad Accelerator India (October 16, 2019), Bangalore on an extended version of the same topic. Deck: http://bit.ly/LPA_3. | Remotely presented on this topic at DevFest Warsaw &amp; Radzymin 2019 (December 7, 2019). | . | Presented my work on Blood Cell Detection using TensorFlow Object Detection API at TensorFlow Roadshow, Bangalore (October 01, 2019). Deck: http://bit.ly/tf-roadshow-sayak. | Remotely presented my work on Predicting Publisher‚Äôs Names from Hackernews Article Titles at Global GDE Summit (October 26, 2019). Video available here (Courtesy: Akshay Bahadur). Deck: http://bit.ly/GDESummit19. | Remotely presented at Machine Learning Weekend, Turkey on Building data pipelines with tf.data (November 3, 2019). | Presented at Kaggle Days Mumbai on On the learning dynamics of neural nets (November 30, 2019). | Conducted a workshop on Applied Deep Learning using TensorFlow 2.0 and GCP (includes topics like data pipeline optimization, cyclical learning rates, mixed-precision training and so on) at Launchpad India Accelerator Bootcamp (December 12 - 13, 2019). Content available here: http://bit.ly/mlb-code-sayak. | Spoke at DevLoop on Your first machine learning project, Ganpat University, Gujrat, India (January 04, 2020). Deck: http://bit.ly/dloop20. I have spoken about this topic at multiple occasions. A session recording is available here. | Spoke at Improving machine learning model on Weights and Biases for better machine learning, Bangalore (February 08, 2020). Deck: http://bit.ly/blr-wb. | Spoke at Sigma 2020 on Machine Learning: For the Community by the Community, Kolkata (February 12, 2020). | Spoke at MENA Digital Days 2020 on Building data pipelines with tf.data. Deck here, session video here. | Spoke for GDG Goa at an online event on Hello, TensorFlow. Deck here, session here. | Spoke for GDG Pune and WTM Pune on Doing more with TensorFlow Lite, April 26, 2020. A session recording is available here (it was for the Deep Learning Salon hosted by Weights and Biases). | Spoke for Global AI Hub, Turkey on Gotchas of Transfer Learning for Image Classification, May 01, 2020. A recording of the session is available here. | Spoke on TensorFlow Hub: Models, Models, and Models for TFUG Hyderabad on May 03, 2020. Deck: http://bit.ly/tf-hub. A recording of the session is available here. | Participated in an ML fire-side chat hosted by Aniedi to speak to the developers of African regions. Recording is available here (May 30, 2020). | Spoke at Pie and AI Kolkata on Becoming One with the Data. Deck here &amp; session recording here (May 31, 2020). | Spoke on Model Optimization 101 for TFUG Thrissur. Deck here (June 07, 2020). A recording on the same topic from a different event is available here. I have also presented this at Google for Startups Accelerator India (GoogleDevsIN tweet). A recording of that session is available here. | Spoke to Tim Scarfe, Connor Shorten, and Yannic Kilcher at the Machine Learning Street Talk: https://bit.ly/ml-st-sayak. | Spoke on A few good stuff in TensorFlow Lite for GDG Berlin (July 16, 2020). Deck is available here and a recording is available here. | Spoke to Mathilde Caron (Research Assistant at Facebook AI), with Ayush Thakur, Tim Scarfe, and Yannic Kilcher at the Machine Learning Street Talk: http://bit.ly/mlst-mathilde. | Spoke on GitHub Actions for Machine Learning at Global AI On Tour Mumbai (September 27, 2020). Deck is available here. | Spoke on The Maker Philosophy with ML APIs at Mindhack Summit (October 12, 2020). Deck is available here. | Spoke on Adversarial Examples in Deep Learning at DevFest UK &amp; Ireland 2020 (October 17, 2020). Deck is available here. | Spoke to Sara Hooker (Google Brain), with Tim Scarfe, and Yannic Kilcher at the Machine Learning Street Talk: bit.ly/sara_mlst. | Spoke to Sanyam Bhutani (H2O.ai), with Tim Scarfe, Yannic Kilcher, and Alex Stenlake at the Machine Learning Street Talk: bit.ly/sanyam_mlst. | Took a workshop on Adversarial Robustness in Deep Learning with Dipanjan Sarkar at Deep Learning DevCon 2020 (October 29, 2020). The materials are available here - bit.ly/adv_learn. | Spoke on Demystifying Self-Supervised Learning for Visual Recognition at SciPy Japan 2020 (October 30, 2020). Deck is available here - bit.ly/scipy-sp. Session recording is available here. | Spoke to Santosh on Demystifying ML and AI for beginners at his podcast Tech Talks With Santosh: https://youtu.be/BhLQN-XIO04 (November 07, 2020). | Spoke with Simon Kornblith (Google Brain) at the Machine Learning Street Talk (with Tim Scarfe and Yannic Kilcher). | . Co-organized by me: . DevFest Kolkata, August 3, 2019. | TensorFlow All-Around Kolkata, August 31, 2019. | Let‚Äôs Build, January 4, 2020. | Kolkata Kreate, February 29, 2020. | TFUG India Summit, September 3 - 6, 2020. | . Note: If you are interested to invite me as a speaker for your event, please get in touch by dropping an email at spsayakpaul@gmail.com. If you are interested in having me submit a CFP first, that is absolutely fine! Please don‚Äôt hesitate to ask that. .",
          "url": "https://sayak.dev/talksseminarsworkshops/",
          "relUrl": "/talksseminarsworkshops/",
          "date": ""
      }
      
  

  
      ,"page11": {
          "title": "XYZ",
          "content": "Taught under-privileged children and managed operations for a TCS-CSR initiative called H20 (Helping Hand Organization) | Moderator of the Artificial Intelligence channel of Campus Commune | Book reviewer at Manning Publications Co | Co-organizer and organizer of GDG Kolkata and TensorFlow User Group Kolkata respectively | Mentorship: Launchpad Women Entrepreneurs | Explore ML Academy, Hyderabad | AI Hack Tunisia | Class III (2019) of Launchpad Accelerator India (Tweet by GoogleDevsIN) | Google Code-in for TensorFlow through December, 2019 - January 2020 (Certificate) | Build For Digital India Bootcamps through January 2020 - February 2020 (Tweet by GoogleDevsIN) | Explore ML Bootcamp, Hyderabad (Tweet by GoogleDevsIN) | Mentored at Google for Startups India Accelerator Class 4 | . | Program committe member, ML Conf EU | Recepient of the Google Open Source Peer Bonus Award. Here‚Äôs a related blog post (from the official Google Open Source Blog) jotting down the experiences that led to this honor. | . Non-tech: . I love listening to all genres of music. A guitar player myself. Have played in a band Behest from 2013 to 2017. | I love watching TV serieses also (Narcos, Suits, Fringe, Seal Team, Ozark being all time favorites). | .",
          "url": "https://sayak.dev/xyz/",
          "relUrl": "/xyz/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
      ,"page15": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://sayak.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}