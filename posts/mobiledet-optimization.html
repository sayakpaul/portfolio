<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.53">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2020-09-29">
<meta name="description" content="Learn about the criticalities of effectively optimizing MobileDet object detectors for mobile deployments.">

<title>Optimizing MobileDet for Mobile Deployments – Sayak Paul</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<link href="../posts/favicon.ico" rel="icon">
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-163448909-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>


<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="Optimizing MobileDet for Mobile Deployments – Sayak Paul">
<meta property="og:description" content="Learn about the criticalities of effectively optimizing MobileDet object detectors for mobile deployments.">
<meta property="og:image" content="https://sayak.dev/posts/benchmark.png">
<meta property="og:site_name" content="Sayak Paul">
<meta property="og:image:height" content="508">
<meta property="og:image:width" content="1187">
<meta name="twitter:title" content="Optimizing MobileDet for Mobile Deployments – Sayak Paul">
<meta name="twitter:description" content="Learn about the criticalities of effectively optimizing MobileDet object detectors for mobile deployments.">
<meta name="twitter:image" content="https://sayak.dev/posts/benchmark.png">
<meta name="twitter:creator" content="@RisingSayak">
<meta name="twitter:card" content="summary">
<meta name="twitter:image-height" content="508">
<meta name="twitter:image-width" content="1187">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Sayak Paul</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../pages/about.html"> 
<span class="menu-text">About Me</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../pages/authoring.html"> 
<span class="menu-text">Authoring</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../pages/research.html"> 
<span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../pages/resources.html"> 
<span class="menu-text">Resources</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/RisingSayak"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/sayak-paul/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/sayakpaul"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Optimizing MobileDet for Mobile Deployments</h1>
                  <div>
        <div class="description">
          Learn about the criticalities of effectively optimizing MobileDet object detectors for mobile deployments.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">tflite</div>
                <div class="quarto-category">model-optimization</div>
                <div class="quarto-category">mobiledet</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 29, 2020</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#why-yet-another-post-on-model-conversion" id="toc-why-yet-another-post-on-model-conversion" class="nav-link active" data-scroll-target="#why-yet-another-post-on-model-conversion">Why yet another post on model conversion?</a></li>
  <li><a href="#the-hassle-free-conversions" id="toc-the-hassle-free-conversions" class="nav-link" data-scroll-target="#the-hassle-free-conversions">The hassle-free conversions</a></li>
  <li><a href="#the-trickier-tflite-conversions-for-mobiledet" id="toc-the-trickier-tflite-conversions-for-mobiledet" class="nav-link" data-scroll-target="#the-trickier-tflite-conversions-for-mobiledet">The trickier TFLite conversions for MobileDet</a>
  <ul class="collapse">
  <li><a href="#representative-dataset" id="toc-representative-dataset" class="nav-link" data-scroll-target="#representative-dataset">Representative dataset</a></li>
  <li><a href="#dealing-with-fake-quantization-nodes-during-conversion" id="toc-dealing-with-fake-quantization-nodes-during-conversion" class="nav-link" data-scroll-target="#dealing-with-fake-quantization-nodes-during-conversion">Dealing with fake quantization nodes during conversion</a></li>
  <li><a href="#integer-quantization-for-cpu-variants-and-float32-precision-models" id="toc-integer-quantization-for-cpu-variants-and-float32-precision-models" class="nav-link" data-scroll-target="#integer-quantization-for-cpu-variants-and-float32-precision-models">Integer quantization for CPU variants and float32 precision models</a></li>
  <li><a href="#dealing-with-non-integer-postprocessing-ops-during-conversion" id="toc-dealing-with-non-integer-postprocessing-ops-during-conversion" class="nav-link" data-scroll-target="#dealing-with-non-integer-postprocessing-ops-during-conversion">Dealing with non-integer postprocessing ops during conversion</a></li>
  </ul></li>
  <li><a href="#show-me-some-results" id="toc-show-me-some-results" class="nav-link" data-scroll-target="#show-me-some-results">Show me some results</a></li>
  <li><a href="#model-benchmarks" id="toc-model-benchmarks" class="nav-link" data-scroll-target="#model-benchmarks">Model benchmarks</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.dev/sayakpaul/portfolio/blob/master/posts/2020-09-29-mobiledet-optimization.ipynb" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/sayakpaul/portfolio/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>This year researchers from the University of Wisconsin-Madison and Google published their work on <a href="https://arxiv.org/abs/2004.14525">MobileDet</a>. MobileDet presents an architectural philosophy for designing object detectors specifically targeted toward running on mobile accelerators like DSP, EdgeTPU, and so on. MobileDet yields significant improvement over architectures MobileNetV2+SSDLite and MobileNetV3+SSDLite on the <a href="https://cocodataset.org/">COCO object detection task</a> with the same accelerated inference time. Long story cut short, if you are planning to use object detection models in mobile applications MobileDets may be an extremely good choice.</p>
<p>One fantastic thing about modern-day research is most of the time, the code and essential artifacts (like the trained models) are available publicly. MobileDet is no exception; the authors released their code and pre-trained models in <a href="https://github.com/tensorflow/models/tree/master/research/object_detection">TensorFlow Object Detection (TFOD) API</a>. The model files come in three different variants -</p>
<ul>
<li>Optimized for mobile CPU</li>
<li>Optimized for EdgeTPU</li>
<li>Optimized for DSP</li>
</ul>
<p>Each of these variants includes the pre-trained checkpoints, a TensorFlow Lite (TFLite) compatible model graph, a TFLite model file, a configuration file, and a graph proto. The models were pre-trained on the COCO dataset.</p>
<p>In this post, I am going to be revisiting the TFLite conversion from the pre-trained model checkpoints along with some of the non-trivial things that come up during the process. It is basically an extension of <a href="https://twitter.com/khanhlvg?lang=en">Khanh LeViet</a> and my findings we shared over <a href="https://github.com/ml-gde/e2e-tflite-tutorials/issues/21">this GitHub thread</a>.</p>
<p>The code discussed throughout this post is available <a href="https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/MobileDet_Conversion_TFLite.ipynb">here as a Colab Notebook</a>.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you want to train MobileDet models on your own dataset you may find <a href="https://github.com/sayakpaul/E2E-Object-Detection-in-TFLite/tree/master/colab_training">these notebooks</a> useful. They show you how to prepare the dataset, fine-tune a MobileDet model with the dataset, and optimize the fine-tuned model with TFLite.</p>
</div>
</div>
<section id="why-yet-another-post-on-model-conversion" class="level2">
<h2 class="anchored" data-anchor-id="why-yet-another-post-on-model-conversion">Why yet another post on model conversion?</h2>
<p>Fair question. After all, there are so many great examples and tutorials that show how to use the <a href="https://www.tensorflow.org/lite/performance/post_training_quantization"><u>post-training quantization APIs</u></a> in TFLite to perform the model conversion. MobileDet models in the TFOD API repository were trained in TensorFlow (TF) 1. If you ever wanted to use the latest TFLite converter to do the conversion, that may not be immediately approachable.</p>
<p>Besides, there are certain caveats to the EdgeTPU and DSP variants. They come in two precision formats - <code>uint8</code> and <code>float32</code>. The models in <code>uint8</code> precision were trained using <a href="https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html"><u>quantization aware training</u></a> (QAT) while the <code>float32</code> models were not. During QAT fake quantization nodes get inserted into a model’s computation graph. So, the models trained using QAT usually require some extra care during the TFLite conversion process as we’ll see in a moment.</p>
<p>If we wanted to convert a single shot detector (SSD) based model to TFLite then we first need to generate a frozen graph first that is compatible with the TFLite operator set (as per these guides - <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md"><u>TF1</u></a> and <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md"><u>TF2</u></a>). The TFOD API team provides stock scripts (<a href="https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_ssd_graph.py"><u>TF1</u></a>, <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_graph_tf2.py"><u>TF2</u></a>) for this. Both of these scripts add optimized postprocessing operations to the model graph. Now, these operations are not yet supported in int8 precision. So, if you ever wanted to convert these pre-trained checkpoints using <a href="https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization"><u>full integer quantization</u></a>, what would have been your approach?</p>
<p>By now, hopefully, I have been able to convince you that this post is not just about regular model conversion in TFLite. The situations we’ll be going through over the next sections may be helpful for your production TFLite models as well.</p>
</section>
<section id="the-hassle-free-conversions" class="level2">
<h2 class="anchored" data-anchor-id="the-hassle-free-conversions">The hassle-free conversions</h2>
<p>Before we build our way toward the fun stuff, let’s start with the conversions that won’t cost us a night’s sleep. Conversions based on <a href="https://www.tensorflow.org/lite/performance/post_training_quant"><u>dynamic-range</u></a> and <a href="https://www.tensorflow.org/lite/performance/post_training_float16_quant">float16</a> quantization would come under this category.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>The EdgeTPU and DSP variants of MobileDet are meant to run on the respective hardware accelerators. These accelerators need a model to be in full integer precision. So converting the EdgeTPU and DSP variants with dynamic-range and <code>float16</code> quantization does not have any practical usage.</p>
</div>
</div>
<p>So, for dynamic-range and <code>float16</code> quantization based conversions, we will be using the CPU variant only. This variant is available <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md#mobile-models">here</a> as <code>ssd_mobiledet_cpu_coco</code>. Once the model bundle is untar’d we get the following files -</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> model.ckpt-400000.data-00000-of-00001</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> model.ckpt-400000.index</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> model.ckpt-400000.meta</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> model.tflite</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> pipeline.config</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> tflite_graph.pb</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="ex">└──</span> tflite_graph.pbtxt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><code>model.ckpt-*</code> files are the pre-trained checkpoints on the COCO dataset. If you train a MobileDet object detection model on your own dataset, you will have your own model checkpoint files. The <code>tflite_graph.pb</code> file is a frozen inference graph that is compatible with the TFLite operator set, which was exported from the pre-trained model checkpoints. <code>model.tflite</code> file is a TFLite model that was converted from the <code>tflite_graph.pb</code> frozen graph.</p>
<p>In case if you ever train a MobileDet model on your dataset, here’s how you’d get the TFLite frozen graph file (based on <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md"><u>this guide</u></a> mentioned above) -</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> PIPELINE_CONFIG=<span class="st">"checkpoint_name/pipeline.config"</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> CKPT_PREFIX=<span class="st">"checkpoint_name/model.ckpt-400000"</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> OUTPUT_DIR=<span class="st">"tflite_graph"</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> python models/research/object_detection/export_tflite_ssd_graph.py <span class="dt">\</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>   <span class="at">--pipeline_config_path</span><span class="op">=</span><span class="va">$PIPELINE_CONFIG</span> <span class="dt">\</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>   <span class="at">--trained_checkpoint_prefix</span><span class="op">=</span><span class="va">$CKPT_PREFIX</span> <span class="dt">\</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>   <span class="at">--output_directory</span><span class="op">=</span><span class="va">$OUTPUT_DIR</span> <span class="dt">\</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>   <span class="at">--add_postprocessing_op</span><span class="op">=</span>true</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>You can see a fully worked out example in the <a href="https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/MobileDet_Conversion_TFLite.ipynb"><u>Colab Notebook</u></a> mentioned above. If everything goes well, then you should have the frozen graph file exported in <code>OUTPUT_DIR</code>. Let’s now proceed to the TFLite model conversion part.</p>
<p>Here’s how the dynamic-range quantization would look like in TensorFlow 2 -</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>converter <span class="op">=</span> tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    graph_def_file<span class="op">=</span>model_to_be_quantized,</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    input_arrays<span class="op">=</span>[<span class="st">'normalized_input_image_tensor'</span>],              </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    output_arrays<span class="op">=</span>[<span class="st">'TFLite_Detection_PostProcess'</span>,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">'TFLite_Detection_PostProcess:1'</span>,</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">'TFLite_Detection_PostProcess:2'</span>,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">'TFLite_Detection_PostProcess:3'</span>],</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>   input_shapes<span class="op">=</span>{<span class="st">'normalized_input_image_tensor'</span>: [<span class="dv">1</span>, <span class="dv">320</span>, <span class="dv">320</span>, <span class="dv">3</span>]}</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>converter.optimizations <span class="op">=</span> [tf.lite.Optimize.DEFAULT]</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>tflite_model <span class="op">=</span> converter.convert()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>A note about some of the parameters and their values from the above code listing -</p>
<ul>
<li><p><code>model_to_be_quantized</code> corresponds to the frozen graph file.</p></li>
<li><p><code>input_arrays</code> and <code>input_shapes</code> are set accordingly with respect to the frozen graph file. As we can see in the figure below that these values have been set correctly.</p>
<p><img src="https://i.ibb.co/F4xGRJB/image2.png" class="img-fluid"></p></li>
<li><p><code>output_arrays</code> is set according to the instructions provided in <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_ssd_graph.py"><u>this guide</u></a>. Those operations represent four arrays: <code>detection_boxes</code>, <code>detection_classes</code>, <code>detection_scores</code>, and <code>num_detections</code>, usually a mandate for any object detector out there.</p></li>
</ul>
<p>The rest of the parts in the code listing should be familiar to you if you already know about the typical post-training quantization process in TFLite. For <code>float16</code> quantization, all the things would remain the same; we just need to add this line before calling <code>convert()</code> - <code>converter.target_spec.supported_types = [tf.float16]</code>.</p>
<p>The dynamic-range quantized model is <strong>4.3 MB</strong> in size and <code>float16</code> one is <strong>8.2 MB</strong>. Later, we will see how fast this model would run on actual mobile devices with and without different accelerators.</p>
</section>
<section id="the-trickier-tflite-conversions-for-mobiledet" class="level2">
<h2 class="anchored" data-anchor-id="the-trickier-tflite-conversions-for-mobiledet">The trickier TFLite conversions for MobileDet</h2>
<p>In this section, we will be dealing with the full integer quantization for the three different variants of MobileDet. Full integer quantization is usually more involved than the other quantization formats supported by TFLite.</p>
<section id="representative-dataset" class="level3">
<h3 class="anchored" data-anchor-id="representative-dataset">Representative dataset</h3>
<p>Our first step toward doing full integer quantization is preparing a representative dataset. It is required to calibrate the activation ranges so that the quantized model is able to retain the original model performance as much as possible. For the purpose of this post, I sampled 100 images from the <a href="https://cocodataset.org/#download"><u>COCO training dataset</u></a> (<code>train2014</code> split). In my experience, 100 samples as the representative dataset have always been sufficient. I have hosted these images <a href="https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite/releases/tag/v0.9.0"><u>here</u></a> in case you are interested to use them.</p>
<p>The following code listing denotes a generator function that produces a preprocessed image to the TFLite converter -</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>rep_ds <span class="op">=</span> tf.data.Dataset.list_files(<span class="st">"train_samples/*.jpg"</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>HEIGHT, WIDTH <span class="op">=</span> <span class="dv">320</span>, <span class="dv">320</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> representative_dataset_gen():</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>   <span class="cf">for</span> image_path <span class="kw">in</span> rep_ds:</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>       img <span class="op">=</span> tf.io.read_file(image_path)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>       img <span class="op">=</span> tf.io.decode_image(img, channels<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>       img <span class="op">=</span> tf.image.convert_image_dtype(img, tf.float32)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>       resized_img <span class="op">=</span> tf.image.resize(img, (HEIGHT, WIDTH))</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>       resized_img <span class="op">=</span> resized_img[tf.newaxis, :]</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>       <span class="cf">yield</span> [resized_img]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Note</strong> that these preprocessing steps should be in sync with the actual preprocessing steps that would apply before running inference with your TFLite model. In case if you are interested to know about more complex representative dataset generators you may find <a href="https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/Magenta_arbitrary_style_transfer_model_conversion.ipynb"><u>this notebook</u></a> useful.</p>
<p>Also, note that dynamic-range and <code>float16</code> quantization of the EdgeTPU and DSP variants don’t have much of practical usage. The next section is going to be solely about full integer quantization of these different variants and the nitty-gritty to take into consideration for the conversion process.</p>
</section>
<section id="dealing-with-fake-quantization-nodes-during-conversion" class="level3">
<h3 class="anchored" data-anchor-id="dealing-with-fake-quantization-nodes-during-conversion">Dealing with fake quantization nodes during conversion</h3>
<p>The figure below represents a portion of the <code>uint8</code> EdgeTPU model computation graph. The nodes highlighted in red are inserted by the QAT mechanism. You would notice the same kind of nodes in the <code>uint8</code> DSP model computation graph as well.</p>
<p><img src="https://i.ibb.co/B2qXzsf/image1.png" class="img-fluid"></p>
<p>Now, these nodes have some important implications that we need to consider during the conversion process -</p>
<ul>
<li>During QAT the activation ranges are already approximated i.e.&nbsp;QAT resembles post-training quantization during training and adjusts the activation ranges accordingly. So, we don’t need to provide a representative dataset for a full integer quantization based conversion.</li>
<li>These fake nodes are generally in integer precision. So, setting an optimization option (<code>converter.optimizations</code>) might lead to inconsistencies.</li>
<li>In order to convert the <code>uint8</code> models with full integer quantization, we need to set the input and output data type of the TFLite models to integer precision (typically <code>uint8</code> or <code>int8</code>). As per <a href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/lite/TFLiteConverter#attributes"><u>this documentation</u></a>, we also need to specify the <code>quantized_input_stats</code> parameter during the conversion process. This is needed in order for the converted TFLite model to map the quantized input values to real values. More details are available <a href="https://www.tensorflow.org/lite/performance/quantization_spec"><u>here</u></a>.</li>
</ul>
<p>So, how do we realize all of these in code?</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>converter <span class="op">=</span> tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>   graph_def_file<span class="op">=</span>model_to_be_quantized,</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>   input_arrays<span class="op">=</span>[<span class="st">'normalized_input_image_tensor'</span>],</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>   output_arrays<span class="op">=</span>[<span class="st">'TFLite_Detection_PostProcess'</span>,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>       <span class="st">'TFLite_Detection_PostProcess:1'</span>,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>       <span class="st">'TFLite_Detection_PostProcess:2'</span>,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>       <span class="st">'TFLite_Detection_PostProcess:3'</span>],</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>   input_shapes<span class="op">=</span>{<span class="st">'normalized_input_image_tensor'</span>: [<span class="dv">1</span>, <span class="dv">320</span>, <span class="dv">320</span>, <span class="dv">3</span>]}</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>converter.inference_input_type <span class="op">=</span> tf.uint8</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>converter.quantized_input_stats <span class="op">=</span> {<span class="st">"normalized_input_image_tensor"</span>: (<span class="dv">128</span>, <span class="dv">128</span>)}</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>tflite_model <span class="op">=</span> converter.convert()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If you’re thinking this does not look all that gory compared to the above code listing - it does not have to be! The tooling should help you do these things seamlessly. But catching these details during your project development may not be trivial. Note that we don’t specify <code>converter.inference_output_type</code>. Hold your breath, we will come to this in a moment.</p>
<p>After successful execution, we get two full integer quantized models - EdgeTPU one is <strong>4.2 MB</strong> and the DSP one is <strong>7.0 MB</strong>.</p>
</section>
<section id="integer-quantization-for-cpu-variants-and-float32-precision-models" class="level3">
<h3 class="anchored" data-anchor-id="integer-quantization-for-cpu-variants-and-float32-precision-models">Integer quantization for CPU variants and float32 precision models</h3>
<p>The variants that don’t contain fake quantization nodes (CPU and all the models in <code>float32</code> precision) have a <em>relatively</em> simpler conversion process. Recollect that the EdgeTPU and DSP variants come in two different precisions - <code>uint8</code> and <code>float32</code>. For example, here’s how it would be for the <code>float32</code> precision models -</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>converter.representative_dataset <span class="op">=</span> representative_dataset_gen</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>converter.inference_input_type <span class="op">=</span> tf.uint8</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>converter.optimizations <span class="op">=</span> [tf.lite.Optimize.DEFAULT]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Note that we are specifying a representative dataset here because the <code>float32</code> precision models weren’t trained using QAT. For the CPU variant model, the lines of code would slightly change -</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>converter.inference_input_type <span class="op">=</span> tf.uint8</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>converter.quantized_input_stats <span class="op">=</span> {<span class="st">"normalized_input_image_tensor"</span>: (<span class="dv">128</span>, <span class="dv">128</span>)}</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>converter.optimizations <span class="op">=</span> [tf.lite.Optimize.DEFAULT]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Honestly, I found this configuration by trial and error. I observed that if I specify a representative dataset then it hurts the predictions of the converted model. Also, I found out that specifying <code>converter.quantized_input_stats</code> helped improve the predictions of the converted model.</p>
<p>We don’t specify <code>converter.inference_output_type</code> in this case as well. Let’s get to it now.</p>
</section>
<section id="dealing-with-non-integer-postprocessing-ops-during-conversion" class="level3">
<h3 class="anchored" data-anchor-id="dealing-with-non-integer-postprocessing-ops-during-conversion">Dealing with non-integer postprocessing ops during conversion</h3>
<p>Remember that frozen graph exporter scripts provided by the TFOD API team add optimized postprocessing operations to the graph. These operations are not supported in integer precision yet. So, even if you wanted to specify <code>converter.inference_output_type</code> as <code>tf.uint8</code> you’ll likely get the following error -</p>
<pre><code>RuntimeError: Unsupported output type UINT8 for output tensor 'TFLite_Detection_PostProcess' of type FLOAT32.</code></pre>
<p>This is why we did not set the <code>converter.inference_output_type</code> parameter.</p>
<p>This should resolve all the problems you may run into if you ever wanted to convert the MobileDet models offered by the TFOD API team. In the last two sections, we’ll see these converted models in action and how fast they can perform on respective hardware accelerators.</p>
</section>
</section>
<section id="show-me-some-results" class="level2">
<h2 class="anchored" data-anchor-id="show-me-some-results">Show me some results</h2>
<p>For the CPU variant model, its <code>float16</code> quantized TFLite provided decent results -</p>
<p><img src="https://i.ibb.co/k6c93CC/image3.png" class="img-fluid"></p>
<p>On Colab, the inference time is about <strong>92.36 ms</strong> for this particular model. I experimented with different threshold values for filtering out the weak predictions and a threshold of <strong>0.3</strong> yielded the best results. These results are pretty consistent across the several different models we talked about.</p>
<p>A major point to note here for the EdgeTPU and DSP variants, their converted counterparts would be much slower on Colab since they were specifically optimized for different hardware accelerators.</p>
<p>You are encouraged to play with the different converted models using the Colab Notebook mentioned above and see these results for yourself.</p>
</section>
<section id="model-benchmarks" class="level2">
<h2 class="anchored" data-anchor-id="model-benchmarks">Model benchmarks</h2>
<p>In this section, we’ll address the question - “So, how do I choose one among these many models?” Well, you could manually try them all out and see which performs the best on the runtime of your choice. But a more practical approach to this would be to first benchmark these models on a set of devices using the <a href="https://www.tensorflow.org/lite/performance/measurement"><u>TFLite Benchmark Tool</u></a> and then decide accordingly.</p>
<p>The following table provides a comprehensive summary of the important statistics about the runtime of different TFLite MobileDet models. These results were generated using the TFLite Benchmark Tool mentioned above.</p>
<img src="https://i.ibb.co/jrKshwB/image.png" class="img-fluid">
<center>
<small>* Device used - Pixel 4 (Inference timings are reported in milliseconds)</small><br> <small>** As reported <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md">here</a></small>
</center>
<p>We can see that with the proper hardware accelerators, the DSP EdgeTPU variants can really shine. For the CPU variant, on a GPU accelerated runtime the <code>float16</code> quantized TFLite model can bring in additional speed boosts.</p>
<p>A catch here is Pixel devices don’t allow third-party applications to use the Hexagon DSP therefore even if we instruct the Benchmark Tool to make use of that the model would fall back to the CPU for execution. This is why for fair benchmarking results for the DSP variants we should consider running the Benchmark Tool on a device (such as Samsung Galaxy S9+) that has Hexagon DSP and also allows third-party applications to use it.</p>
<img src="https://i.ibb.co/mHkyfpd/image.png" class="img-fluid">
<center>
<small>* Device used - Samsung Galaxy S9+ (Inference timings are reported in milliseconds)</small>
</center>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>To train a custom MobileDet-based object detector you can refer to <a href="https://github.com/sayakpaul/E2E-Object-Detection-in-TFLite/tree/master/colab_training">these notebooks</a>.</p>
</div>
</div>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>In this post, we discussed some of the intricate problems one may run into while converting different variants of the MobileDet model in TFLite. One aspect about TFLite that I really like is how it provides the tooling needed to deal with practical problems like this.</p>
<p>I am thankful to Khanh for thoroughly guiding me while writing this post. Thanks to <a href="https://sg.linkedin.com/in/martinandrews">Martin Andrews</a> for suggesting textual edits.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/sayak\.dev\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="sayakpaul/portfolio" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.dev/sayakpaul/portfolio/blob/master/posts/2020-09-29-mobiledet-optimization.ipynb" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/sayakpaul/portfolio/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>