@inproceedings{
    lipman2023flow,
    title={Flow Matching for Generative Modeling},
    author={Yaron Lipman and Ricky T. Q. Chen and Heli Ben-Hamu and Maximilian Nickel and Matthew Le},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=PqvMRDCJT9t}
}

@inproceedings{NEURIPS2020_4c5bcfec,
    author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
    pages = {6840--6851},
    publisher = {Curran Associates, Inc.},
    title = {Denoising Diffusion Probabilistic Models},
    url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
    volume = {33},
    year = {2020}
}

@inproceedings{NIPS2014_f033ed80,
    author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Generative Adversarial Nets},
    url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/f033ed80deb0234979a61f95710dbe25-Paper.pdf},
    volume = {27},
    year = {2014}
}

@online{blackforestlabs2024announcing,
    title        = {Announcing Black Forest Labs},
    author       = {{Black Forest Labs}},
    year         = {2024},
    month        = aug,
    day          = {1},
    url          = {https://blackforestlabs.ai/announcing-black-forest-labs/},
    note         = {Accessed on \today}
}

@inproceedings{NIPS2017_3f5ee243,
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Attention is All you Need},
    url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
    volume = {30},
    year = {2017}
}

@InProceedings{pmlr-v235-esser24a,
    title = 	 {Scaling Rectified Flow Transformers for High-Resolution Image Synthesis},
    author =       {Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M\"{u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and Podell, Dustin and Dockhorn, Tim and English, Zion and Rombach, Robin},
    booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
    pages = 	 {12606--12633},
    year = 	 {2024},
    editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
    volume = 	 {235},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {21--27 Jul},
    publisher =    {PMLR},
    pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/esser24a/esser24a.pdf},
    url = 	 {https://proceedings.mlr.press/v235/esser24a.html},
    abstract = 	 {Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models. Stability AI is considering making experimental data, code, and model weights publicly available.}
}

@InProceedings{Rombach_2022_CVPR,
    author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\"orn},
    title     = {High-Resolution Image Synthesis With Latent Diffusion Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {10684-10695}
}

@misc{warner2024smarterbetterfasterlonger,
    title={Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference}, 
    author={Benjamin Warner and Antoine Chaffin and Benjamin Clavié and Orion Weller and Oskar Hallström and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Iacopo Poli},
    year={2024},
    eprint={2412.13663},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2412.13663}, 
}

@InProceedings{Xiong_2021_CVPR,
    author    = {Xiong, Yunyang and Liu, Hanxiao and Gupta, Suyog and Akin, Berkin and Bender, Gabriel and Wang, Yongzhe and Kindermans, Pieter-Jan and Tan, Mingxing and Singh, Vikas and Chen, Bo},
    title     = {MobileDets: Searching for Object Detection Architectures for Mobile Accelerators},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {3825-3834}
}

@misc{bercovich2025puzzledistillationbasednasinferenceoptimized,
    title={Puzzle: Distillation-Based NAS for Inference-Optimized LLMs}, 
    author={Akhiad Bercovich and Tomer Ronen and Talor Abramovich and Nir Ailon and Nave Assaf and Mohammad Dabbah and Ido Galil and Amnon Geifman and Yonatan Geifman and Izhak Golan and Netanel Haber and Ehud Karpas and Roi Koren and Itay Levy and Pavlo Molchanov and Shahar Mor and Zach Moshe and Najeeb Nabwani and Omri Puny and Ran Rubin and Itamar Schen and Ido Shahaf and Oren Tropp and Omer Ullman Argov and Ran Zilberstein and Ran El-Yaniv},
    year={2025},
    eprint={2411.19146},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2411.19146}, 
}

@inproceedings{10.5555/3600270.3602446,
    author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Vinyals, Oriol and Rae, Jack W. and Sifre, Laurent},
    title = {Training compute-optimal large language models},
    year = {2022},
    isbn = {9781713871088},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {We investigate the optimal model size and number of tokens for training a Transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\texttimes{} more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
    booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
    articleno = {2176},
    numpages = {15},
    location = {New Orleans, LA, USA},
    series = {NIPS '22}
}

@misc{hacohen2024ltxvideorealtimevideolatent,
    title={LTX-Video: Realtime Video Latent Diffusion}, 
    author={Yoav HaCohen and Nisan Chiprut and Benny Brazowski and Daniel Shalem and Dudu Moshe and Eitan Richardson and Eran Levin and Guy Shiran and Nir Zabari and Ori Gordon and Poriya Panet and Sapir Weissbuch and Victor Kulikov and Yaki Bitterman and Zeev Melumian and Ofir Bibi},
    year={2024},
    eprint={2501.00103},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2501.00103}, 
}

@inproceedings{
    xie2025sana,
    title={{SANA}: Efficient High-Resolution Text-to-Image Synthesis with Linear Diffusion Transformers},
    author={Enze Xie and Junsong Chen and Junyu Chen and Han Cai and Haotian Tang and Yujun Lin and Zhekai Zhang and Muyang Li and Ligeng Zhu and Yao Lu and Song Han},
    booktitle={The Thirteenth International Conference on Learning Representations},
    year={2025},
    url={https://openreview.net/forum?id=N8Oj1XhtYZ}
}

@online{blackforestlabs2024flux1tools,
    title        = {Introducing FLUX.1 Tools},
    author       = {{Black Forest Labs}},
    year         = {2024},
    month        = nov,
    day          = {1},
    url          = {https://bfl.ai/announcements/24-11-21-tools},
    note         = {Accessed on \today}
}

@online{blackforestlabs2025flux1kontextdev,
    title        = {FLUX.1 Kontext [dev] – Open Weights for Image Editing},
    author       = {{Black Forest Labs}},
    year         = {2025},
    month        = jun,
    day          = {26},
    url          = {https://bfl.ai/announcements/flux-1-kontext-dev},
    note         = {Accessed on \today}
}

@inproceedings{black2023ddpo,
    title={Training Diffusion Models with Reinforcement Learning},
    author={Kevin Black and Michael Janner and Yilun Du and Ilya Kostrikov and Sergey Levine},
    year={2023},
    eprint={2305.13301},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@InProceedings{Wallace_2024_CVPR,
    author    = {Wallace, Bram and Dang, Meihua and Rafailov, Rafael and Zhou, Linqi and Lou, Aaron and Purushwalkam, Senthil and Ermon, Stefano and Xiong, Caiming and Joty, Shafiq and Naik, Nikhil},
    title     = {Diffusion Model Alignment Using Direct Preference Optimization},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {8228-8238}
}

@misc{zhang2023addingconditionalcontroltexttoimage, title={Adding Conditional Control to Text-to-Image Diffusion Models}, author={Lvmin Zhang and Anyi Rao and Maneesh Agrawala}, year={2023}, eprint={2302.05543}, archivePrefix={arXiv}, primaryClass={cs.CV}, url={https://arxiv.org/abs/2302.05543}, }

@misc{dao2022flashattentionfastmemoryefficientexact, title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}, author={Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher Ré}, year={2022}, eprint={2205.14135}, archivePrefix={arXiv}, primaryClass={cs.LG}, url={https://arxiv.org/abs/2205.14135}, }

@misc{shah2024flashattention3fastaccurateattention, title={FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision}, author={Jay Shah and Ganesh Bikshandi and Ying Zhang and Vijay Thakkar and Pradeep Ramani and Tri Dao}, year={2024}, eprint={2407.08608}, archivePrefix={arXiv}, primaryClass={cs.LG}, url={https://arxiv.org/abs/2407.08608}, }

@InProceedings{Ma_2025_CVPR,
    author    = {Ma, Nanye and Tong, Shangyuan and Jia, Haolin and Hu, Hexiang and Su, Yu-Chuan and Zhang, Mingda and Yang, Xuan and Li, Yandong and Jaakkola, Tommi and Jia, Xuhui and Xie, Saining},
    title     = {Scaling Inference Time Compute for Diffusion Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2025},
    pages     = {2523-2534}
}

@misc{zhuo2025reflectionperfectionscalinginferencetime,
    title={From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning}, 
    author={Le Zhuo and Liangbing Zhao and Sayak Paul and Yue Liao and Renrui Zhang and Yi Xin and Peng Gao and Mohamed Elhoseiny and Hongsheng Li},
    year={2025},
    eprint={2504.16080},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2504.16080}, 
}

@misc{singhal2025generalframeworkinferencetimescaling, title={A General Framework for Inference-time Scaling and Steering of Diffusion Models}, author={Raghav Singhal and Zachary Horvitz and Ryan Teehan and Mengye Ren and Zhou Yu and Kathleen McKeown and Rajesh Ranganath}, year={2025}, eprint={2501.06848}, archivePrefix={arXiv}, primaryClass={cs.LG}, url={https://arxiv.org/abs/2501.06848}, }

@misc{betker2023improvingimagegenerationwithbettercaptions,
  title={Improving Image Generation with Better Captions},
  author={James Betker and Heewoo Jun and Long Ouyang and Gabriel Goh and Aditya Ramesh and others},
  year={2023},
  howpublished={OpenAI Technical Report},
  url={https://cdn.openai.com/papers/dall-e-3.pdf}
}

@misc{segalis2023pictureworththousandwords,
    title={A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation}, 
    author={Eyal Segalis and Dani Valevski and Danny Lumen and Yossi Matias and Yaniv Leviathan},
    year={2023},
    eprint={2310.16656},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2310.16656}, 
}

@misc{liu2024playgroundv3improvingtexttoimage,
    title={Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large Language Models}, 
    author={Bingchen Liu and Ehsan Akhgari and Alexander Visheratin and Aleks Kamko and Linmiao Xu and Shivam Shrirao and Chase Lambert and Joao Souza and Suhail Doshi and Daiqing Li},
    year={2024},
    eprint={2409.10695},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2409.10695}, 
}

@inproceedings{10.1007/978-3-031-72670-5_12,
author = {Chatterjee, Agneet and Stan, Gabriela Ben Melech and Aflalo, Estelle and Paul, Sayak and Ghosh, Dhruba and Gokhale, Tejas and Schmidt, Ludwig and Hajishirzi, Hannaneh and Lal, Vasudev and Baral, Chitta and Yang, Yezhou},
title = {Getting it Right: Improving Spatial Consistency in&nbsp;Text-to-Image Models},
year = {2024},
isbn = {978-3-031-72669-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-72670-5_12},
doi = {10.1007/978-3-031-72670-5_12},
abstract = {One of the key shortcomings in current text-to-image (T2I) models is their inability to consistently generate images which faithfully follow the spatial relationships specified in the text prompt. In this paper, we offer a comprehensive investigation of this limitation, while also developing datasets and methods that support algorithmic solutions to improve spatial reasoning in T2I models. We find that spatial relationships are under-represented in the image descriptions found in current vision-language datasets. To alleviate this data bottleneck, we create SPRIGHT, the first spatially focused, large-scale dataset, by re-captioning 6 million images from 4 widely used vision datasets and through a 3-fold evaluation and analysis pipeline, show that SPRIGHT improves the proportion of spatial relationships in existing datasets. We show the efficacy of SPRIGHT data by showing that&nbsp;using only ∼0.25\% of SPRIGHT results in a 22\% improvement in generating spatially accurate images while also improving FID and CMMD scores. We also find that training on images containing a larger number of objects leads to substantial improvements in spatial consistency, including state-of-the-art results on T2I-CompBench with a spatial score of 0.2133, by fine-tuning on <500 images. Through a set of controlled experiments and ablations, we document additional findings that could support future work that seeks to understand factors that affect spatial consistency in text-to-image models. Project page: .},
booktitle = {Computer Vision – ECCV 2024: 18th European Conference, Milan, Italy, September 29 – October 4, 2024, Proceedings,  Part XXII},
pages = {204–222},
numpages = {19},
keywords = {Text to Image Generation, Spatial Relationships},
location = {Milan, Italy}
}

@misc{hinton2015distillingknowledgeneuralnetwork, title={Distilling the Knowledge in a Neural Network}, author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean}, year={2015}, eprint={1503.02531}, archivePrefix={arXiv}, primaryClass={stat.ML}, url={https://arxiv.org/abs/1503.02531}, }

@misc{sreenivas2024llmpruningdistillationpractice, title={LLM Pruning and Distillation in Practice: The Minitron Approach}, author={Sharath Turuvekere Sreenivas and Saurav Muralidharan and Raviraj Joshi and Marcin Chochowski and Ameya Sunil Mahabaleshwarkar and Gerald Shen and Jiaqi Zeng and Zijia Chen and Yoshi Suhara and Shizhe Diao and Chenhan Yu and Wei-Chun Chen and Hayley Ross and Oluwatobi Olabiyi and Ashwath Aithal and Oleksii Kuchaiev and Daniel Korzekwa and Pavlo Molchanov and Mostofa Patwary and Mohammad Shoeybi and Jan Kautz and Bryan Catanzaro}, year={2024}, eprint={2408.11796}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2408.11796}, }

@misc{salimans2022progressivedistillationfastsampling,
      title={Progressive Distillation for Fast Sampling of Diffusion Models}, 
      author={Tim Salimans and Jonathan Ho},
      year={2022},
      eprint={2202.00512},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.00512}, 
}

@misc{yin2024improveddistributionmatchingdistillation,
      title={Improved Distribution Matching Distillation for Fast Image Synthesis}, 
      author={Tianwei Yin and Michaël Gharbi and Taesung Park and Richard Zhang and Eli Shechtman and Fredo Durand and William T. Freeman},
      year={2024},
      eprint={2405.14867},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2405.14867}, 
}

@misc{song2023consistencymodels,
      title={Consistency Models}, 
      author={Yang Song and Prafulla Dhariwal and Mark Chen and Ilya Sutskever},
      year={2023},
      eprint={2303.01469},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2303.01469}, 
}

@misc{chen2025sanasprintonestepdiffusioncontinuoustime,
      title={SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency Distillation}, 
      author={Junsong Chen and Shuchen Xue and Yuyang Zhao and Jincheng Yu and Sayak Paul and Junyu Chen and Han Cai and Song Han and Enze Xie},
      year={2025},
      eprint={2503.09641},
      archivePrefix={arXiv},
      primaryClass={cs.GR},
      url={https://arxiv.org/abs/2503.09641}, 
}

@misc{song2024sdxsrealtimeonesteplatent,
      title={SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions}, 
      author={Yuda Song and Zehao Sun and Xuanwu Yin},
      year={2024},
      eprint={2403.16627},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.16627}, 
}

@misc{ho2022classifierfreediffusionguidance, title={Classifier-Free Diffusion Guidance}, author={Jonathan Ho and Tim Salimans}, year={2022}, eprint={2207.12598}, archivePrefix={arXiv}, primaryClass={cs.LG}, url={https://arxiv.org/abs/2207.12598}, }

@misc{meng2023distillationguideddiffusionmodels, title={On Distillation of Guided Diffusion Models}, author={Chenlin Meng and Robin Rombach and Ruiqi Gao and Diederik P. Kingma and Stefano Ermon and Jonathan Ho and Tim Salimans}, year={2023}, eprint={2210.03142}, archivePrefix={arXiv}, primaryClass={cs.CV}, url={https://arxiv.org/abs/2210.03142}, }

@misc{ren2024hypersdtrajectorysegmentedconsistency, title={Hyper-SD: Trajectory Segmented Consistency Model for Efficient Image Synthesis}, author={Yuxi Ren and Xin Xia and Yanzuo Lu and Jiacheng Zhang and Jie Wu and Pan Xie and Xing Wang and Xuefeng Xiao}, year={2024}, eprint={2404.13686}, archivePrefix={arXiv}, primaryClass={cs.CV}, url={https://arxiv.org/abs/2404.13686}, }

@misc{dehghani2022efficiencymisnomer,
      title={The Efficiency Misnomer}, 
      author={Mostafa Dehghani and Anurag Arnab and Lucas Beyer and Ashish Vaswani and Yi Tay},
      year={2022},
      eprint={2110.12894},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.12894}, 
}

@article{wan2.1,
    title   = {Wan: Open and Advanced Large-Scale Video Generative Models},
    author  = {Wan Team},
    journal = {},
    year    = {2025}
}

@misc{anthony2024casecodesigningmodelarchitectures,
      title={The Case for Co-Designing Model Architectures with Hardware}, 
      author={Quentin Anthony and Jacob Hatef and Deepak Narayanan and Stella Biderman and Stas Bekman and Junqi Yin and Aamir Shafi and Hari Subramoni and Dhabaleswar Panda},
      year={2024},
      eprint={2401.14489},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2401.14489}, 
}