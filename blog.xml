<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Sayak Paul</title>
<link>https://sayak.dev/blog.html</link>
<atom:link href="https://sayak.dev/blog.xml" rel="self" type="application/rss+xml"/>
<description>Personal site of Sayak Paul.</description>
<generator>quarto-1.7.32</generator>
<lastBuildDate>Thu, 27 Feb 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>Flavors of attention in modern diffusion models</title>
  <link>https://sayak.dev/posts/attn-diffusion.html</link>
  <description><![CDATA[ 






<p>Attention is a crucial component in generative neural architectures for continuous modalities like images and videos from natural language. More specifically, cross-attention helps to contextualize the relationship between the natural language prompt inputs and the media, being generated.</p>
<p>With modern diffusion models (or shall we say “flow”) for condition-guided image and video generation, we saw the community going beyond cross-attention. For example, Stable Diffusion 3 (SD 3) <span class="citation" data-cites="esser2024scalingrectifiedflowtransformers">[1]</span> introduced “joint-attention” in its MMDiT architecture. SANA <span class="citation" data-cites="xie2024sanaefficienthighresolutionimage">[2]</span>, on the other hand, introduced a variant of “linear attention”, moving away from the standard attention mechanism.</p>
<p>While the changes between these variants may appear architecturally simple, it can be helpful to understand the factors that distinguish them. In this post, we will investigate the popular forms of attention blocks used in modern diffusion models. We will tear them apart with simple PyTorch code and comment on some additional findings.</p>
<p>Readers are expected to be familiar with diffusion-based image generation models and encoder-based transformer architectures.</p>
<div align="center">
<img src="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/attn_diffusion/llama_header.png" width="700/">
<p><small>Image generated with <a href="https://huggingface.co/black-forest-labs/FLUX.1-dev">Flux.1-Dev</a>.</small>
</p></div>
<section id="self-attention" class="level2">
<h2 class="anchored" data-anchor-id="self-attention">Self-attention</h2>
<p>For the sake of completeness, let’s take a quick look at how self-attention is implemented. This will help us understand how it can be evolved to cross-attention and others.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x, attn_mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb1-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># x shape: (batch_size, seq_length, embed_dim)</span></span>
<span id="cb1-3">    bsz, seq_length, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x.size()</span>
<span id="cb1-4">    </span>
<span id="cb1-5">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute queries, keys, and values using separate linear layers</span></span>
<span id="cb1-6">    q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_q(x)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># shape: (batch_size, seq_length, embed_dim)</span></span>
<span id="cb1-7">    k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_k(x)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># shape: (batch_size, seq_length, embed_dim)</span></span>
<span id="cb1-8">    v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_v(x)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># shape: (batch_size, seq_length, embed_dim)</span></span>
<span id="cb1-9">    </span>
<span id="cb1-10">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Reshape and transpose to get dimensions </span></span>
<span id="cb1-11">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (batch_size, num_heads, seq_length, head_dim)</span></span>
<span id="cb1-12">    q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> q.view(bsz, seq_length, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_heads, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb1-13">    k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> k.view(bsz, seq_length, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_heads, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb1-14">    v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> v.view(bsz, seq_length, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_heads, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb1-15">    </span>
<span id="cb1-16">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute scaled dot product attention using PyTorch's built-in function</span></span>
<span id="cb1-17">    attn_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> F.scaled_dot_product_attention(</span>
<span id="cb1-18">        q, k, v, attn_mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>attn_mask, dropout_p<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.dropout.p, is_causal<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb1-19">    )  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># shape: (batch_size, num_heads, seq_length, head_dim)</span></span>
<span id="cb1-20">    </span>
<span id="cb1-21">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Combine the attention output from multiple heads</span></span>
<span id="cb1-22">    attn_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> attn_output.transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>).reshape(bsz, seq_length, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.embed_dim)</span>
<span id="cb1-23">    </span>
<span id="cb1-24">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Final linear projection</span></span>
<span id="cb1-25">    output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.out_proj(attn_output)</span>
<span id="cb1-26">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> output</span></code></pre></div>
<p>With self-attention, we model the interactions between the different parts of the <em>same input</em>.</p>
<p>Regarding the implementation above, the initialization part of the underlying class was intentionally left out in the interest of brevity. We will follow this kind of snippets for the rest of this post.</p>
</section>
<section id="cross-attention" class="level2">
<h2 class="anchored" data-anchor-id="cross-attention">Cross-attention</h2>
<p>The premise of cross-attention is we want to model how two <em>different</em> <em>inputs</em> interact with each other. For example, image patches and text tokens.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode diff code-with-copy"><code class="sourceCode diff"><span id="cb2-1"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ def forward(self, x, context=None, attn_mask=None):</span></span>
<span id="cb2-2">    # x shape: (batch_size, seq_length, embed_dim)</span>
<span id="cb2-3"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    bsz, target_seq_len, _ = x.size()</span></span>
<span id="cb2-4">    </span>
<span id="cb2-5"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    if context is None:</span></span>
<span id="cb2-6"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+        context = x</span></span>
<span id="cb2-7"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    source_seq_len = context.size(1)</span></span>
<span id="cb2-8">    </span>
<span id="cb2-9"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    # Compute queries from x; keys and values from context</span></span>
<span id="cb2-10"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    q = self.to_q(x)      # (batch_size, target_seq_length, embed_dim)</span></span>
<span id="cb2-11"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    k = self.to_k(context)  # (batch_size, source_seq_length, embed_dim)</span></span>
<span id="cb2-12"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    v = self.to_v(context)  # (batch_size, source_seq_length, embed_dim)</span></span>
<span id="cb2-13">    </span>
<span id="cb2-14">    # Reshape and transpose to get dimensions</span>
<span id="cb2-15"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    q = q.view(bsz, target_seq_len, self.num_heads, self.head_dim).transpose(1, 2)</span></span>
<span id="cb2-16"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    k = k.view(bsz, source_seq_len, self.num_heads, self.head_dim).transpose(1, 2)</span></span>
<span id="cb2-17"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    v = v.view(bsz, source_seq_len, self.num_heads, self.head_dim).transpose(1, 2)</span></span>
<span id="cb2-18">    </span>
<span id="cb2-19">    # Compute scaled dot product attention using PyTorch's built-in function</span>
<span id="cb2-20">    attn_output = F.scaled_dot_product_attention(</span>
<span id="cb2-21">        q, k, v, attn_mask=attn_mask, dropout_p=self.dropout.p, is_causal=False</span>
<span id="cb2-22">    )  # shape: (batch_size, num_heads, seq_length, head_dim)</span>
<span id="cb2-23">    </span>
<span id="cb2-24">    # Combine the attention output from multiple heads</span>
<span id="cb2-25">    attn_output = attn_output.transpose(1, 2).reshape(</span>
<span id="cb2-26">        bsz, target_seq_len, self.embed_dim</span>
<span id="cb2-27">    )</span>
<span id="cb2-28">    </span>
<span id="cb2-29">    # Final linear projection</span>
<span id="cb2-30">    output = self.out_proj(attn_output)</span>
<span id="cb2-31">    return output</span></code></pre></div>
<p>For the context of this post, <code>x</code> would be the noisy latents we will denoise during inference and <code>context</code> would be the representations computed from input text prompts. In this case, the attention masks (<code>attn_mask</code>) are usually computed from the <code>context</code>. For example, for text prompts, the attention masks are constructed from the actual text tokens and the padding tokens.</p>
<p>Let’s consider the sentence - “a dog”. Without going into too many details, if we want to tokenize it with a maximum sequence length of 10, the attention masks would look like so:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">[1,</span> 1, 1, 1, 0, 0, 0, 0, 0, 0]</span></code></pre></div>
<p>The exact text tokens would change based on the tokenizer being used but we get the idea of how attention masks might look like.</p>
<p>With the presence of attention masks, attention computation accelerates while also reducing memory footprint.</p>
<p>This implementation often meets with other elements that help stabilize training, improve the end performance, extrapolate to larger resolutions, etc. Some of these popular elements include:</p>
<p><strong>QK normalization</strong></p>
<p>Introduced in ViT-22B <span class="citation" data-cites="dehghani2023scalingvisiontransformers22">[3]</span>, QK normalization is a commonly used technique to help stabilize the training of transformers at scale. In code, this is simple to implement:</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode diff code-with-copy"><code class="sourceCode diff"><span id="cb4-1">...</span>
<span id="cb4-2">q = self.to_q(x)</span>
<span id="cb4-3">k = self.to_k(k)</span>
<span id="cb4-4">...</span>
<span id="cb4-5"></span>
<span id="cb4-6"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ q = self.q_norm_layer(q)</span></span>
<span id="cb4-7"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ k = self.k_norm_layer(k)</span></span>
<span id="cb4-8">...</span></code></pre></div>
<p>Some choices of norms include LayerNorm, RMSNorm, and L2Norm, with the first two being the most common.</p>
<p><strong>Grouped-query attention (GQA)</strong></p>
<p>In the standard attention, every query in the sequence independently computes its attention weights with every key. But there may be redundancy <span class="citation" data-cites="ainslie2023gqatraininggeneralizedmultiquery">[4]</span> in this setup. We can maintain a reduced space for the keys, and the values repeat them across groups of queries. In practice, this looks like so:</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode diff code-with-copy"><code class="sourceCode diff"><span id="cb5-1">...</span>
<span id="cb5-2">q = self.to_q(x) # (batch_size, seq_length, embed_dim)</span>
<span id="cb5-3"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ k = self.to_k(context) # (batch_size, context_seq_length, embed_dim // reduced_kv_heads)</span></span>
<span id="cb5-4"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ v = self.to_v(context) # (batch_size, context_seq_length, embed_dim // reduced_kv_heads)</span></span>
<span id="cb5-5"></span>
<span id="cb5-6"># Note there's no transpose yet (.transpose(1, 2)).</span>
<span id="cb5-7">q = q.view(batch_size, target_seq_length, self.num_heads, self.head_dim)</span>
<span id="cb5-8"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ k = k.view(batch_size, source_seq_length, self.kv_heads, self.head_dim)</span></span>
<span id="cb5-9"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ v = v.view(batch_size, source_seq_length, self.kv_heads, self.head_dim)</span></span>
<span id="cb5-10"></span>
<span id="cb5-11"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ n_rep = self.num_heads // self.kv_heads</span></span>
<span id="cb5-12"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ if n_rep &gt;= 1:</span></span>
<span id="cb5-13"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+       # Perform repeats.</span></span>
<span id="cb5-14"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    k = k.unsqueeze(3).repeat(1, 1, 1, n_rep, 1).flatten(2, 3)</span></span>
<span id="cb5-15"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    v = v.unsqueeze(3).repeat(1, 1, 1, n_rep, 1).flatten(2, 3)</span></span>
<span id="cb5-16"></span>
<span id="cb5-17"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ # Complete the transpose to get (batch_size, num_heads, seq_length, head_dim).</span></span>
<span id="cb5-18"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ q = q.transpose(1, 2)</span></span>
<span id="cb5-19"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ k = k.transpose(1, 2)</span></span>
<span id="cb5-20"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ v = v.transpose(1, 2)</span></span>
<span id="cb5-21"></span>
<span id="cb5-22"># Apply `scaled_dot_product_attention()`</span>
<span id="cb5-23">...</span></code></pre></div>
<p>This helps reduce memory overhead without hurting performance too much. This is crucial when generating high-resolution images and videos.</p>
<p><strong>Rotary position embeddings (RoPE)</strong></p>
<p>Rotary position embeddings have become de facto as they help extrapolate to longer sequences. Image (and video) generation is no exception! The explanation of RoPE is out of this post’s scope. Interested readers should check out <a href="https://huggingface.co/blog/designing-positional-encoding">this post</a>, instead.</p>
<p>Below, we provide where RoPE is usually incorporated when computing attention:</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode diff code-with-copy"><code class="sourceCode diff"><span id="cb6-1">...</span>
<span id="cb6-2"></span>
<span id="cb6-3">q = self.to_q(x) # (batch_size, seq_length, embed_dim)</span>
<span id="cb6-4">k = self.to_k(context) # (batch_size, context_seq_length, embed_dim)</span>
<span id="cb6-5">v = self.to_v(context) # (batch_size, context_seq_length, embed_dim)</span>
<span id="cb6-6"></span>
<span id="cb6-7"># Note there's no transpose yet (.transpose(1, 2)).</span>
<span id="cb6-8">q = q.view(batch_size, target_seq_length, self.num_heads, self.head_dim)</span>
<span id="cb6-9">k = k.view(batch_size, source_seq_length, self.num_heads, self.head_dim)</span>
<span id="cb6-10">v = v.view(batch_size, source_seq_length, self.num_heads, self.head_dim) </span>
<span id="cb6-11"></span>
<span id="cb6-12"># The `*_rotary_emb()` below are computed based on the inputs.</span>
<span id="cb6-13"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ query = apply_rotary_emb(query, query_rotary_emb, use_real=False)</span></span>
<span id="cb6-14"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ key = apply_rotary_emb(key, key_rotary_emb, use_real=False)</span></span>
<span id="cb6-15"></span>
<span id="cb6-16">...</span></code></pre></div>
<p>Popular models that use cross-attention include Stable Diffusion XL <span class="citation" data-cites="podell2023sdxlimprovinglatentdiffusion">[5]</span>, PixArt-{Alpha. Sigma} <span class="citation" data-cites="chen2023pixartalphafasttrainingdiffusion chen2024pixartsigmaweaktostrongtrainingdiffusion">[6, 7]</span>, Lumina-Next <span class="citation" data-cites="zhuo2024luminanextmakingluminat2xstronger">[8]</span>, LTX-Video <span class="citation" data-cites="hacohen2024ltxvideorealtimevideolatent">[9]</span>, etc. Lumina-Next incorporates all the other elements as well.</p>
</section>
<section id="joint-attention" class="level2">
<h2 class="anchored" data-anchor-id="joint-attention">Joint-attention</h2>
<p>Through cross-attention, we also inherit any bias that might be present in the prompt embeddings computed with text encoders. For example, if a text encoder exhibits a unidirectional bias (through causal attention), that can creep unexpectedly into the diffusion model representations. Joint-attention alleviates this by allowing the representations coming from two different modalities to co-evolve with training.</p>
<p>The diagram below depicts the MMDiT architecture, which also introduces joint-attention.</p>
<div align="center">
<img src="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/attn_diffusion/mmdit.png" width="600/">
<p><small>MMDiT architecture. Figure taken from SD 3 paper [@esser2024scalingrectifiedflowtransformers].</small>
</p></div>
<p>If the diagram appears to be overwhelming, don’t worry, the implementation of it is simpler than one might think. In a nutshell, in joint-attention, the QKV projection is performed separately (with separate sets of params) on each of the two modalities shown above (<code>c</code> being the representation computed from the text prompts and <code>x</code> being noisy latents to be denoised). Before computing the attention, these projections are concatenated. Quoting from the SD3 paper <span class="citation" data-cites="esser2024scalingrectifiedflowtransformers">[1]</span>:</p>
<blockquote class="blockquote">
<p>Since text and image embeddings are conceptually quite different, we use two separate sets of weights for the two modalities. […], this is equivalent to having two independent transformers for each modality, but joining the sequences of the two modalities for the attention operation, such that both representations can work in their own space yet take the other one into account.</p>
</blockquote>
<p>Interested readers can check out <a href="https://x.com/RisingSayak/status/1888462811971400063">this thread</a> for more insights from the community.</p>
<p>Let’s now turn our attention to how it is implemented in practice.</p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x, context<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, attn_mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb7-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> context <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb7-3">        source_seq_len <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> context.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb7-4">    </span>
<span id="cb7-5">    bsz, target_seq_len, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x.size()</span>
<span id="cb7-6">    </span>
<span id="cb7-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute projections on the different modalities separately</span></span>
<span id="cb7-8">    q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_q(x)      </span>
<span id="cb7-9">    k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_k(x)  </span>
<span id="cb7-10">    v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_v(x)  </span>
<span id="cb7-11"></span>
<span id="cb7-12">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Reshape and transpose for multi-head attention</span></span>
<span id="cb7-13">    q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> q.view(bsz, target_seq_len, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_heads, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb7-14">    k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> k.view(bsz, target_seq_len, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_heads, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb7-15">    v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> v.view(bsz, target_seq_len, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_heads, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb7-16"></span>
<span id="cb7-17">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute projections on the condition separately</span></span>
<span id="cb7-18">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> context <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb7-19">        context_q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_add_q(context) </span>
<span id="cb7-20">        context_k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_add_k(context)</span>
<span id="cb7-21">        context_v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_add_v(context)</span>
<span id="cb7-22">        </span>
<span id="cb7-23">        context_q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> context_q.view(bsz, source_seq_len, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_heads, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb7-24">        context_k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> context_k.view(bsz, source_seq_len, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_heads, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb7-25">        context_v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> context_v.view(bsz, source_seq_len, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_heads, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb7-26"></span>
<span id="cb7-27">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Concatenate across the sequence length dimension</span></span>
<span id="cb7-28">        q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.cat([context_q, q], dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb7-29">        k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.cat([context_k, k], dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb7-30">        v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.cat([context_v, v], dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb7-31">    </span>
<span id="cb7-32">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute scaled dot product attention</span></span>
<span id="cb7-33">    attn_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> F.scaled_dot_product_attention(</span>
<span id="cb7-34">        q, k, v, attn_mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>attn_mask, dropout_p<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.dropout.p, is_causal<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb7-35">    )  </span>
<span id="cb7-36">    </span>
<span id="cb7-37">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Combine attention heads</span></span>
<span id="cb7-38">    attn_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> attn_output.transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>).reshape(</span>
<span id="cb7-39">        bsz, target_seq_len, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.embed_dim</span>
<span id="cb7-40">    )</span>
<span id="cb7-41"></span>
<span id="cb7-42">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Seperate context from latents and final linear projection</span></span>
<span id="cb7-43">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> context <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb7-44">        context, x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb7-45">            attn_output[:, : source_seq_len],</span>
<span id="cb7-46">            attn_output[:, source_seq_len :],</span>
<span id="cb7-47">        )</span>
<span id="cb7-48">        context <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.add_out_proj(context)</span>
<span id="cb7-49">        x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.out_proj(x)</span>
<span id="cb7-50"></span>
<span id="cb7-51">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> x, context</span>
<span id="cb7-52">    </span>
<span id="cb7-53">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb7-54">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.out_proj(attn_output)</span></code></pre></div>
<p>With joint-attention, it becomes unclear how to incorporate attention masks while computing attention and how much of a performance penalty it incurs due to that.</p>
<p><strong>Part joint-attention, part self-attention</strong></p>
<p>Subsequent works like AuraFlow <span class="citation" data-cites="auraflow">[10]</span> and Flux <span class="citation" data-cites="flux">[11]</span> introduced a small change in the original MMDiT architecture. They use joint attention for the first few layers within the diffusion transformer. They then concatenate the two different outputs and operate on the concatenated output. As per the AuraFlow authors, it helps with better FLOPs optimization. In pseudo-code, it looks like so:</p>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Regular MMDiT blocks.</span></span>
<span id="cb8-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> block <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.double_blocks:</span>
<span id="cb8-3">    context, x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> block(</span>
<span id="cb8-4">        x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>x,</span>
<span id="cb8-5">        context<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>context, </span>
<span id="cb8-6">        ...</span>
<span id="cb8-7">    )</span>
<span id="cb8-8"></span>
<span id="cb8-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Concatenate.</span></span>
<span id="cb8-10">context_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.cat([context, x], dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)  </span>
<span id="cb8-11"></span>
<span id="cb8-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Continue with the rest.</span></span>
<span id="cb8-13"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> block <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.single_blocks:</span>
<span id="cb8-14">    x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> block(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>context_x, ...) </span></code></pre></div>
<p>The joint-attention implementation provided above is already equipped to handle situations when <code>context</code> may not be provided while computing attention (the <code>context is not None</code> code path).</p>
<p><strong>Attention gymnastics</strong></p>
<p>Flux, additionally, improves the hardware efficiency by using parallel layers <span class="citation" data-cites="dehghani2023scalingvisiontransformers22">[3]</span>. To better understand how parallel layers can be helpful in improving efficiency, let’s look at the first set of equations that govern an encoder-style transformer block:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20y%5E%7B%5Cprime%7D=%5Coperatorname%7BLayerNorm%7D(x)%20%5C%5C%0A&amp;%20y=x+%5Coperatorname%7BMLP%7D%5Cleft(y%5E%7B%5Cprime%7D%5Cright)+%5Coperatorname%7BAttention%7D%5Cleft(y%5E%7B%5Cprime%7D%5Cright)%0A%5Cend%7Baligned%7D%0A"></p>
<p>We can combine the linear projection layers of attention (QKV) and the MLP. From the ViT-22B paper:</p>
<blockquote class="blockquote">
<p>In particular, the matrix multiplication for query/key/value-projections and the first linear layer of the MLP are fused into a single operation, and the same is done for the attention out-projection and second linear layer of the MLP.</p>
</blockquote>
<p>To understand how it is implemented in practice, we first need to understand QKV fusion. It lets us perform the three different projections involved in attention in one go. Instead of having three different projection layers, we only keep one:</p>
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">...</span>
<span id="cb9-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># This becomes</span></span>
<span id="cb9-3"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(embed_dim, embed_dim)</span>
<span id="cb9-4"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(embed_dim, embed_dim)</span>
<span id="cb9-5"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(embed_dim, embed_dim)</span>
<span id="cb9-6"></span>
<span id="cb9-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># this ⬇️</span></span>
<span id="cb9-8"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_qkv <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(embed_dim, embed_dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span></code></pre></div>
<p>Then, during the forward pass, we do:</p>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">...</span>
<span id="cb10-2">qkv <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_qkv(x)</span>
<span id="cb10-3">split_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> qkv.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb10-4">q, k, v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.split(qkv, split_size, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb10-5"></span>
<span id="cb10-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Rest of the process is the same</span></span>
<span id="cb10-7">...</span></code></pre></div>
<p>Now, to incorporate the MLP into the mix, we need some changes in the initialization:</p>
<div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">...</span>
<span id="cb11-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># MLP ratio is typically 4.</span></span>
<span id="cb11-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The first part of fusion. QKV + first layer of an MLP from a transformer block.</span></span>
<span id="cb11-4"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.qkv_mlp_first <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(embed_dim, embed_dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> mlp_ratio))</span>
<span id="cb11-5"></span>
<span id="cb11-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Second part of fusion. Attention out projection + second MLP layer.</span></span>
<span id="cb11-7"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.attn_proj_mlp_second <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(</span>
<span id="cb11-8">    embed_dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> embed_dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> mlp_ratio, embed_dim</span>
<span id="cb11-9">)</span></code></pre></div>
<p>The forward pass would be:</p>
<div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">qkv, mlp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.split(</span>
<span id="cb12-2">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.qkv_mlp_first(x_mod), </span>
<span id="cb12-3">    [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.embed_dim, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.embed_dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mlp_ratio)], </span>
<span id="cb12-4">    dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb12-5">)</span>
<span id="cb12-6">q, k, v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.split(qkv, qkv.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb12-7"></span>
<span id="cb12-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute attention</span></span>
<span id="cb12-9">...</span>
<span id="cb12-10"></span>
<span id="cb12-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># MLP computation</span></span>
<span id="cb12-12">concat_attn_mlp_in <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.cat((attn_output, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mlp_act(mlp)), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb12-13">output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.attn_proj_mlp_second(concat_attn_mlp_in)</span></code></pre></div>
<p>The ViT-22B paper <span class="citation" data-cites="dehghani2023scalingvisiontransformers22">[3]</span> also provides a great visualization for this:</p>
<div align="center">
<img src="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/attn_diffusion/parallel_layers.png" width="600/">
<p><small>Transformer block with parallel layers. Figure taken from the ViT-22B paper [@dehghani2023scalingvisiontransformers22].</small>
</p></div>
<p>It is also a good idea to mention that some of the other elements we discussed earlier — QK normalization, GQA, and RoPE — can also be combined with joint attention. One of the most popular models, Flux, uses QK normalization and RoPE. Lumina2 <span class="citation" data-cites="lumina2">[12]</span> combines all three and uses joint-attention with a twist:</p>
<ul>
<li>It first uses very few layers of self-attention transformer blocks separately on the noisy latents and the conditional representations.</li>
<li>It then combines the two representations and runs it through a number of self-attention transformer blocks.</li>
</ul>
<p>Interested readers can check out the implementation details <a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/transformers/transformer_lumina2.py#L498C1-L525C1">here</a>. The figure below provides a side-by-side comparison of the differences in attention used in SD3 and Lumina2:</p>
<div align="center">
<img src="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/attn_diffusion/comparison_sd3_lumina2.jpg" width="600/">
<p><small>Comparison between the attention schemes used in SD3 and Lumina2, respectively. Figures were intentionally simplified to convey the main idea.</small>
</p></div>
</section>
<section id="linear-attention" class="level2">
<h2 class="anchored" data-anchor-id="linear-attention">Linear attention</h2>
<p>As the world already knows, attention has a quadratic time complexity. This can pose prohibitive challenges when operating with very long sequences despite improved techniques like Flash Attention.</p>
<p>SANA <span class="citation" data-cites="xie2024sanaefficienthighresolutionimage">[2]</span> replaced all vanilla attention with linear attention. More specifically, in each of its transformer blocks, SANA has two kinds of attention:</p>
<ol type="1">
<li>linear self-attention for the noisy latents,</li>
<li>regular cross-attention for the noisy latents (<code>x</code>) and the condition representations (<code>context</code>).</li>
</ol>
<p>To facilitate local interactions between the tokens, it used “Mix-FFN” blocks <span class="citation" data-cites="xie2021segformersimpleefficientdesign">[13]</span>, <span class="citation" data-cites="liu2023efficientvitmemoryefficientvision">[14]</span>.</p>
<div align="center">
<img src="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/attn_diffusion/sana_linear_attn.png" width="600/">
<p><small>Linear attention block and the Mix-FFN block. Figure taken from the SANA paper [@xie2024sanaefficienthighresolutionimage].</small>
</p></div>
<p>Implementation of this linear-attention variant is by far the simplest. We show the main changes introduced when compared to the classic self-attention:</p>
<div class="sourceCode" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode diff code-with-copy"><code class="sourceCode diff"><span id="cb13-1">def forward(self, x):</span>
<span id="cb13-2">    # x shape: (batch_size, seq_length, embed_dim)</span>
<span id="cb13-3">    bsz, seq_length, _ = x.size()</span>
<span id="cb13-4">    </span>
<span id="cb13-5">    # Compute queries, keys, and values using separate linear layers</span>
<span id="cb13-6">    q = self.to_q(x)  # shape: (batch_size, seq_length, embed_dim)</span>
<span id="cb13-7">    k = self.to_k(x)  # shape: (batch_size, seq_length, embed_dim)</span>
<span id="cb13-8">    v = self.to_v(x)  # shape: (batch_size, seq_length, embed_dim)</span>
<span id="cb13-9">    </span>
<span id="cb13-10">    # Reshape and transpose to get dimensions </span>
<span id="cb13-11">    # (batch_size, num_heads, seq_length, head_dim)</span>
<span id="cb13-12">    q = q.view(bsz, seq_length, self.num_heads, self.head_dim).transpose(1, 2)</span>
<span id="cb13-13">    k = k.view(bsz, seq_length, self.num_heads, self.head_dim).transpose(1, 2)</span>
<span id="cb13-14">    v = v.view(bsz, seq_length, self.num_heads, self.head_dim).transpose(1, 2)</span>
<span id="cb13-15"></span>
<span id="cb13-16"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+   # Reshape to (batch_size, seq_length, num_heads, head_dim)</span></span>
<span id="cb13-17"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+   q = q.transpose(2, 3)</span></span>
<span id="cb13-18"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+   v = v.transpose(2, 3)</span></span>
<span id="cb13-19">    </span>
<span id="cb13-20"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    # Introduce non-linearity</span></span>
<span id="cb13-21"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    q = F.relu(q)</span></span>
<span id="cb13-22"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    k = F.relu(k)</span></span>
<span id="cb13-23"></span>
<span id="cb13-24"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    # Combine scores</span></span>
<span id="cb13-25"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    scores = torch.matmul(v, k)</span></span>
<span id="cb13-26"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    x = torch.matmul(scores, q)</span></span>
<span id="cb13-27">    </span>
<span id="cb13-28"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    # Scale</span></span>
<span id="cb13-29"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    x = x[:, :, :-1] / (x[:, :, -1:] + 1e-15)</span></span>
<span id="cb13-30">    </span>
<span id="cb13-31">    # Combine the output from multiple heads</span>
<span id="cb13-32">    x = x.transpose(1, 2).reshape(bsz, seq_length, self.embed_dim)</span>
<span id="cb13-33">    </span>
<span id="cb13-34">    # Final linear projection</span>
<span id="cb13-35">    output = self.out_proj(x)</span>
<span id="cb13-36">    return output</span></code></pre></div>
<p>It’s also worth pointing out that SANA uses no positional encodings (so-called <strong>NoPE</strong>). The SANA authors conjectured that the use of Mix-FFN blocks helped them get away with NoPE without incurring any loss in performance.</p>
</section>
<section id="thoughts" class="level2">
<h2 class="anchored" data-anchor-id="thoughts">Thoughts</h2>
<p>Throughout the course of this post, we saw many architectural configurations of the attention mechanism. Some questions that may still arise:</p>
<ul>
<li>Is there any benefit to using cross-attention for image-video generation at all?</li>
<li>How can we compensate for the compute intensity of joint-attention?</li>
<li>Is the Lumina2 way of doing joint-attention the way to go?</li>
<li>Is it necessary to do masking in joint-attention? If so, what are the benefits?</li>
</ul>
<p>In defense of the widely adopted and optimized vanilla attention, could we interleave quadratic attention and window attention (as done in Gemma2 <span class="citation" data-cites="gemmateam2024gemma2improvingopen">[15]</span>)?</p>
<p>All of these questions (and possibly more) warrant a careful ablation study.</p>
<p><em>Acknowledgements</em>: Thanks to <a href="https://twitter.com/ariG23498">Aritra Roy Gosthipaty</a> for useful feedback.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body">
<div id="ref-esser2024scalingrectifiedflowtransformers" class="csl-entry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Esser P, Kulal S, Blattmann A, et al (2024) <a href="https://arxiv.org/abs/2403.03206">Scaling rectified flow transformers for high-resolution image synthesis</a></div>
</div>
<div id="ref-xie2024sanaefficienthighresolutionimage" class="csl-entry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Xie E, Chen J, Chen J, et al (2024) <a href="https://arxiv.org/abs/2410.10629">SANA: Efficient high-resolution image synthesis with linear diffusion transformers</a></div>
</div>
<div id="ref-dehghani2023scalingvisiontransformers22" class="csl-entry">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">Dehghani M, Djolonga J, Mustafa B, et al (2023) <a href="https://arxiv.org/abs/2302.05442">Scaling vision transformers to 22 billion parameters</a></div>
</div>
<div id="ref-ainslie2023gqatraininggeneralizedmultiquery" class="csl-entry">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">Ainslie J, Lee-Thorp J, Jong M de, Zemlyanskiy Y, Lebrón F, Sanghai S (2023) <a href="https://arxiv.org/abs/2305.13245">GQA: Training generalized multi-query transformer models from multi-head checkpoints</a></div>
</div>
<div id="ref-podell2023sdxlimprovinglatentdiffusion" class="csl-entry">
<div class="csl-left-margin">5. </div><div class="csl-right-inline">Podell D, English Z, Lacey K, et al (2023) <a href="https://arxiv.org/abs/2307.01952">SDXL: Improving latent diffusion models for high-resolution image synthesis</a></div>
</div>
<div id="ref-chen2023pixartalphafasttrainingdiffusion" class="csl-entry">
<div class="csl-left-margin">6. </div><div class="csl-right-inline">Chen J, Yu J, Ge C, et al (2023) <a href="https://arxiv.org/abs/2310.00426">PixArt-<img src="https://latex.codecogs.com/png.latex?%5Calpha">: Fast training of diffusion transformer for photorealistic text-to-image synthesis</a></div>
</div>
<div id="ref-chen2024pixartsigmaweaktostrongtrainingdiffusion" class="csl-entry">
<div class="csl-left-margin">7. </div><div class="csl-right-inline">Chen J, Ge C, Xie E, et al (2024) <a href="https://arxiv.org/abs/2403.04692">PixArt-: Weak-to-strong training of diffusion transformer for 4K text-to-image generation</a></div>
</div>
<div id="ref-zhuo2024luminanextmakingluminat2xstronger" class="csl-entry">
<div class="csl-left-margin">8. </div><div class="csl-right-inline">Zhuo L, Du R, Xiao H, et al (2024) <a href="https://arxiv.org/abs/2406.18583">Lumina-next: Making lumina-T2X stronger and faster with next-DiT</a></div>
</div>
<div id="ref-hacohen2024ltxvideorealtimevideolatent" class="csl-entry">
<div class="csl-left-margin">9. </div><div class="csl-right-inline">HaCohen Y, Chiprut N, Brazowski B, et al (2024) <a href="https://arxiv.org/abs/2501.00103">LTX-video: Realtime video latent diffusion</a></div>
</div>
<div id="ref-auraflow" class="csl-entry">
<div class="csl-left-margin">10. </div><div class="csl-right-inline">fal.ai, Simo <a href="https://blog.fal.ai/auraflow">Introducing AuraFlow v0.1, an open exploration of large rectified flow models</a></div>
</div>
<div id="ref-flux" class="csl-entry">
<div class="csl-left-margin">11. </div><div class="csl-right-inline">Labs BF <a href="https://blackforestlabs.ai/announcing-black-forest-labs/">Announcing black forest labs</a></div>
</div>
<div id="ref-lumina2" class="csl-entry">
<div class="csl-left-margin">12. </div><div class="csl-right-inline">Team L <a href="https://github.com/Alpha-VLLM/Lumina-Image-2.0">Lumina-image 2.0 : A unified and efficient image generative model</a></div>
</div>
<div id="ref-xie2021segformersimpleefficientdesign" class="csl-entry">
<div class="csl-left-margin">13. </div><div class="csl-right-inline">Xie E, Wang W, Yu Z, Anandkumar A, Alvarez JM, Luo P (2021) <a href="https://arxiv.org/abs/2105.15203">SegFormer: Simple and efficient design for semantic segmentation with transformers</a></div>
</div>
<div id="ref-liu2023efficientvitmemoryefficientvision" class="csl-entry">
<div class="csl-left-margin">14. </div><div class="csl-right-inline">Liu X, Peng H, Zheng N, Yang Y, Hu H, Yuan Y (2023) <a href="https://arxiv.org/abs/2305.07027">EfficientViT: Memory efficient vision transformer with cascaded group attention</a></div>
</div>
<div id="ref-gemmateam2024gemma2improvingopen" class="csl-entry">
<div class="csl-left-margin">15. </div><div class="csl-right-inline">Team G (2024) <a href="https://arxiv.org/abs/2408.00118">Gemma 2: Improving open language models at a practical size</a></div>
</div>
</div></section></div> ]]></description>
  <category>diffusion</category>
  <category>diffusion-transformers</category>
  <category>diffusers</category>
  <guid>https://sayak.dev/posts/attn-diffusion.html</guid>
  <pubDate>Thu, 27 Feb 2025 00:00:00 GMT</pubDate>
  <media:content url="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/attn_diffusion/llama_header.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Streamlining PyPI Releases: A Case Study with 🧨 Diffusers</title>
  <link>https://sayak.dev/posts/streamlined-releases.html</link>
  <description><![CDATA[ 






<p>Releasing a new version of an open-source library is an exhilarating experience. You ship new features, bug fixes, improved documentation, etc., to serve your users and also the mission of your library. Being one of the maintainers of the <a href="https://github.com/huggingface/diffusers/">🧨&nbsp;Diffusers library</a>, I am no exception to this.</p>
<p>Once a release is finalized, it’s usually published on a software repository for the distribution programming language you’re using. Diffusers is a Python library, so <a href="https://pypi.org/">PyPI</a> is our publishing platform.</p>
<p>In this post, I share some of what I learned from trying to streamline the entire process of releasing a new version of the library and then publishing it. If you have similar responsibilities at your workplace or for your personal projects, this post might be helpful for you.</p>
<section id="an-example-release-workflow-manual" class="level2">
<h2 class="anchored" data-anchor-id="an-example-release-workflow-manual">An example release workflow (manual)</h2>
<p>Before we proceed to the other sections of the post, it will be helpful to have a schematic of what constitutes a release. Note that this workflow will vary from library to library, but some principles will still apply. I will take the workflow we follow for Diffusers as an example.</p>
<p>The steps are well laid out in <code>setup.py</code> and can be found <a href="https://github.com/huggingface/diffusers/blob/main/setup.py#L20C1-L78C55">here</a>. Broadly, these are:</p>
<ol type="1">
<li>Prepare the release branch and cut it out from the <code>main</code>.</li>
<li>Run any test on the release branch and wait for them to pass. Fix any failures if needed.</li>
<li>Tag the release branch and push the tag.</li>
<li>Build the package source and wheel.<br>
</li>
<li>Upload the package distribution to the <a href="https://test.pypi.org/">Test PyPI server</a> and run any tests.</li>
<li>Finally, upload to the actual <a href="https://pypi.org/">PyPI server</a>.</li>
</ol>
<p>We identified that steps 1-3 will always require a bit of human intervention and cannot be automated much (props if that’s not the case for you). But steps 3-6 can indeed be automated. These steps require more attention, too:</p>
<ul>
<li>When building the package distribution, one must delete the previous one before starting the build. Otherwise, it can have unintended consequences.</li>
<li>Managing the credentials for the Test PyPI and PyPI servers.</li>
<li>Running any tests after publishing them on the Test PyPI server.</li>
</ul>
<p>These steps would be better automated in your library’s Continuous Integration suite, greatly reducing the mental burden.</p>
</section>
<section id="semi-automating-the-release-workflow" class="level2">
<h2 class="anchored" data-anchor-id="semi-automating-the-release-workflow">Semi-automating the release workflow</h2>
<p>Once we identified the above findings, we prepared a GitHub Actions workflow that gets triggered after a release is tagged and the tag is pushed. Additionally, we configured the workflow to be manually triggerable in case any intervention was needed.</p>
<p>This workflow takes the following steps:</p>
<ol type="1">
<li>Find out the release branch so that it can be checked out for the sequential steps.</li>
<li>Steps 3-6 as outlined in the above section.</li>
</ol>
<p>It’s worth noting that the trigger for this kind of workflow should be figured out to suit what’s best for the given project. In the case of Diffusers, we realized that release steps that come after pushing the release tags can be largely automated. Hence, we went with that trigger.</p>
<p>The workflow file is available <a href="https://github.com/huggingface/diffusers/blob/main/.github/workflows/pypi_publish.yaml">here</a>. When successfully executed, it appears like so:</p>
<p><img src="https://github.com/sayakpaul/portfolio/blob/master/posts/release_graph.png?raw=true" class="img-fluid"></p>
<p>You can find the complete details about the action run <a href="https://github.com/huggingface/diffusers/actions/runs/8283556088">here</a>.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pay attention to the dependencies
</div>
</div>
<div class="callout-body-container callout-body">
<p>The initial workflow was missing a dependency that was needed to run the import tests after Test PyPI publishing. This was fixed in <a href="https://github.com/huggingface/diffusers/pull/7339">this PR</a>. So, please double-check any dependency that might be needed to run the tests after your package has been published on the Test PyPI server.</p>
</div>
</div>
<p>The <a href="https://github.com/huggingface/diffusers/blob/main/.github/workflows/pypi_publish.yaml">workflow</a> doesn’t make use of any pre-built actions (such as <a href="https://github.com/pypa/gh-action-pypi-publish"><code>pypa/gh-action-pypi-publish@v1.6.4</code></a>) for publishing on PyPI. Instead, we decided to just follow what we’d do manually, i.e., use <code>twine</code> to manage the process. If you’re looking to use such an action, <a href="https://github.com/philschmid/easyllm/blob/main/.github/workflows/publish.yaml">this</a> can be a handy reference.</p>
</section>
<section id="publishing-the-release-notes-and-communications" class="level2">
<h2 class="anchored" data-anchor-id="publishing-the-release-notes-and-communications">Publishing the release notes and communications</h2>
<p>The next step in the release process involves publishing the release notes on your repository and tagging it. Once a release is published, team members usually communicate about it internally within an organization and also more broadly with their communities through social media channels.</p>
<p>On the Diffusers team, we take release notes pretty seriously (<a href="https://github.com/huggingface/diffusers/releases/tag/v0.27.0">example notes</a>). This is why we intentionally keep the process of writing the notes and publishing them purely manual. Once a release is published on the repository, a workflow gets automatically triggered to communicate about it to an internal Slack channel. Successful execution of this workflow makes a bot automatically post the message below to a particular Slack channel:</p>
<p><img src="https://github.com/sayakpaul/portfolio/blob/master/posts/bot.png?raw=true" class="img-fluid"></p>
<p>This workflow can be found <a href="https://github.com/huggingface/diffusers/blob/main/.github/workflows/notify_slack_about_release.yml">here</a>.</p>
<p>Both the above steps were introduced in Diffusers through <a href="https://github.com/huggingface/diffusers/pull/7270">this PR</a>. I recommend readers to go through it if they want to incorporate similar changes in their projects.</p>
</section>
<section id="considerations" class="level2">
<h2 class="anchored" data-anchor-id="considerations">Considerations</h2>
<p>I played with the workflows rigorously on a <a href="https://github.com/sayakpaul/blossom">dummy repository</a> before introducing them in Diffusers. This is optional but highly recommended to confidently land similar changes in your actual projects.</p>
<p>We used incoming webhooks on Slack so that the bot could post messages. If you’re configuring something similar, this <a href="https://api.slack.com/messaging/webhooks">official tutorial</a> can be quite useful.</p>


</section>

 ]]></description>
  <category>pypi-releases</category>
  <category>github-actions</category>
  <category>diffusers</category>
  <guid>https://sayak.dev/posts/streamlined-releases.html</guid>
  <pubDate>Fri, 15 Mar 2024 00:00:00 GMT</pubDate>
  <media:content url="https://sayak.dev/posts/pypi-releases-diffusers.png" medium="image" type="image/png" height="72" width="144"/>
</item>
<item>
  <title>Practicing ML in a Non-ML Job</title>
  <link>https://sayak.dev/posts/ml-practice.html</link>
  <description><![CDATA[ 






<p>Many people who aspire to become Machine Learning (ML) practitioners find it particularly difficult to continue to hone relevant skills when they pursue a job that does not involve even a tiny bit of ML. So, if you’re serious about choosing ML as a potential career option, it’s important to ensure you continue to practise what you’re learning along the way. Otherwise, there’d likely be nothing for a recruiter to trust in your candidature which, in turn, minimizes your chances of landing the ML job you always wanted.</p>
<p>I myself am not an exception to this. Back in 2017, when I was working at Tata Consultancy Services Limited (TCS), I didn’t get any assignments involving ML expertise. But I tried to utilize my off-work hours in a way that helped me improve my ML-specific knowledge as well as strengthen my candidature.</p>
<p>So, in this post, I’ll share what I did during those days in the hope of providing some meaningful ways for navigation.</p>
<p><strong>Disclaimer</strong>: The opinions stated in this post are solely mine and they are not meant to demean anyone else’s opinions about the same topic.</p>
<section id="assumptions" class="level2">
<h2 class="anchored" data-anchor-id="assumptions">Assumptions</h2>
<p>The post is best-suited for professionals that have prior experience in coding (preferably in Python) and know their way around the fundamentals of ML. If you’re an absolute beginner then I recommend picking up a book (<a href="https://www.manning.com/books/grokking-machine-learning">an example</a>) or a course (<a href="https://developers.google.com/machine-learning/crash-course">an example</a>) to get started. Also, if you haven’t yet picked up an ML framework (Scikit-Learn, PyTorch, TensorFlow, JAX, etc.), then I highly recommend picking one up.</p>
</section>
<section id="solace-in-uncertainty" class="level2">
<h2 class="anchored" data-anchor-id="solace-in-uncertainty">Solace in Uncertainty</h2>
<p>Set your objectives straight. Ask yourself if you’re absolutely certain about wanting to pursue a career in ML. If so, then are you willing to make the adjustments necessary to attain that at any cost? Although these questions are not specific to the purposes of this post, they help set a mindset to push through uncertain times.</p>
<p>I was hell-bent on taking up a career in ML that helped me to work on myself in those extra hours after work. It didn’t feel like I’m being forced into doing this. I thoroughly enjoyed the process and I trusted it. There are things I still enjoy doing like, reading a new paper, learning about a new concept, implementing it, etc.</p>
</section>
<section id="overwhelm-and-courage-to-learn" class="level2">
<h2 class="anchored" data-anchor-id="overwhelm-and-courage-to-learn">Overwhelm and Courage to Learn</h2>
<p>Feeling overwhelmed especially in the ML domain is common given how vast the field is and how rapidly it is evolving regularly. I see this positively because I know that there are things I don’t yet know and I use it as a learning opportunity to improve my knowledge.</p>
<p>One might wonder, do I learn each and everything that comes out? That’s impossible and likely, isn’t very helpful. So, I like to pick up something from the vault of things that genuinely interest me in ML and start digging deeper. I find it incredibly helpful in boosting my confidence. I also figured that the more I did this, the better I was able to develop a general understanding of a broad number of relevant things.</p>
<p>In a nutshell, treating the feeling of “overwhelm” as a learning opportunity has been quite helpful for me.</p>
</section>
<section id="learn-apply-demo-repeat" class="level2">
<h2 class="anchored" data-anchor-id="learn-apply-demo-repeat">Learn, Apply, (Demo), Repeat</h2>
<p>Learning is not enough. You need to be able to develop evidence that shows you can apply what you’ve learned successfully. I highly recommend reading <a href="https://blog.floydhub.com/emils-story-as-a-self-taught-ai-researcher/">this interview with Emil Wallner</a> who’s an “internet-taught” ML Researcher working as a resident at Google.</p>
<p>Below, I discuss a few things you can do to exercise your ML learnings.</p>
<section id="kaggle" class="level3">
<h3 class="anchored" data-anchor-id="kaggle">Kaggle</h3>
<p>Kaggle is arguably one of the best platforms to develop skills for data preprocessing and applying ML in creative ways to solve unique problems. So, pick an interesting dataset or a competition <em>just for learning purposes</em>. Putting the competitive mindset aside, during my initial years it really helped me to develop a mindset of always learning to facilitate self-improvement. If you commit to it hard enough, you will have developed a bunch of useful skills. Over time, you’ll definitely get better.</p>
<p>Keeping an open mind for learning is important here because expectations of outcomes can quickly derail you. Also, remember that the rate of improvement is not the same for everyone. So, it’s better to just do things that are within your control (for example, learning something), and consistently get better at those.</p>
</section>
<section id="papers-concepts" class="level3">
<h3 class="anchored" data-anchor-id="papers-concepts">Papers / Concepts</h3>
<p>Reading research papers is a common undertaking in ML. It can be rewarding to summarize, implement, and blog about a paper that is impactful and tackles interesting problems. Extending on this theme, you have a number of options:</p>
<ul>
<li><p>You can summarize a paper in your own words and publish it on platforms like <a href="https://medium.com/">Medium</a> or even on <a href="https://github.com/fastai/fastpages">your own blog</a>. It’s also important to get feedback on your work. So, feel free to share your work on Social Media as well as let the authors of the actual paper know about your work. A paper summary is supposed to be a reflection of how you perceived the paper. So, if you have criticisms of a paper, do include those with solid reasoning. If you’re looking for an example, definitely check out <a href="https://medium.com/@nainaakash012">Aakash Kumar Nain’s paper summaries</a>.</p>
<p>Picking a paper could be a non-trivial work especially when there’s always a flood of papers on <a href="https://arxiv.org/">arXiv</a>. I usually follow the blogs of research labs at <a href="https://ai.googleblog.com/">Google</a>, <a href="https://ai.facebook.com/blog">Meta</a>, <a href="https://blog.allenai.org/">AI2</a>, <a href="https://bair.berkeley.edu/">BAIR</a>, etc., to keep myself up-to-date about the work I care about. There’s a good chance you’ll find your niche there. Following the works of the most accomplished researchers from my favorite domains is another practice I incorporate.</p>
<p>In this regard, I highly recommend the following two books that actively cite examples of relevant research papers and also implement them in ways that are practically beneficial: <a href="https://www.oreilly.com/library/view/deep-learning-for/9781492045519/">Deep Learning for Coders with fastai and PyTorch</a> by Jeremy Howard and Sylvain Gugger, <a href="https://www.oreilly.com/library/view/natural-language-processing/9781098103231/">Natural Language Processing with Transformers</a> by Lewis Tunstall, Leandro von Werra, and Thomas Wolf. For developing a general understanding of different areas in ML, I recommend the articles on <a href="https://distill.pub/">Distill Pub</a>. <br><br></p></li>
<li><p>Nowadays, a majority of ML papers come with official open-source implementations in the interest of reproducibility. But some don’t. Regardless of either, it’s a good exercise to try to implement the novel bits of a paper. The <a href="https://github.com/rwightman/pytorch-image-models">timm</a><a href="https://github.com/rwightman/pytorch-image-models">libary</a> is a great example of how paper reimplementations should be structured. <br><br></p></li>
<li><p>Blogging has easily become one of the most effective ways to communicate your understanding of something. This does not need to be just tied to papers, though. You can always pick up an interesting concept and blog about it. Many ML stalwarts keep pressing on why you should blog and here is one such example: <a href="https://medium.com/@racheltho/why-you-yes-you-should-blog-7d2544ac1045">Why you (yes, you) should blog</a> by Rachel Thomas.</p></li>
</ul>
<p>You can also consider making videos on papers, concepts, and so on. If you haven’t already, then definitely get to know <a href="https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfew">Yannic Kilcher</a> who has revolutionized the way forward in this theme.</p>
</section>
<section id="open-source-contributions" class="level3">
<h3 class="anchored" data-anchor-id="open-source-contributions">Open-source Contributions</h3>
<p>From my personal experience, I can confirm that making open-source contributions is one of the most useful ways to stay involved in the domain. All the popular ML Python libraries (Scikit-Learn, PyTorch, TensorFlow, Keras, JAX, Hugging Face Transformers, etc.) are open-source and that provides even more opportunities to learn and grow.</p>
<p>I have a <a href="https://www.youtube.com/watch?v=VsBEFUoESR4&amp;t=3123s">separate presentation</a> on this topic but here, I provide my perspectives for context:</p>
<ul>
<li><p>When you’re contributing to a well-maintained open-source library for the first time there’s a high chance that you’ll learn a few things other than just ML. These include writing unit tests, setting up the local development environment, library building tools, etc. This way, you get first-hand exposure to how software engineering is approached in the ML domain in general.</p>
<p>So, not only do you get to contribute to your favorite open-source library (which is an inexplicable feeling anyway), but you also get to learn skills that are practically quite demanding. Beyond these, you get a chance to interact with experts and get their feedback to improve your work. Additionally, you get to collect objective evidence of your skills - coding, thorough understanding of a critical component and the library, building a library, etc. - all of which are noteworthy.</p>
<p>Note that you’re not alone if you’re feeling lost when you’re just starting to contribute to an open-source library. It happens to most. But when you put your mind toward making your contribution anyway, you get to get better in the process. <br><br></p></li>
<li><p>If you feel you’re not ready yet to make contributions, working on your own open-source projects is another promising avenue to pursue. Take Andrej Karpathy’s <a href="https://github.com/karpathy/minGPT">miniGPT</a> project as an example. Besides being an amazing educational resource for learning about the <a href="https://en.wikipedia.org/wiki/GPT-3">GPT model</a>, it serves as a great reference for implementing many of the foundational blocks of <a href="https://arxiv.org/abs/1706.03762">Transformer</a>-based architectures.</p>
<p>If you’re looking for open-source project ideas then <a href="https://youtu.be/dllfKQKlzvg">my presentation</a> on this topic might be helpful.</p></li>
</ul>
<p>Now that we’ve looked into different ways of being engaged with our independent ML practice, let us take examples of two individuals from the ML community who have followed similar paths in this regard.</p>
</section>
</section>
<section id="references-from-the-community" class="level2">
<h2 class="anchored" data-anchor-id="references-from-the-community">References from the Community</h2>
<p><a href="https://twitter.com/carrigmat">Matt</a> (ML Engineer at Hugging Face) says -</p>
<blockquote class="blockquote">
<p><em>[…] I did a few small projects to get familiar with Keras and then tried reimplementing papers or building examples to contribute to places like <a href="https://github.com/keras-team/keras-contrib">keras-contrib</a> or <a href="https://keras.io/">keras.io</a>.</em></p>
</blockquote>
<p><br></p>
<p><a href="https://twitter.com/algo_diver">Chansung</a> (ML-GDE and MLOps Engineer) says -</p>
<blockquote class="blockquote">
<p><em>[…] Anyways, I actually didn’t plan what to do for the next few years. I just have followed my interests and the joy to participate as a community member. And whenever I make any moves, I found other exciting events are waiting for me. These days, I am really enjoying creating open-source projects and applied ML products, and collaborative projects with you as well.</em></p>
</blockquote>
<p><br></p>
<p>Both of them continue to push their boundaries for self-improvement and are exceptional at what they do.</p>
</section>
<section id="finishing-up" class="level2">
<h2 class="anchored" data-anchor-id="finishing-up">Finishing Up</h2>
<p>The pursuit of betterment doesn’t stop after you land the job you were aspiring for. I continue to benefit from my open-source engagements even after professionally working in the area for some time now. I hope you’re able to take forward the pointers discussed in the post and experiment with them. If you have any suggestions for other interesting ways for independent ML practice please <a href="mailto:spsayakpaul@gmail.com">let me know</a>.</p>
</section>
<section id="acknowledgments" class="level2">
<h2 class="anchored" data-anchor-id="acknowledgments">Acknowledgments</h2>
<p>I want to thank all my wonderful collaborators and mentors who continue to inspire me to be better. I am also thankful to <a href="https://www.instagram.com/neerajanmusic/">Neerajan Saha</a> for proofreading this post.</p>


</section>

 ]]></description>
  <category>ml-practice</category>
  <category>jobs</category>
  <guid>https://sayak.dev/posts/ml-practice.html</guid>
  <pubDate>Fri, 27 May 2022 00:00:00 GMT</pubDate>
</item>
<item>
  <title>First Steps in GSoC</title>
  <link>https://sayak.dev/posts/gsoc-faqs.html</link>
  <description><![CDATA[ 






<p>In this post, I discuss my perspective on two primary questions pertaining to the <a href="https://summerofcode.withgoogle.com/">Google Summer of Code (GSoC) program</a>. Even though my work is centered around Machine Learning (ML), I believe these pointers are domain-agnostic. This is based on <a href="https://blog.tensorflow.org/2021/09/tensorflow-hubs-experience-with-gsoc-2021.html">my experience of mentoring for TensorFlow at GSoC 2021</a>. Please note that these thoughts are of my own and may not reflect what anyone else (including the program organizers and my employer) thinks.</p>
<section id="how-should-i-get-started" class="level2">
<h2 class="anchored" data-anchor-id="how-should-i-get-started">How should I get started?</h2>
<p>First and foremost, it’s important to acknowledge that GSoC requires some amount of open-source experience beforehand. That not only makes your application stronger but also sets you up for the program itself. But beyond everything else, having a genuine passion for contributing to open-source is important and is a key enabling factor. Open-source should be a fun engagement driven by your passion for helping a community. So, ensure you’re chasing the right things.</p>
<ul>
<li><p>Understand what GSoC is, how it works, what are the rules, and study some projects from the previous years by going to GSoC’s official website: ​​<a href="https://summerofcode.withgoogle.com/" class="uri">https://summerofcode.withgoogle.com/</a>.</p>
<p>Here’s <a href="https://developers.google.com/open-source/gsoc/resources/downloads/GSoC2022Presentation.pdf">another resource</a> that might be equally helpful. It’s important that you set your expectations right from the very beginning. Sometimes having conversations with the past GSoC contributors is really helpful in this regard. <br></p></li>
<li><p>Take a look at the <a href="https://summerofcode.withgoogle.com/programs/2022/organizations">organizations</a> taking part in GSoC. You’ll notice that they have all their projects listed for which they are welcoming contributions. <br></p></li>
<li><p>Study all the official resources that are out there for the project you want to contribute to. You may be interested in multiple projects but it helps to laser focus on one so that you can precisely figure out what components you’d want to work on, your timeline, etc. <br></p></li>
<li><p>Get started contributing. Here are some good examples that make it clear what a GSoC contributor should do first:</p>
<ul>
<li><a href="https://ardupilot.org/dev/docs/gsoc.html">ArduPilot</a></li>
<li><a href="https://gsoc.gnome.org/">GNOME</a></li>
<li><a href="https://pcp.io/gsoc/contributors.html">Performance Co-Pilot</a></li>
<li><a href="https://robocomp.github.io/web/gsoc/2022/contributor_guidance">RoboComp</a></li>
<li><a href="https://www.mediawiki.org/wiki/Google_Summer_of_Code/Participants#Application_process_steps">Wikimedia Foundation</a> <br><br></li>
</ul></li>
<li><p>Sometimes a project may not require having prior contribution experience but having it is almost always better. <a href="https://www.tensorflow.org/hub/overview">TensorFlow Hub</a> (TF-Hub) is one such example where you’re generally asked to code a ML model, (optionally) train it, and host the model on TF-Hub thereby making it easier for the community to use the model. <br><br></p></li>
<li><p>Lastly, if you haven’t worked with a version control system before definitely spend time doing that. Git is a good example of such a system and <a href="https://www.udacity.com/course/version-control-with-git--ud123">here’s</a> a good course from Udacity that can be helpful.</p></li>
</ul>
</section>
<section id="what-makes-a-proposal-great" class="level2">
<h2 class="anchored" data-anchor-id="what-makes-a-proposal-great">What makes a proposal great?</h2>
<p>Firstly, I’d like to acknowledge that the answers to this question should be subjective. That said, I think there are certain common aspects that are shared by all great GSoC proposals.</p>
<ul>
<li><p>GSoC is about building things. So including your experience that reflects the same immediately catches the eye. You build that experience over a period of time, it’s not something built overnight. That way, your experience speaks about a few things: consistency, technical depth, punctuality, communication, etc. Let me provide some examples. <br><br> Say, you wanted to contribute to <a href="https://github.com/keras-team/keras-cv/">KerasCV</a> by adding a new layer(s) to it. If you can show that you’ve already worked on something that reflects the experience relevant to the contribution, it puts you in a better position than someone without that experience.</p>
<p>Similarly, if you wanted to contribute a model to TF-Hub, it helps to show that you’ve experience implementing models and relevant things such as layers, blocks, etc. <br></p></li>
<li><p>When you’re talking about an experience in your proposal be sure to back it with verifiable links. Without that, the mention becomes practically void. <br></p></li>
<li><p>Don’t just mention the components of the project you’d like to work on. Include all the nitty-gritty of that – why you’d like to work on them and why it’s useful, what your approaches will be, etc. If you anticipate some edge cases or blockers include them too. This speaks volumes about your maturity. <br></p></li>
<li><p>Keep your proposal grammatically correct and easily understandable. This helps you communicate your proposal better. Remember that it’s your responsibility to ensure that your proposal was communicated in an expected way. <br><br></p></li>
<li><p>Sometimes, applications come with incomplete sentences, inconsistency in sentence casing, without punctuations, etc. This is an anti-pattern. Try hard to ensure your proposal doesn’t have those things. This may readily reduce the seriousness of your proposal and the work you put into it. <br><br></p></li>
<li><p>Include a realistic timeline that covers the project deliverables and includes enough time for you and the mentors to communicate effectively. Unexpected things can happen all the time so, it helps to also include some extra time to dedicate to those situations.</p></li>
</ul>
<p>Sometimes, a project may welcome ideas from the contributors. If you’d like to propose something that’s already not enlisted in a project, be sure to reach out to the project mentor to discuss the feasibility of your idea before working on the proposal.</p>
</section>
<section id="additional-notes" class="level2">
<h2 class="anchored" data-anchor-id="additional-notes">Additional notes</h2>
<p>During <a href="https://summerofcode.withgoogle.com/archive/2021/organizations/6649841832165376">GSoC 2021</a>, I had the opportunity to work with Aditya Kane and Vausdev Gupta as their mentor. Here are their GSoC proposals:</p>
<ul>
<li><a href="https://docs.google.com/document/d/1h9kZCywWWveekUFH1SS5rBZINK-W9SDb/edit">Aditya</a></li>
<li><a href="https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/assets/milestone.pdf">Vasudev</a></li>
</ul>
<p>I’m fortunate to be mentoring for TensorFlow at GSoC 2022 as well. If you’re curious, I have a <a href="https://www.youtube.com/watch?v=VsBEFUoESR4&amp;t=3123s">presentation</a> that discusses how open-source can enable different possibilities in the ML world. If you’re looking for an example of a non-ML proposal, then <a href="https://drive.google.com/file/d/1c-nqgm54pIvm_YQKJ4SohLUV6iKGGYH6/view">Anubhav Singh’s proposal</a> is a great example.</p>
<p>Additionally, we penned down our mentorship experience in <a href="https://blog.tensorflow.org/2021/09/tensorflow-hubs-experience-with-gsoc-2021.html">this blog post</a> that may provide additional context.</p>


</section>

 ]]></description>
  <category>gsoc</category>
  <category>open-source</category>
  <guid>https://sayak.dev/posts/gsoc-faqs.html</guid>
  <pubDate>Sun, 13 Mar 2022 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Publishing ConvNeXt Models on TensorFlow Hub</title>
  <link>https://sayak.dev/posts/convnext-tfhub.html</link>
  <description><![CDATA[ 






<p>I recently added 15 different variants of the <a href="https://arxiv.org/abs/2201.03545">ConvNeXt architecture</a> to <a href="https://tfhub.dev/sayakpaul/collections/convnext/1">TensorFlow Hub</a> (TF-Hub). This post is a reflection of what had to be done to get to that point. First, we’ll discuss the implementation of ConvNeXt in Keras and how the original pre-trained parameters were ported into these models. We’ll then talk about TF-Hub’s ConvNeXt collection and what it offers.</p>
<p>I hope this post is useful for anyone willing to contribute models to TF-Hub as doing it the right way can be a good amount of work.</p>
<section id="about-convnext" class="level1">
<h1>About ConvNeXt</h1>
<p>ConvNeXt models were proposed by Liu et al.&nbsp;in <a href="https://arxiv.org/abs/2201.03545">A ConvNet for the 2020s</a>. ConvNeXt models are composed of standard layers such as depthwise convolutions, layer normalization, etc., and use standard network topologies. They don’t use self-attention or any hybrid approaches, unlike the recent architectures such as <a href="https://arxiv.org/abs/2010.11929">Vision Transformers</a>, <a href="https://arxiv.org/abs/2106.04803">CoAtNet</a>, etc. The authors start with a base architecture and gradually refine it to match some of the design choices of <a href="https://arxiv.org/abs/2103.14030">Swin Transformers</a>. In the process, they developed a family of models named ConvNeXt achieving performance on the <a href="https://www.image-net.org/">ImageNet-1k dataset</a> with efficiency. For details, check out the <a href="https://arxiv.org/abs/2201.03545">original paper</a>.</p>
<img src="https://github.com/sayakpaul/portfolio/raw/master/posts/convnext_tfhub/convnext.png" class="img-fluid">
<center>
<b>Figure 1</b>: ConvNeXt performance (source: original paper).
</center>
</section>
<section id="implementation-and-weight-porting" class="level1">
<h1>Implementation and weight porting</h1>
<p>The ConvNeXt models are fairly easy to implement especially with the <a href="https://github.com/facebookresearch/ConvNeXt">official PyTorch codebase available</a> for reference. As mentioned before, these models can be implemented using the standard components provided in most of the major deep learning frameworks such as JAX, PyTorch, and TensorFlow.</p>
<p>ConvNeXt models use the following block structure with layer scaling as introduced in <a href="https://arxiv.org/abs/2103.17239">Going deeper with image transformers</a> by Touvron et al.</p>
<center>
<figure class="figure">
<img src="https://github.com/sayakpaul/portfolio/raw/master/posts/convnext_tfhub/convnext_block.png" class="figure-img">
</figure>
</center>
<center>
<b>Figure 2</b>: ConvNeXt block (source: original paper).
</center>
<p>The skip connection is controlled with <a href="https://arxiv.org/abs/1603.09382">Stochastic Depth</a> to induce regularization <em>during</em> training. Different ConvNeXt variants correspond to different depths along with different channels used in each of the stages. For example, the “tiny” variant uses the following setup:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb1-1">depths <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>]</span>
<span id="cb1-2">dims <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">96</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">192</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">384</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">768</span>]</span></code></pre></div>
<p>If you plan to populate the implemented models with the original parameters then it helps to align the architecture implementation with the official one as much as possible. Since I went with this approach I tried closely following <a href="https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py">the official implementation</a>. My final implementation is available in <a href="https://github.com/sayakpaul/ConvNeXt-TF/blob/main/models/convnext_tf.py">this script</a>. Note that, it does not yet include the <a href="https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext_isotropic.py">isotropic ConvNeXt models</a>.</p>
<p>Coming to the weight porting part, this is usually the most interesting part because there’s no standard recipe that’d work for all the models. You’ll need to think about how to best align the original model parameters with your implementation.</p>
<p>A ConvNeXt model is divided into three main parts: (1) stem which directly operates on the input image, (2) downsample blocks that reduce the resolution of feature maps as the network progresses, and (3) stages that apply the ConvNeXt blocks shown above. This is why I organized my weight porting script such that it has a correspondence between these different parts with the original parameters. Here is an example:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> layer <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> stem_block.layers:</span>
<span id="cb2-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(layer, tf.keras.layers.Conv2D):</span>
<span id="cb2-3">        layer.kernel.assign(</span>
<span id="cb2-4">            tf.Variable(param_list[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].numpy().transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>))</span>
<span id="cb2-5">        )</span>
<span id="cb2-6">        layer.bias.assign(tf.Variable(param_list[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].numpy()))</span>
<span id="cb2-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(layer, tf.keras.layers.LayerNormalization):</span>
<span id="cb2-8">        layer.gamma.assign(tf.Variable(param_list[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>].numpy()))</span>
<span id="cb2-9">        layer.beta.assign(tf.Variable(param_list[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>].numpy()))</span></code></pre></div>
<p>The most difficult bit was figuring out how to properly populate the weights of the convolutional layers in TensorFlow from PyTorch. In an earlier implementation, I was simply using <code>transpose()</code>. The resulting models were giving poorer performance than expected. <a href="https://in.linkedin.com/in/vasudevgupta7">Vasudev</a> helped me figure out the correct transposition of the weight axes and the models were then coming out as expected. More about the evaluation of these models in a moment.</p>
<p>Once the weights were ported successfully, the next task was to verify if the outputs of the intermediate layers matched with their original counterparts. One minor detail to note here is that the outputs of layers are <strong>not</strong> the same as their parameters. So, even if you check if the parameters of your implemented model and the original model are matching, their outputs could still mismatch. This mainly happens because of mismatches between the layer configurations of your model and the original one.</p>
<p>The final model conversion script is available <a href="https://github.com/sayakpaul/ConvNeXt-TF/blob/main/convert.py">here</a>.</p>
</section>
<section id="evaluation-of-the-models" class="level1">
<h1>Evaluation of the models</h1>
<p>To be more certain, it’s also important to check the evaluation metrics of the converted models on the datasets used during training. In this case, we need to use the top-1 accuracy of the models on the ImageNet-1k dataset (validation set).</p>
<p>To set up this evaluation, I developed <a href="https://github.com/sayakpaul/ConvNeXt-TF/blob/main/i1k_eval/eval.ipynb">this notebook</a> where I closely followed <a href="https://github.com/facebookresearch/ConvNeXt/blob/main/datasets.py">the preprocessing used in the official codebase</a> for inference. The following table reflects the top-1 accuracies of the converted models along with the original scores reported <a href="https://github.com/facebookresearch/ConvNeXt/#results-and-pre-trained-models">here</a>.</p>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
</style>
<table class="tg">
<thead>
<tr>
<th class="tg-c3ow">
<span style="font-weight:bold">Name</span>
</th>
<th class="tg-c3ow">
<span style="font-weight:bold">Original acc@1</span>
</th>
<th class="tg-c3ow">
<span style="font-weight:bold">Keras acc@1</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-c3ow">
convnext_tiny_1k_224
</td>
<td class="tg-c3ow">
82.1
</td>
<td class="tg-c3ow">
81.312
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_small_1k_224
</td>
<td class="tg-c3ow">
83.1
</td>
<td class="tg-c3ow">
82.392
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_base_1k_224
</td>
<td class="tg-c3ow">
83.8
</td>
<td class="tg-c3ow">
83.28
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_base_1k_384
</td>
<td class="tg-c3ow">
85.1
</td>
<td class="tg-c3ow">
84.876
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_large_1k_224
</td>
<td class="tg-c3ow">
84.3
</td>
<td class="tg-c3ow">
83.844
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_large_1k_384
</td>
<td class="tg-c3ow">
85.5
</td>
<td class="tg-c3ow">
85.376
</td>
</tr>
<tr>
<td class="tg-c3ow">
</td>
<td class="tg-c3ow">
</td>
<td class="tg-c3ow">
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_base_21k_1k_224
</td>
<td class="tg-c3ow">
85.8
</td>
<td class="tg-c3ow">
85.364
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_base_21k_1k_384
</td>
<td class="tg-c3ow">
86.8
</td>
<td class="tg-c3ow">
86.79
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_large_21k_1k_224
</td>
<td class="tg-c3ow">
86.6
</td>
<td class="tg-c3ow">
86.36
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_large_21k_1k_384
</td>
<td class="tg-c3ow">
87.5
</td>
<td class="tg-c3ow">
87.504
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_xlarge_21k_1k_224
</td>
<td class="tg-c3ow">
87.0
</td>
<td class="tg-c3ow">
86.732
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_xlarge_21k_1k_384
</td>
<td class="tg-c3ow">
87.8
</td>
<td class="tg-c3ow">
87.68
</td>
</tr>
</tbody>
</table>
<p><code>Keras acc@1</code> refers to the scores of my implementation. Differences in the results are primarily because of the differences in the library implementations, especially how image resizing is implemented in PyTorch and TensorFlow. My evaluation logs are available at <a href="https://tensorboard.dev/experiment/odN7OPCqQvGYCRpJP1GhRQ/">this URL</a>. I’d like to thank <a href="https://twitter.com/gusthema">Gus</a> from the TF-Hub team for the productive discussions during this phase.</p>
</section>
<section id="publishing-on-tf-hub" class="level1">
<h1>Publishing on TF-Hub</h1>
<p>With the models converted as expected, I was now tasked with publishing them on TF-Hub. These models can be categorized into two different variants: (1) off-the-shelf classifiers and (2) feature extractors used for downstream tasks. This means that the 15 model variants that I had converted would actually amount to 30 models.</p>
<p>Whenever I publish models on TF-Hub, I try to accompany each model with the following:</p>
<ul>
<li>Documentation that includes references of the models, how it was exported, etc.</li>
<li>Colab Notebook showing the model usage.</li>
</ul>
<p>Doing these things (especially the documentation part) for 30 models can be quite cumbersome. <a href="https://www.linkedin.com/in/willi-gierke/">Willi</a> from the TF-Hub team supported me in automatically generating documentation for 30 models. The script is available <a href="https://github.com/sayakpaul/ConvNeXt-TF/blob/main/hub_utilities/generate_doc.py">here</a>. This script was basically generated from a documentation template and can be used for generating documentation when publishing more than one model. Additionally, I worked on a <a href="https://github.com/sayakpaul/ConvNeXt-TF/blob/main/hub_utilities/export_to_hub.py">script</a> that can archive the TensorFlow SavedModels in a way accepted by TF-Hub.</p>
<p>I hope these scripts will be beneficial for anyone planning to contribute models to TF-Hub.</p>
<p>As of today, all 30 models are <a href="https://tfhub.dev/sayakpaul/collections/convnext/1">available on TF-Hub</a>. They come with Colab Notebooks and documentation so that it’s easier to get started. Moreover, these TF-Hub models are not black-box SavedModels. You can load them as <code>tf.keras.Model</code> objects for further inspection. Here’s an example:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb3-1">model_gcs_path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gs://tfhub-modules/sayakpaul/convnext_tiny_1k_224/1/uncompressed"</span></span>
<span id="cb3-2">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.models.load_model(model_gcs_path)</span>
<span id="cb3-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(model.summary(expand_nested<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>))</span></code></pre></div>
</section>
<section id="wrapping-up" class="level1">
<h1>Wrapping up</h1>
<p>That’s it for the post. We discussed a standard workflow that I use to publish models on TF-Hub and the difficulties that can arise during the process. I hope you’ve found it to be worthy of your time and thank you for reading!</p>
</section>
<section id="acknowledgements" class="level1">
<h1>Acknowledgements</h1>
<ul>
<li><a href="https://github.com/vasudevgupta7">Vasudev</a> for helping with transposition bug</li>
<li><a href="https://twitter.com/gusthema">Gus</a> for fruitful discussions</li>
<li><a href="https://www.linkedin.com/in/willi-gierke/">Willi</a> for helping with publishing</li>
<li><a href="https://developers.google.com/programs/experts/">ML-GDE program</a> for providing Google Cloud Platform credits</li>
</ul>


</section>

 ]]></description>
  <category>tensorflow</category>
  <category>keras</category>
  <category>cnns</category>
  <category>imagenet-1k</category>
  <category>convnext</category>
  <guid>https://sayak.dev/posts/convnext-tfhub.html</guid>
  <pubDate>Thu, 03 Feb 2022 00:00:00 GMT</pubDate>
  <media:content url="https://sayak.dev/posts/convnext_tfhub/convnext_header.png" medium="image" type="image/png" height="93" width="144"/>
</item>
<item>
  <title>Building and Installing OpenCV 4.5.0 on an M1 Mac</title>
  <link>https://sayak.dev/posts/install-opencv-m1.html</link>
  <description><![CDATA[ 






<p>This post shows how to build and install OpenCV 4.5.0 on a MacBook Pro that comes with an <a href="https://www.apple.com/in/mac/m1/">M1 chip</a>. Yes, you guessed it right - as of <strong>January 01, 2021</strong>, there’s no pre-compiled OpenCV binary compatible with this MacBook Pro variant. So, open up a terminal and get started!</p>
<p>Here’s a brief summary of the configuration of my MacBook -</p>
<p><img src="https://i.ibb.co/XDZZQ4t/image.png" class="img-fluid"></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The following steps should run well on an M1 MacBook Air too.</p>
</div>
</div>
<section id="install-xcode-and-homebrew" class="level2">
<h2 class="anchored" data-anchor-id="install-xcode-and-homebrew">Install Xcode and Homebrew</h2>
<p>We start by executing <code>sudo xcodebuild -license</code> from a terminal.</p>
<p>When you execute the above command, you would need to accept the Xcode license. Then, in order to make use of Apple command line tools, we need to install it - <code>sudo xcode-select --install</code>.</p>
<p>Homebrew manages packages on a Mac. In order to install it execute the following - <code>/usr/bin/ruby -e "%(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"</code>.</p>
<p>You would want to add the command brew after the installation is complete. To do so, execute the following - <code>nano ~/.zshrc</code></p>
<p>Then insert <code>export PATH=$PATH:/opt/homebrew/bin</code> into it and press <code>Ctrl + X</code> from your keyboard. Then execute <code>source ~/.zshrc</code> from the terminal.</p>
<p>Note that the exact path to Homebrew might be different for your system, so please double check that.</p>
<p>Next up, we install a few system-level utilities required by OpenCV on a Mac.</p>
</section>
<section id="install-conda" class="level2">
<h2 class="anchored" data-anchor-id="install-conda">Install conda</h2>
<p>My favorite Python virtual environment manager is virtualenv. Unfortunately, it does not play out that well with the new M1 chip. This is mostly because the pip-installable packages often break during their installations on the chip. This is why conda, specifically its <strong>miniforge</strong> distribution is the recommended package manager for a Mac shipped with M1. You can install it from <a href="https://github.com/conda-forge/miniforge#miniforge3">here</a>. This installs <strong>Python 3.8</strong>.</p>
<p>After the installation is complete, please create a new Python virtual environment by executing <code>conda create --name &lt;environment_name&gt;</code>. Then activate it by running <code>conda activate  &lt;environment_name&gt;</code>.</p>
<p>Running <code>conda install -y python==3.8.6</code> will install a few common Python packages for you. I highly recommend running this.</p>
</section>
<section id="install-numpy" class="level2">
<h2 class="anchored" data-anchor-id="install-numpy">Install NumPy</h2>
<p>NumPy is needed by OpenCV. So, we need to install it before we build and install OpenCV. Apple provides a <code>numpy</code> wheel that is compatible with the M1 chip. Follow the steps below to install it -</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> wget https://github.com/apple/tensorflow_macos/releases/download/v0.1alpha0/tensorflow_macos-0.1alpha0.tar.gz</span>
<span id="cb1-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> tar xvf tensorflow_macos-0.1alpha0.tar.gz</span>
<span id="cb1-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> cd tensorflow_macos/arm64</span>
<span id="cb1-4"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> pip install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--upgrade</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--no-dependencies</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--force</span> numpy-1.18.5-cp38-cp38-macosx_11_0_arm64.whl </span>
<span id="cb1-5"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> cd ~</span></code></pre></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Be sure to activate your conda environment before doing the pip-install.</p>
</div>
</div>
</section>
<section id="compile-opencv" class="level2">
<h2 class="anchored" data-anchor-id="compile-opencv">Compile OpenCV</h2>
<p>First, let’s download the OpenCV and OpenCV extended module files and prepare them for compilation.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> wget <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-O</span> opencv.zip https://github.com/opencv/opencv/archive/4.5.0.zip</span>
<span id="cb2-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> wget <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-O</span> opencv_contrib.zip https://github.com/opencv/opencv_contrib/archive/4.5.0.zip</span>
<span id="cb2-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> unzip opencv.zip</span>
<span id="cb2-4"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> unzip opencv_contrib.zip</span>
<span id="cb2-5"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> cd opencv-4.5.0</span>
<span id="cb2-6"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> mkdir build <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">cd</span> build</span></code></pre></div>
<p>Now, we are all set to fire the <code>cmake</code> command that would build OpenCV for us. Let’s review it briefly -</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> cmake <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-2">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-DCMAKE_SYSTEM_PROCESSOR</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>arm64 <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-3">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-DCMAKE_OSX_ARCHITECTURES</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>arm64 <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-4">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-DWITH_OPENJPEG</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OFF <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-5">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-DWITH_IPP</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OFF <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-6">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> CMAKE_BUILD_TYPE=RELEASE <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-7">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> CMAKE_INSTALL_PREFIX=/usr/local <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-8">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> OPENCV_EXTRA_MODULES_PATH=/Users/sayakpaul/Downloads/opencv_contrib-4.5.0/modules <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-9">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> PYTHON3_EXECUTABLE=/Users/sayakpaul/miniforge3/envs/dev/bin/python3 <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-10">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> BUILD_opencv_python2=OFF <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-11">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> BUILD_opencv_python3=ON <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-12">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> INSTALL_PYTHON_EXAMPLES=ON <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-13">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> INSTALL_C_EXAMPLES=OFF <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-14">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> OPENCV_ENABLE_NONFREE=ON <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-15">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> BUILD_EXAMPLES=ON ..</span></code></pre></div>
<p>As per <a href="https://github.com/opencv/opencv/issues/18049#issuecomment-671878454">this issue comment</a>, <code>DCMAKE_SYSTEM_PROCESSOR</code>, <code>DCMAKE_OSX_ARCHITECTURES</code>, <code>DWITH_OPENJPEG</code>, and <code>DWITH_IPP</code> are needed to be set during the compilation step. Also, please pay attention to the following arguments - <code>OPENCV_EXTRA_MODULES_PATH</code> and <code>PYTHON3_EXECUTABLE</code>. For these two arguments, you would want to first determine the paths and then supply them accordingly.</p>
<p>Now, before you run the above <code>cmake</code> command, activate the conda environment you created in an earlier step (<code>conda activate &lt;environment_name&gt;</code>) if you haven’t already. The compilation took <em>~3 minutes</em> for me and it should produce outputs like so -</p>
<p><img src="https://i.ibb.co/YdpBSh0/image.png" class="img-fluid"></p>
<p>Next, we launch the make command - <code>make -j8</code>. With all the eight cores (<code>j8</code> stands for eight cores here) chugging along, this step took <em>~8 minutes</em> for me. You can adjust the <code>j</code> option with respect to the hardware available. After it’s done you should get an output like so -</p>
<p><img src="https://i.ibb.co/yFJq4jJ/image.png" class="img-fluid"></p>
<p>The final step here is to execute - <code>sudo make install</code>. It should take just a few seconds to complete execution. Upon successful completion, you should get an output like so -</p>
<p><img src="https://i.ibb.co/Pzzmxy4/image.png" class="img-fluid"></p>
</section>
<section id="sym-link-opencv-4-on-macos-to-virtual-environment-site-packages" class="level2">
<h2 class="anchored" data-anchor-id="sym-link-opencv-4-on-macos-to-virtual-environment-site-packages">Sym-link OpenCV 4 on macOS to virtual environment <code>site-packages</code></h2>
<p>To do this, we first need to locate the <code>.so</code> file generated during the compilation step. We can do this with the <code>mdfind</code> command -</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> mdfind cv2.cpython</span>
<span id="cb4-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">/.../opencv-4.5.0/build/lib/python3/cv2.cpython-38-darwin.so</span></span>
<span id="cb4-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">/usr/local/lib/python3.8/site-packages/cv2/python-3.8/cv2.cpython-38-darwin.so</span></span>
<span id="cb4-4"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">...</span></span></code></pre></div>
<p>Please note that I obfuscated some parts of the outputs for privacy reasons. In the above output, we can see the absolute locations for the <code>.so</code> files that were generated. Now, we need to execute the following to sym-link one of the <code>.so</code> files in our current Python virtual environment -</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> cd /.../miniforge3/envs/dev/lib/python3.8/site-packages</span>
<span id="cb5-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> ln <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-s</span> /usr/local/lib/python3.8/site-packages/cv2/python-3.8/cv2.cpython-38-darwin.so cv2.so</span></code></pre></div>
<p>Please double-check the paths before executing the commands.</p>
<p>And that’s it!</p>
<p>You can test the installation by executing the following -</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> conda activate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span>environment_name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">(</span><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">you</span> haven<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'t already)</span></span>
<span id="cb6-2"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">% python</span></span>
<span id="cb6-3"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">&gt;&gt;&gt; import cv2</span></span>
<span id="cb6-4"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">&gt;&gt;&gt; cv2.__version__</span></span></code></pre></div>
<p>It should print <code>'4.5.0'</code>.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://www.pyimagesearch.com/2018/08/17/install-opencv-4-on-macos/">Install OpenCV 4 on macOS</a></li>
<li><a href="https://towardsdatascience.com/tensorflow-2-4-on-apple-silicon-m1-installation-under-conda-environment-ba6de962b3b8">TensorFlow 2.4 on Apple Silicon M1 : installation under Conda environment</a></li>
</ul>


</section>

 ]]></description>
  <guid>https://sayak.dev/posts/install-opencv-m1.html</guid>
  <pubDate>Fri, 01 Jan 2021 00:00:00 GMT</pubDate>
  <media:content url="https://sayak.dev/posts/opencv_m1.png" medium="image" type="image/png" height="87" width="144"/>
</item>
<item>
  <title>A Battle of Text Detectors for Mobile Deployments: CRAFT vs. EAST</title>
  <dc:creator>Sayak Paul</dc:creator>
  <dc:creator>Tulasi Ram Laghumavarapu</dc:creator>
  <link>https://sayak.dev/posts/optimizing-text-detectors.html</link>
  <description><![CDATA[ 






<p>In the <a href="https://tulasi.dev/craft-in-tflite">previous post</a>, we saw how to convert the pre-trained <a href="https://arxiv.org/pdf/1904.01941">CRAFT</a> model from PyTorch to TensorFlow Lite (TFLite) and run inference with the converted TFLite model. In this post, we will be comparing the TFLite variants of the CRAFT model to another text detection model - <a href="https://arxiv.org/abs/1704.03155">EAST</a>. The objective of this post is to provide a comparative study between these two models with respect to various deployment-specific pointers such as inference latency, model size, performance on dense text regions, and so on. Text detection continues to be a very important use-case across many verticals. So we hope this post will serve as a systematic guide for developers that are interested to explore on-device text detection models.</p>
<p>Precisely, we will be comparing the two models on the basis of the following pointers which we think are very crucial when it comes to deploying them out in the wild -</p>
<ul>
<li>Visual Inspection of Performance</li>
<li>Model Size</li>
<li>Inference Latency</li>
<li>Memory Usage</li>
</ul>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you are interested to know about the conversion process and inference pipelines of the models, please refer to these notebooks - <a href="https://github.com/tulasiram58827/craft_tflite/tree/main/colabs">CRAFT</a> and <a href="https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/EAST_TFLite.ipynb">EAST</a>. The pre-converted models are available on TensorFlow Hub - <a href="https://tfhub.dev/tulasiram58827/lite-model/craft-text-detector/dr/1">CRAFT</a> and <a href="https://tfhub.dev/sayakpaul/lite-model/east-text-detector/dr/1">EAST</a>.</p>
</div>
</div>
<section id="benchmark-setup" class="level2">
<h2 class="anchored" data-anchor-id="benchmark-setup">Benchmark Setup</h2>
<p>We used the <a href="https://www.tensorflow.org/lite/performance/measurement">TensorFlow Lite Benchmark tool</a> in order to gather results on inference latency and memory usage of the models with <strong>Redmi K20 Pro</strong> as the target device. We chose a mobile device for this purpose because text detection is a pretty prevalent recipe of many mobile applications such as <a href="https://play.google.com/store/apps/details?id=com.google.ar.lens&amp;hl=en_IN&amp;gl=US">Google Lens</a>.</p>
<p>In order to make the comparisons fair, we consider the two models with three different image resolutions - 320x320, 640x416, and 1200x800. For each of these resolutions, we consider two different <a href="https://www.tensorflow.org/lite/performance/post_training_quantization">post-training quantization schemes</a> - dynamic-range and float16. <em>The CRAFT model conversion is not yet supported in the integer variant, hence we do not consider integer quantization (but the EAST model does support it)</em>.</p>
</section>
<section id="visual-inspection-of-performance" class="level2">
<h2 class="anchored" data-anchor-id="visual-inspection-of-performance">Visual Inspection of Performance</h2>
<p>In this setting, we run both of the models and their different variants (dynamic-range and float16 quantized) on a sample image that has dense text regions, and then we visualize the results. We observed that both of these models perform fairly well on images having lighter text regions. Here’s the sample image we used for the purpose -</p>
<p><img src="https://i.ibb.co/KVKnnct/image.png" class="img-fluid"></p>
<center>
<small>Image is taken from the <a href="https://rrc.cvc.uab.es/?ch=13">SROIE dataset</a>.</small><br>
</center>
<p>Time to detect some texts!</p>
<section id="craft---320x320-dynamic-range-float16" class="level3">
<h3 class="anchored" data-anchor-id="craft---320x320-dynamic-range-float16">CRAFT - 320x320 Dynamic-Range &amp; float16</h3>
<p>In the dynamic-range quantization setting, we can see the model misses out on some text blocks.</p>
<p><img src="https://i.ibb.co/RBX8XDn/image-w-593-h-442-rev-1-ac-1-parent-19-Qb-WABWc-E3n-SLPE6zqm-Tw6b-Vxkxee-YUw-Om-KTDn-Dz8k-Q.png" class="img-fluid"></p>
<center>
<small>Inference results from the 320x320 dynamic-range and float16 quantized CRAFT models.</small><br>
</center>
<p>With increased numerical precision i.e.&nbsp;float16, we can clearly see quite a bit of improvement in the results. It’s important to note that this improvement comes at the cost of increased model size.</p>
<p>Next up, we apply the same steps to the EAST model.</p>
</section>
<section id="east---320x320-dynamic-range-float16" class="level3">
<h3 class="anchored" data-anchor-id="east---320x320-dynamic-range-float16">EAST - 320x320 Dynamic-Range &amp; float16</h3>
<p>EAST apparently performs better than CRAFT under dynamic-range quantization. If we look closely, it appears that the CRAFT model produces far fewer overlaps in the detections compared to EAST. On the other hand, the EAST model is able to detect more text blocks. When developing practical applications with text detectors, it often becomes a classic case of <em>precision-recall</em> trade-offs like the one we are currently seeing. So, you would want to consider the application-specific needs in order to decide the level of trade-off to be achieved there.</p>
<p><img src="https://i.ibb.co/qsCMC5N/image-w-624-h-520-rev-37-ac-1-parent-19-Qb-WABWc-E3n-SLPE6zqm-Tw6b-Vxkxee-YUw-Om-KTDn-Dz8k-Q.png" class="img-fluid"></p>
<center>
<small>Inference results from the 320x320 dynamic-range and float16 quantized EAST models.</small><br>
</center>
<p>With increased precision, the above-mentioned points still hold, i.e.&nbsp;the number of overlaps being way higher for the EAST model than they are in the CRAFT equivalent. In this setting (float16 quantization), superiority in the performance of the CRAFT model is quite evident in regards to the EAST model.</p>
<p>As different applications may use different image resolutions we decided to test the performance of the models on larger dimensions as well. This is what we are going to see next.</p>
</section>
<section id="craft---640x416-dynamic-range-float16" class="level3">
<h3 class="anchored" data-anchor-id="craft---640x416-dynamic-range-float16">CRAFT - 640x416 Dynamic-Range &amp; float16</h3>
<p>On an increased resolution, the CRAFT model performs pretty well -</p>
<p><img src="https://i.ibb.co/VxbyWch/image-w-624-h-568-rev-38-ac-1-parent-19-Qb-WABWc-E3n-SLPE6zqm-Tw6b-Vxkxee-YUw-Om-KTDn-Dz8k-Q.png" class="img-fluid"></p>
<center>
<small>Inference results from the 640x416 dynamic-range and float16 quantized CRAFT models.</small><br>
</center>
<p>The float16 version of this resolution is a slam dunk (rightfully leaving behind the barcode which is not a piece of text).</p>
</section>
<section id="east---640x416-dynamic-range-float16" class="level3">
<h3 class="anchored" data-anchor-id="east---640x416-dynamic-range-float16">EAST - 640x416 Dynamic-Range &amp; float16</h3>
<p>The performance of the EAST model under these settings are very equivalent to CRAFT -</p>
<p><img src="https://i.ibb.co/ynBbrFZ/image-w-597-h-612-rev-36-ac-1-parent-19-Qb-WABWc-E3n-SLPE6zqm-Tw6b-Vxkxee-YUw-Om-KTDn-Dz8k-Q.png" class="img-fluid"></p>
<center>
<small>Inference results from the 640x416 dynamic-range and float16 quantized EAST models.</small><br>
</center>
<p>With float16 quantization and 640x416 as the resolution, the CRAFT model is a clear winner. Notice that the EAST model is still unable to discard the barcode part which might be an important point to note for some applications.</p>
<p>Time to inspect the results for our final and highest resolution - 1280x800.</p>
</section>
<section id="craft---1280x800-dynamic-range-float16" class="level3">
<h3 class="anchored" data-anchor-id="craft---1280x800-dynamic-range-float16">CRAFT - 1280x800 Dynamic-Range &amp; float16</h3>
<p>Under dynamic-range quantization, the results look okayish. The model misses out on a number of text blocks but the only ones that it detects appear to be neat.</p>
<p><img src="https://i.ibb.co/QMDpH9M/image-w-624-h-453-rev-34-ac-1-parent-19-Qb-WABWc-E3n-SLPE6zqm-Tw6b-Vxkxee-YUw-Om-KTDn-Dz8k-Q.png" class="img-fluid"></p>
<center>
<small>Inference results from the 1280x800 dynamic-range and float16 quantized CRAFT models.</small><br>
</center>
<p>The results from the float16 variant are tremendous (as you probably have guessed by now).</p>
</section>
<section id="east---1280x800-dynamic-range-float16" class="level3">
<h3 class="anchored" data-anchor-id="east---1280x800-dynamic-range-float16">EAST - 1280x800 Dynamic-Range &amp; float16</h3>
<p>At this resolution, the EAST model seems to be performing well too -</p>
<p><img src="https://i.ibb.co/xYHfXXn/image-w-624-h-483-rev-29-ac-1-parent-19-Qb-WABWc-E3n-SLPE6zqm-Tw6b-Vxkxee-YUw-Om-KTDn-Dz8k-Q.png" class="img-fluid"></p>
<center>
<small>Inference results from the 1280x800 dynamic-range and float16 quantized EAST models.</small><br>
</center>
<p>With float16 quantization as well, the CRAFT model beats EAST in terms of the detection quality.</p>
</section>
</section>
<section id="model-size" class="level2">
<h2 class="anchored" data-anchor-id="model-size">Model Size</h2>
<p>When it comes to deploying models to mobile devices model size becomes a really important factor. You may not want to have a heavy model that would, in turn, make your mobile application bulky. Moreover, <a href="https://support.google.com/googleplay/android-developer/answer/113469#apk">Playstore</a> and <a href="https://developer.apple.com/forums/thread/12455">AppStore</a> also have size restrictions on the applications one can host there.</p>
<p>On the other hand, heavier models tend to be slower. If your application cannot have increased inference latency then you would want to have the model size as low as possible.</p>
<p>The following figure shows the size of the CRAFT and EAST models -</p>
<p><img src="https://i.ibb.co/tX7bknk/nyrm-wh-z-itikr9-cnyl6-z1-fq3.png" class="img-fluid"></p>
<center>
<small>Model (TFLite variants) sizes of CRAFT and EAST.</small><br>
</center>
<p>The dynamic-range quantized versions of both the models are in a well-acceptable range with respect to size. However, the float16 variants may still be a bit heavier for some applications.</p>
</section>
<section id="inference-latency" class="level2">
<h2 class="anchored" data-anchor-id="inference-latency">Inference Latency</h2>
<p>Inference latency is also one of the major factors for mobile-based deployments especially when your applications might require instantaneous predictions. We are going to show a comparison between all the settings we considered in the visual inspection section.</p>
<p>To reiterate we performed the benchmarks for this section on a Redmi K20 Pro using 4 threads. In the following figures, we present inference latency of different variants of the CRAFT and EAST models.</p>
<p><img src="https://i.ibb.co/1GyPgR6/ylz3-vh2l-ownf4av-amai-w0j-oz.png" class="img-fluid"></p>
<center>
<small>Inference latency of different variants of the CRAFT model.</small><br>
</center>
<p><img src="https://i.ibb.co/ySBsQvs/z-q-o-zf7cl-hu-tfh-ou-a7-yscgm.png" class="img-fluid"></p>
<center>
<small>Inference latency of different variants of the EAST model.</small><br>
</center>
<p>As expected, with increased resolution the inference latency also increases. Inference latency is also quite lower for all the variants of the EAST model compared to CRAFT. Earlier we saw how a quantization affects model performance under a particular resolution. As stated earlier, when using these models inside a mobile application, the “<em>Size vs.&nbsp;Performance</em>” trade-off becomes extremely vital.</p>
<blockquote class="blockquote">
<p>important: The results for the float16 1280x800 CRAFT model could not be obtained on our target device.</p>
</blockquote>
</section>
<section id="memory-usage" class="level2">
<h2 class="anchored" data-anchor-id="memory-usage">Memory Usage</h2>
<p>In section, we shed light on the total memory allocated for the models while running the TensorFlow Lite Benchmark tool. Knowing about the memory usage of these models helps us plan application releases accordingly as not all the mobile phones may support extensive memory requirements. So based on this information, you may want to set some device requirements for your application using these models. On the other hand, if you would want your application to be as device-agnostic as possible then you may want to maintain separate models according to their size and memory usage.</p>
<p>In this case, also, we are going to consider all the settings we had considered in the previous sections. The following figures give us a sense of the memory footprint left behind by the models -</p>
<p><img src="https://i.ibb.co/TrnZ9vX/webp-net-resizeimage.png" class="img-fluid"></p>
<center>
<small>Memory footprint of different variants of the CRAFT model.</small><br>
</center>
<p><img src="https://i.ibb.co/3szkpK0/hfp-jmc4-nej-lloj-bc2-q-nz515y.png" class="img-fluid"></p>
<center>
<small>Memory footprint of different variants of the EAST model.</small><br>
</center>
<p>Detection performance-wise, CRAFT was a winner in many cases but if we factor in for inference latency and memory footprint the situation might need reconsideration. In other words, the best performing (with respect to a certain task, detection in this case) model may not always be the best candidate for deployments.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>The results for the float16 1280x800 CRAFT model could not be obtained on our target device.</p>
</div>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In this post, we presented a comparative study between two text detection models - CRAFT and EAST. We went beyond their task-specific performance and considered various essential factors that one needs to consider when deploying these models. At this point, you might have felt the need to consider another important factor of these models - <em>FPS information of the models on real-time videos</em>. Please check out <a href="https://github.com/farmaker47/OCR_with_Keras">this repository</a> to get a handle on how to approach that development.</p>
</section>
<section id="contribution" class="level2">
<h2 class="anchored" data-anchor-id="contribution">Contribution</h2>
<p><a href="https://www.linkedin.com/in/tulasi-ram-laghumavarapu-aba672103/">Tulasi</a> worked on the CRAFT model while Sayak worked on the EAST model. For the purpose of this post, Tulasi focused on gathering all the relevant information for doing the comparisons while Sayak focused on the writing part.</p>
<p>Thanks to <a href="https://twitter.com/khanhlvg">Khanh LeViet</a> from the TFLite team for reviewing the post.</p>


</section>

 ]]></description>
  <guid>https://sayak.dev/posts/optimizing-text-detectors.html</guid>
  <pubDate>Fri, 27 Nov 2020 00:00:00 GMT</pubDate>
  <media:content url="https://sayak.dev/posts/text_detector_benchmark.png" medium="image" type="image/png" height="131" width="144"/>
</item>
<item>
  <title>Optimizing MobileDet for Mobile Deployments</title>
  <link>https://sayak.dev/posts/mobiledet-optimization.html</link>
  <description><![CDATA[ 






<p>This year researchers from the University of Wisconsin-Madison and Google published their work on <a href="https://arxiv.org/abs/2004.14525">MobileDet</a>. MobileDet presents an architectural philosophy for designing object detectors specifically targeted toward running on mobile accelerators like DSP, EdgeTPU, and so on. MobileDet yields significant improvement over architectures MobileNetV2+SSDLite and MobileNetV3+SSDLite on the <a href="https://cocodataset.org/">COCO object detection task</a> with the same accelerated inference time. Long story cut short, if you are planning to use object detection models in mobile applications MobileDets may be an extremely good choice.</p>
<p>One fantastic thing about modern-day research is most of the time, the code and essential artifacts (like the trained models) are available publicly. MobileDet is no exception; the authors released their code and pre-trained models in <a href="https://github.com/tensorflow/models/tree/master/research/object_detection">TensorFlow Object Detection (TFOD) API</a>. The model files come in three different variants -</p>
<ul>
<li>Optimized for mobile CPU</li>
<li>Optimized for EdgeTPU</li>
<li>Optimized for DSP</li>
</ul>
<p>Each of these variants includes the pre-trained checkpoints, a TensorFlow Lite (TFLite) compatible model graph, a TFLite model file, a configuration file, and a graph proto. The models were pre-trained on the COCO dataset.</p>
<p>In this post, I am going to be revisiting the TFLite conversion from the pre-trained model checkpoints along with some of the non-trivial things that come up during the process. It is basically an extension of <a href="https://twitter.com/khanhlvg?lang=en">Khanh LeViet</a> and my findings we shared over <a href="https://github.com/ml-gde/e2e-tflite-tutorials/issues/21">this GitHub thread</a>.</p>
<p>The code discussed throughout this post is available <a href="https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/MobileDet_Conversion_TFLite.ipynb">here as a Colab Notebook</a>.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you want to train MobileDet models on your own dataset you may find <a href="https://github.com/sayakpaul/E2E-Object-Detection-in-TFLite/tree/master/colab_training">these notebooks</a> useful. They show you how to prepare the dataset, fine-tune a MobileDet model with the dataset, and optimize the fine-tuned model with TFLite.</p>
</div>
</div>
<section id="why-yet-another-post-on-model-conversion" class="level2">
<h2 class="anchored" data-anchor-id="why-yet-another-post-on-model-conversion">Why yet another post on model conversion?</h2>
<p>Fair question. After all, there are so many great examples and tutorials that show how to use the <a href="https://www.tensorflow.org/lite/performance/post_training_quantization"><u>post-training quantization APIs</u></a> in TFLite to perform the model conversion. MobileDet models in the TFOD API repository were trained in TensorFlow (TF) 1. If you ever wanted to use the latest TFLite converter to do the conversion, that may not be immediately approachable.</p>
<p>Besides, there are certain caveats to the EdgeTPU and DSP variants. They come in two precision formats - <code>uint8</code> and <code>float32</code>. The models in <code>uint8</code> precision were trained using <a href="https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html"><u>quantization aware training</u></a> (QAT) while the <code>float32</code> models were not. During QAT fake quantization nodes get inserted into a model’s computation graph. So, the models trained using QAT usually require some extra care during the TFLite conversion process as we’ll see in a moment.</p>
<p>If we wanted to convert a single shot detector (SSD) based model to TFLite then we first need to generate a frozen graph first that is compatible with the TFLite operator set (as per these guides - <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md"><u>TF1</u></a> and <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md"><u>TF2</u></a>). The TFOD API team provides stock scripts (<a href="https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_ssd_graph.py"><u>TF1</u></a>, <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_graph_tf2.py"><u>TF2</u></a>) for this. Both of these scripts add optimized postprocessing operations to the model graph. Now, these operations are not yet supported in int8 precision. So, if you ever wanted to convert these pre-trained checkpoints using <a href="https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization"><u>full integer quantization</u></a>, what would have been your approach?</p>
<p>By now, hopefully, I have been able to convince you that this post is not just about regular model conversion in TFLite. The situations we’ll be going through over the next sections may be helpful for your production TFLite models as well.</p>
</section>
<section id="the-hassle-free-conversions" class="level2">
<h2 class="anchored" data-anchor-id="the-hassle-free-conversions">The hassle-free conversions</h2>
<p>Before we build our way toward the fun stuff, let’s start with the conversions that won’t cost us a night’s sleep. Conversions based on <a href="https://www.tensorflow.org/lite/performance/post_training_quant"><u>dynamic-range</u></a> and <a href="https://www.tensorflow.org/lite/performance/post_training_float16_quant">float16</a> quantization would come under this category.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>The EdgeTPU and DSP variants of MobileDet are meant to run on the respective hardware accelerators. These accelerators need a model to be in full integer precision. So converting the EdgeTPU and DSP variants with dynamic-range and <code>float16</code> quantization does not have any practical usage.</p>
</div>
</div>
<p>So, for dynamic-range and <code>float16</code> quantization based conversions, we will be using the CPU variant only. This variant is available <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md#mobile-models">here</a> as <code>ssd_mobiledet_cpu_coco</code>. Once the model bundle is untar’d we get the following files -</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">├──</span> model.ckpt-400000.data-00000-of-00001</span>
<span id="cb1-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">├──</span> model.ckpt-400000.index</span>
<span id="cb1-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">├──</span> model.ckpt-400000.meta</span>
<span id="cb1-4"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">├──</span> model.tflite</span>
<span id="cb1-5"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">├──</span> pipeline.config</span>
<span id="cb1-6"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">├──</span> tflite_graph.pb</span>
<span id="cb1-7"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">└──</span> tflite_graph.pbtxt</span></code></pre></div>
<p><code>model.ckpt-*</code> files are the pre-trained checkpoints on the COCO dataset. If you train a MobileDet object detection model on your own dataset, you will have your own model checkpoint files. The <code>tflite_graph.pb</code> file is a frozen inference graph that is compatible with the TFLite operator set, which was exported from the pre-trained model checkpoints. <code>model.tflite</code> file is a TFLite model that was converted from the <code>tflite_graph.pb</code> frozen graph.</p>
<p>In case if you ever train a MobileDet model on your dataset, here’s how you’d get the TFLite frozen graph file (based on <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md"><u>this guide</u></a> mentioned above) -</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> PIPELINE_CONFIG=<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"checkpoint_name/pipeline.config"</span></span>
<span id="cb2-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> CKPT_PREFIX=<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"checkpoint_name/model.ckpt-400000"</span></span>
<span id="cb2-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> OUTPUT_DIR=<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"tflite_graph"</span></span>
<span id="cb2-4"> </span>
<span id="cb2-5"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> python models/research/object_detection/export_tflite_ssd_graph.py <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb2-6">   <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--pipeline_config_path</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$PIPELINE_CONFIG</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb2-7">   <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--trained_checkpoint_prefix</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$CKPT_PREFIX</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb2-8">   <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--output_directory</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$OUTPUT_DIR</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb2-9">   <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--add_postprocessing_op</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>true</span></code></pre></div>
<p>You can see a fully worked out example in the <a href="https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/MobileDet_Conversion_TFLite.ipynb"><u>Colab Notebook</u></a> mentioned above. If everything goes well, then you should have the frozen graph file exported in <code>OUTPUT_DIR</code>. Let’s now proceed to the TFLite model conversion part.</p>
<p>Here’s how the dynamic-range quantization would look like in TensorFlow 2 -</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">converter <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(</span>
<span id="cb3-2">    graph_def_file<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model_to_be_quantized,</span>
<span id="cb3-3">    input_arrays<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'normalized_input_image_tensor'</span>],              </span>
<span id="cb3-4">    output_arrays<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess'</span>,</span>
<span id="cb3-5">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess:1'</span>,</span>
<span id="cb3-6">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess:2'</span>,</span>
<span id="cb3-7">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess:3'</span>],</span>
<span id="cb3-8">   input_shapes<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'normalized_input_image_tensor'</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">320</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">320</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>]}</span>
<span id="cb3-9">)</span>
<span id="cb3-10">converter.optimizations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [tf.lite.Optimize.DEFAULT]</span>
<span id="cb3-11">tflite_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> converter.convert()</span></code></pre></div>
<p>A note about some of the parameters and their values from the above code listing -</p>
<ul>
<li><p><code>model_to_be_quantized</code> corresponds to the frozen graph file.</p></li>
<li><p><code>input_arrays</code> and <code>input_shapes</code> are set accordingly with respect to the frozen graph file. As we can see in the figure below that these values have been set correctly.</p>
<p><img src="https://i.ibb.co/F4xGRJB/image2.png" class="img-fluid"></p></li>
<li><p><code>output_arrays</code> is set according to the instructions provided in <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_ssd_graph.py"><u>this guide</u></a>. Those operations represent four arrays: <code>detection_boxes</code>, <code>detection_classes</code>, <code>detection_scores</code>, and <code>num_detections</code>, usually a mandate for any object detector out there.</p></li>
</ul>
<p>The rest of the parts in the code listing should be familiar to you if you already know about the typical post-training quantization process in TFLite. For <code>float16</code> quantization, all the things would remain the same; we just need to add this line before calling <code>convert()</code> - <code>converter.target_spec.supported_types = [tf.float16]</code>.</p>
<p>The dynamic-range quantized model is <strong>4.3 MB</strong> in size and <code>float16</code> one is <strong>8.2 MB</strong>. Later, we will see how fast this model would run on actual mobile devices with and without different accelerators.</p>
</section>
<section id="the-trickier-tflite-conversions-for-mobiledet" class="level2">
<h2 class="anchored" data-anchor-id="the-trickier-tflite-conversions-for-mobiledet">The trickier TFLite conversions for MobileDet</h2>
<p>In this section, we will be dealing with the full integer quantization for the three different variants of MobileDet. Full integer quantization is usually more involved than the other quantization formats supported by TFLite.</p>
<section id="representative-dataset" class="level3">
<h3 class="anchored" data-anchor-id="representative-dataset">Representative dataset</h3>
<p>Our first step toward doing full integer quantization is preparing a representative dataset. It is required to calibrate the activation ranges so that the quantized model is able to retain the original model performance as much as possible. For the purpose of this post, I sampled 100 images from the <a href="https://cocodataset.org/#download"><u>COCO training dataset</u></a> (<code>train2014</code> split). In my experience, 100 samples as the representative dataset have always been sufficient. I have hosted these images <a href="https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite/releases/tag/v0.9.0"><u>here</u></a> in case you are interested to use them.</p>
<p>The following code listing denotes a generator function that produces a preprocessed image to the TFLite converter -</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">rep_ds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.data.Dataset.list_files(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train_samples/*.jpg"</span>)</span>
<span id="cb4-2">HEIGHT, WIDTH <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">320</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">320</span></span>
<span id="cb4-3"> </span>
<span id="cb4-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> representative_dataset_gen():</span>
<span id="cb4-5">   <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> image_path <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> rep_ds:</span>
<span id="cb4-6">       img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.io.read_file(image_path)</span>
<span id="cb4-7">       img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.io.decode_image(img, channels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb4-8">       img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.convert_image_dtype(img, tf.float32)</span>
<span id="cb4-9">       resized_img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.resize(img, (HEIGHT, WIDTH))</span>
<span id="cb4-10">       resized_img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> resized_img[tf.newaxis, :]</span>
<span id="cb4-11">       <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">yield</span> [resized_img]</span></code></pre></div>
<p><strong>Note</strong> that these preprocessing steps should be in sync with the actual preprocessing steps that would apply before running inference with your TFLite model. In case if you are interested to know about more complex representative dataset generators you may find <a href="https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/Magenta_arbitrary_style_transfer_model_conversion.ipynb"><u>this notebook</u></a> useful.</p>
<p>Also, note that dynamic-range and <code>float16</code> quantization of the EdgeTPU and DSP variants don’t have much of practical usage. The next section is going to be solely about full integer quantization of these different variants and the nitty-gritty to take into consideration for the conversion process.</p>
</section>
<section id="dealing-with-fake-quantization-nodes-during-conversion" class="level3">
<h3 class="anchored" data-anchor-id="dealing-with-fake-quantization-nodes-during-conversion">Dealing with fake quantization nodes during conversion</h3>
<p>The figure below represents a portion of the <code>uint8</code> EdgeTPU model computation graph. The nodes highlighted in red are inserted by the QAT mechanism. You would notice the same kind of nodes in the <code>uint8</code> DSP model computation graph as well.</p>
<p><img src="https://i.ibb.co/B2qXzsf/image1.png" class="img-fluid"></p>
<p>Now, these nodes have some important implications that we need to consider during the conversion process -</p>
<ul>
<li>During QAT the activation ranges are already approximated i.e.&nbsp;QAT resembles post-training quantization during training and adjusts the activation ranges accordingly. So, we don’t need to provide a representative dataset for a full integer quantization based conversion.</li>
<li>These fake nodes are generally in integer precision. So, setting an optimization option (<code>converter.optimizations</code>) might lead to inconsistencies.</li>
<li>In order to convert the <code>uint8</code> models with full integer quantization, we need to set the input and output data type of the TFLite models to integer precision (typically <code>uint8</code> or <code>int8</code>). As per <a href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/lite/TFLiteConverter#attributes"><u>this documentation</u></a>, we also need to specify the <code>quantized_input_stats</code> parameter during the conversion process. This is needed in order for the converted TFLite model to map the quantized input values to real values. More details are available <a href="https://www.tensorflow.org/lite/performance/quantization_spec"><u>here</u></a>.</li>
</ul>
<p>So, how do we realize all of these in code?</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">converter <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(</span>
<span id="cb5-2">   graph_def_file<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model_to_be_quantized,</span>
<span id="cb5-3">   input_arrays<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'normalized_input_image_tensor'</span>],</span>
<span id="cb5-4">   output_arrays<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess'</span>,</span>
<span id="cb5-5">       <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess:1'</span>,</span>
<span id="cb5-6">       <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess:2'</span>,</span>
<span id="cb5-7">       <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess:3'</span>],</span>
<span id="cb5-8">   input_shapes<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'normalized_input_image_tensor'</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">320</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">320</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>]}</span>
<span id="cb5-9">)</span>
<span id="cb5-10">converter.inference_input_type <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.uint8</span>
<span id="cb5-11">converter.quantized_input_stats <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"normalized_input_image_tensor"</span>: (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>)}</span>
<span id="cb5-12">tflite_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> converter.convert()</span></code></pre></div>
<p>If you’re thinking this does not look all that gory compared to the above code listing - it does not have to be! The tooling should help you do these things seamlessly. But catching these details during your project development may not be trivial. Note that we don’t specify <code>converter.inference_output_type</code>. Hold your breath, we will come to this in a moment.</p>
<p>After successful execution, we get two full integer quantized models - EdgeTPU one is <strong>4.2 MB</strong> and the DSP one is <strong>7.0 MB</strong>.</p>
</section>
<section id="integer-quantization-for-cpu-variants-and-float32-precision-models" class="level3">
<h3 class="anchored" data-anchor-id="integer-quantization-for-cpu-variants-and-float32-precision-models">Integer quantization for CPU variants and float32 precision models</h3>
<p>The variants that don’t contain fake quantization nodes (CPU and all the models in <code>float32</code> precision) have a <em>relatively</em> simpler conversion process. Recollect that the EdgeTPU and DSP variants come in two different precisions - <code>uint8</code> and <code>float32</code>. For example, here’s how it would be for the <code>float32</code> precision models -</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">converter.representative_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> representative_dataset_gen</span>
<span id="cb6-2">converter.inference_input_type <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.uint8</span>
<span id="cb6-3">converter.optimizations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [tf.lite.Optimize.DEFAULT]</span></code></pre></div>
<p>Note that we are specifying a representative dataset here because the <code>float32</code> precision models weren’t trained using QAT. For the CPU variant model, the lines of code would slightly change -</p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">converter.inference_input_type <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.uint8</span>
<span id="cb7-2">converter.quantized_input_stats <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"normalized_input_image_tensor"</span>: (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>)}</span>
<span id="cb7-3">converter.optimizations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [tf.lite.Optimize.DEFAULT]</span></code></pre></div>
<p>Honestly, I found this configuration by trial and error. I observed that if I specify a representative dataset then it hurts the predictions of the converted model. Also, I found out that specifying <code>converter.quantized_input_stats</code> helped improve the predictions of the converted model.</p>
<p>We don’t specify <code>converter.inference_output_type</code> in this case as well. Let’s get to it now.</p>
</section>
<section id="dealing-with-non-integer-postprocessing-ops-during-conversion" class="level3">
<h3 class="anchored" data-anchor-id="dealing-with-non-integer-postprocessing-ops-during-conversion">Dealing with non-integer postprocessing ops during conversion</h3>
<p>Remember that frozen graph exporter scripts provided by the TFOD API team add optimized postprocessing operations to the graph. These operations are not supported in integer precision yet. So, even if you wanted to specify <code>converter.inference_output_type</code> as <code>tf.uint8</code> you’ll likely get the following error -</p>
<pre><code>RuntimeError: Unsupported output type UINT8 for output tensor 'TFLite_Detection_PostProcess' of type FLOAT32.</code></pre>
<p>This is why we did not set the <code>converter.inference_output_type</code> parameter.</p>
<p>This should resolve all the problems you may run into if you ever wanted to convert the MobileDet models offered by the TFOD API team. In the last two sections, we’ll see these converted models in action and how fast they can perform on respective hardware accelerators.</p>
</section>
</section>
<section id="show-me-some-results" class="level2">
<h2 class="anchored" data-anchor-id="show-me-some-results">Show me some results</h2>
<p>For the CPU variant model, its <code>float16</code> quantized TFLite provided decent results -</p>
<p><img src="https://i.ibb.co/k6c93CC/image3.png" class="img-fluid"></p>
<p>On Colab, the inference time is about <strong>92.36 ms</strong> for this particular model. I experimented with different threshold values for filtering out the weak predictions and a threshold of <strong>0.3</strong> yielded the best results. These results are pretty consistent across the several different models we talked about.</p>
<p>A major point to note here for the EdgeTPU and DSP variants, their converted counterparts would be much slower on Colab since they were specifically optimized for different hardware accelerators.</p>
<p>You are encouraged to play with the different converted models using the Colab Notebook mentioned above and see these results for yourself.</p>
</section>
<section id="model-benchmarks" class="level2">
<h2 class="anchored" data-anchor-id="model-benchmarks">Model benchmarks</h2>
<p>In this section, we’ll address the question - “So, how do I choose one among these many models?” Well, you could manually try them all out and see which performs the best on the runtime of your choice. But a more practical approach to this would be to first benchmark these models on a set of devices using the <a href="https://www.tensorflow.org/lite/performance/measurement"><u>TFLite Benchmark Tool</u></a> and then decide accordingly.</p>
<p>The following table provides a comprehensive summary of the important statistics about the runtime of different TFLite MobileDet models. These results were generated using the TFLite Benchmark Tool mentioned above.</p>
<img src="https://i.ibb.co/jrKshwB/image.png" class="img-fluid">
<center>
<small>* Device used - Pixel 4 (Inference timings are reported in milliseconds)</small><br> <small>** As reported <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md">here</a></small>
</center>
<p>We can see that with the proper hardware accelerators, the DSP EdgeTPU variants can really shine. For the CPU variant, on a GPU accelerated runtime the <code>float16</code> quantized TFLite model can bring in additional speed boosts.</p>
<p>A catch here is Pixel devices don’t allow third-party applications to use the Hexagon DSP therefore even if we instruct the Benchmark Tool to make use of that the model would fall back to the CPU for execution. This is why for fair benchmarking results for the DSP variants we should consider running the Benchmark Tool on a device (such as Samsung Galaxy S9+) that has Hexagon DSP and also allows third-party applications to use it.</p>
<img src="https://i.ibb.co/mHkyfpd/image.png" class="img-fluid">
<center>
<small>* Device used - Samsung Galaxy S9+ (Inference timings are reported in milliseconds)</small>
</center>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>To train a custom MobileDet-based object detector you can refer to <a href="https://github.com/sayakpaul/E2E-Object-Detection-in-TFLite/tree/master/colab_training">these notebooks</a>.</p>
</div>
</div>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>In this post, we discussed some of the intricate problems one may run into while converting different variants of the MobileDet model in TFLite. One aspect about TFLite that I really like is how it provides the tooling needed to deal with practical problems like this.</p>
<p>I am thankful to Khanh for thoroughly guiding me while writing this post. Thanks to <a href="https://sg.linkedin.com/in/martinandrews">Martin Andrews</a> for suggesting textual edits.</p>


</section>

 ]]></description>
  <category>tflite</category>
  <category>model-optimization</category>
  <category>mobiledet</category>
  <guid>https://sayak.dev/posts/mobiledet-optimization.html</guid>
  <pubDate>Tue, 29 Sep 2020 00:00:00 GMT</pubDate>
  <media:content url="https://sayak.dev/posts/benchmark.png" medium="image" type="image/png" height="62" width="144"/>
</item>
<item>
  <title>Different data augmentation recipes in tf.keras for image classification</title>
  <link>https://sayak.dev/posts/augmentation-recipes.html</link>
  <description><![CDATA[ 






<p>Data augmentation is a favorite recipe among deep learning practitioners especially for the ones working in the field of computer vision. Data augmentation is a technique used for introducing variety in training data thereby helping to mitigate overfitting.</p>
<p>When using Keras for training image classification models, using the <code>ImageDataGenerator</code> class for handling data augmentation is pretty much a standard choice. However, with TensorFlow, we get a number of different ways we can apply data augmentation to image datasets. In this tutorial, we are going to discuss three such ways. Knowing about these different ways of plugging in data augmentation in your image classification training pipelines will help you decide the best way for a given scenario.</p>
<p>Here’s a brief overview of the different ways we are going to cover:</p>
<ul>
<li>Using the standard ImageDataGenerator class</li>
<li>Using TensorFlow image ops with a TensorFlow dataset</li>
<li>Using Keras’s (experimental) image processing layers</li>
<li>Mix-matching different image ops &amp; image processing layers</li>
</ul>
<p>Let’s get started!</p>
<section id="experimental-setup" class="level1">
<h1>Experimental setup</h1>
<p>We are going to use the flowers dataset to demonstrate the experiments. Downloading the dataset is just as easy as executing the following line of code:</p>
<p><code>flowers</code> contains the path (which in my case is - <code>/root/.keras/datasets/flower_photos</code>) where the dataset got downloaded. The structure of the dataset looks like so -</p>
<pre><code>├── daisy [633 entries]
├── dandelion [898]
├── roses [641]
├── sunflowers [699 entries]
├── tulips [799 entries]
└── LICENSE.txt</code></pre>
<div id="cell-6" class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Get the flowers dataset</span></span>
<span id="cb2-2">flowers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.utils.get_file(</span>
<span id="cb2-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'flower_photos'</span>,</span>
<span id="cb2-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz'</span>,</span>
<span id="cb2-5">    untar<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</div>
<p>Using the standard ImageDataGenerator class For most of the scenarios, the ImageDataGenerator should be good enough. Its flexible API design is really to follow and it makes it easier to work with custom image datasets by providing meaningful high-level abstractions.</p>
<p>We instantiate the ImageDataGenerator class like so -</p>
<div id="cell-10" class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">img_gen <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.preprocessing.image.ImageDataGenerator(</span>
<span id="cb3-2">    rescale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">255</span>,</span>
<span id="cb3-3">    rotation_range<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>,</span>
<span id="cb3-4">    horizontal_flip<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</div>
<p>We specify two augmentation operations and a pixel rescaling operation in there. <code>ImageDataGenerator</code> comes with a handy <code>flow_from_directory</code> method that allows us to read images from a directory and apply the specified operations on the fly during the time of training. Here’s how to instruct the <code>img_gen</code> object to read images from a directory -</p>
<div id="cell-12" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="821f7d34-ab72-4cfc-9a52-3833df630437">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">IMG_SHAPE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">224</span></span>
<span id="cb4-2">BATCH_SIZE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span></span>
<span id="cb4-3"></span>
<span id="cb4-4">img_flow <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> img_gen.flow_from_directory(flowers, </span>
<span id="cb4-5">    shuffle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, </span>
<span id="cb4-6">    batch_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>BATCH_SIZE,</span>
<span id="cb4-7">    target_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(IMG_SHAPE, IMG_SHAPE))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Found 3670 images belonging to 5 classes.</code></pre>
</div>
</div>
<p>We then verify the images and the labels and they are indeed parsed right -</p>
<div id="cell-14" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:608}}" data-outputid="4a3e083a-6a12-49e4-8e29-88e79b0f86d1">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">images, labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>(img_flow)</span>
<span id="cb6-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(images.shape, labels.shape)</span>
<span id="cb6-3">show_batch(images, labels)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(32, 224, 224, 3) (32, 5)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://sayak.dev/posts/2020-05-10-augmemtation-recipes_files/figure-html/cell-9-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Training with an ImageDataGenerator instance is extremely straight-forward -</p>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_training_model()</span>
<span id="cb8-2">model.fit(img_flow, ...)</span></code></pre></div>
<p>For a fully worked out example, refer to <a href="https://www.pyimagesearch.com/2019/06/03/fine-tuning-with-keras-and-deep-learning/">this tutorial</a>.</p>
<p>As can be seen in <a href="https://www.tensorflow.org/tutorials/load_data/images#performance">this blog post</a>, <code>ImageDataGenerator</code>’s overall data loading performance can have a significant effect on how fast your model trains. To tackle situations, where you need to maximize the hardware utilization without burning unnecessary bucks, <a href="https://www.tensorflow.org/guide/data">TensorFlow’s data module</a> can be really helpful (comes at some costs).</p>
</section>
<section id="tensorflow-image-ops-with-tf.data-apis" class="level1">
<h1>TensorFlow image ops with tf.data APIs</h1>
<p>The blog post I mentioned in the previous section shows the kind of performance boost achievable with <code>tf.data</code> APIs. But it’s important to note that boost comes at the cost of writing boilerplate code which makes the overall process more involved. For example, here’s how you would load and preprocess your images and labels -</p>
<div id="cell-17" class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> parse_images(image_path):</span>
<span id="cb9-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Load and preprocess the image</span></span>
<span id="cb9-3">    img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.io.read_file(image_path) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># read the raw image</span></span>
<span id="cb9-4">    img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.decode_jpeg(img, channels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># decode the image back to proper format</span></span>
<span id="cb9-5">    img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.convert_image_dtype(img, tf.float32) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># scale the pixel values to [0, 1] </span></span>
<span id="cb9-6">    img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.resize(img, [IMG_SHAPE, IMG_SHAPE]) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># resize the image</span></span>
<span id="cb9-7"></span>
<span id="cb9-8">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Parse the labels</span></span>
<span id="cb9-9">    label <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.strings.split(image_path, os.path.sep)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>]</span>
<span id="cb9-10"></span>
<span id="cb9-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (img, label)</span></code></pre></div>
</div>
<p>You would then write a separate augmentation policy with the <a href="https://www.tensorflow.org/api_docs/python/tf/image">TensorFlow Image ops</a> -</p>
<div id="cell-19" class="cell">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> augment(image, label):</span>
<span id="cb10-2">    img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.rot90(image)</span>
<span id="cb10-3">    img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.flip_left_right(img)</span>
<span id="cb10-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (img, label)</span></code></pre></div>
</div>
<p>To chain the above two together you would first create an initial dataset consisting of only the image paths -</p>
<div id="cell-21" class="cell">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">image_paths <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(paths.list_images(flowers))</span>
<span id="cb11-2">list_ds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.data.Dataset.from_tensor_slices((image_paths))</span></code></pre></div>
</div>
<p>Now, you would read, preprocess, shuffle, augment, and batch your dataset -</p>
<div id="cell-23" class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">AUTO <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.data.experimental.AUTOTUNE</span>
<span id="cb12-2"></span>
<span id="cb12-3">train_ds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb12-4">    list_ds</span>
<span id="cb12-5">    .<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(parse_images, num_parallel_calls<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>AUTO)</span>
<span id="cb12-6">    .shuffle(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1024</span>)</span>
<span id="cb12-7">    .<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(augment, num_parallel_calls<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>AUTO) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># augmentation call</span></span>
<span id="cb12-8">    .batch(BATCH_SIZE)</span>
<span id="cb12-9">    .prefetch(AUTO)</span>
<span id="cb12-10">)</span></code></pre></div>
</div>
<p><code>num_parallel_calls</code> allows you to parallelize the mapping function and <code>tf.data.experimental.AUTOTUNE</code> lets TensorFlow decide the level of parallelism to use dynamically (how cool is that?). prefetch allows loading in the next batch of data well before your model finishes the current epoch of training. It is evident that this process is more involved than the previous one.</p>
<p>Verifying if we constructed the data input pipeline correctly is a vital step before you feed your data to the model -</p>
<div id="cell-25" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:608}}" data-outputid="de640bb6-85e6-4905-cce9-4ffa2cf1fa29">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">image_batch, label_batch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">iter</span>(train_ds))</span>
<span id="cb13-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(image_batch.shape, label_batch.shape)</span>
<span id="cb13-3">show_batch(image_batch.numpy(), label_batch.numpy(), image_data_gen<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(32, 224, 224, 3) (32,)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://sayak.dev/posts/2020-05-10-augmemtation-recipes_files/figure-html/cell-14-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The “b”s appear before the class labels because TensorFlow parses the strings as byte-strings. Using train_ds with your model is also just about executing -</p>
<div class="sourceCode" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_training_model()</span>
<span id="cb15-2">model.fit(train_ds, ...)</span></code></pre></div>
<p><a href="https://www.tensorflow.org/tutorials/images/data_augmentation">Here</a> you can find a fully worked out example. <a href="https://www.tensorflow.org/guide/data_performance">Here</a> you can know more about the different performance considerations when using tf.data. There are more image ops available with <strong>TensorFlow Addons</strong> which can found <a href="https://www.tensorflow.org/addons/tutorials/image_ops">here</a>.</p>
<p>Recently, Keras introduced <a href="https://keras.io/api/preprocessing/image/#image_dataset_from_directory"><code>image_dataset_from_directory</code></a> function (only available in <code>tf-nightly</code> at the time of writing this) which takes care of many of the boilerplate code we saw above and still yields pretty good performance. Here’s <a href="https://colab.research.google.com/drive/1umJnCp8tZ7UDTYSQsuWdKRhqbHts38AC">a tutorial</a> that shows how to use it.</p>
<p>Keras has also introduced <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing">a number of image processing layers</a> which can be very useful to build flexible augmentation pipelines using the Sequential API. In the next section, let’s see how.</p>
</section>
<section id="using-kerass-experimental-image-processing-layers" class="level1">
<h1>Using Keras’s (experimental) image processing layers</h1>
<p>Just like you would construct an entire model using the <code>Sequential</code> API, you can now construct very flexible data augmentation pipelines using the newly introduced (although experimental at the time of writing this) image processing layers. If we were to convert the data augmentation operations we have been following in the tutorial so far, building a data augmentation pipeline using this approach would be like so -</p>
<div id="cell-28" class="cell">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">data_augmentation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.Sequential([</span>
<span id="cb16-2">    tf.keras.layers.experimental.preprocessing.RandomFlip(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'horizontal'</span>),</span>
<span id="cb16-3">    tf.keras.layers.experimental.preprocessing.RandomRotation(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>)</span>
<span id="cb16-4">])</span></code></pre></div>
</div>
<p>Before passing your data through this stack of layers <strong>makes sure you haven’t applied any augmentation already</strong>. So, it’s safe to create a separate TensorFlow dataset without mapping the augmentation function like we previously did -</p>
<div id="cell-30" class="cell">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create TensorFlow dataset without any augmentation</span></span>
<span id="cb17-2">train_ds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb17-3">    list_ds</span>
<span id="cb17-4">    .<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(parse_images, num_parallel_calls<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>AUTO)</span>
<span id="cb17-5">    .shuffle(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1024</span>)</span>
<span id="cb17-6">    .batch(BATCH_SIZE)</span>
<span id="cb17-7">    .prefetch(AUTO)</span>
<span id="cb17-8">)</span></code></pre></div>
</div>
<p>Now, we can see how to examine some of the augmented images that would come out of this mini pipeline -</p>
<div id="cell-32" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:591}}" data-outputid="f8f269a9-8bd3-492d-a445-212fbc6897c0">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">image_batch, label_batch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">iter</span>(train_ds))</span>
<span id="cb18-2"></span>
<span id="cb18-3">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>))</span>
<span id="cb18-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> n <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>):</span>
<span id="cb18-5">    ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplot(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb18-6">    augmented_image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data_augmentation(tf.expand_dims(image_batch[n], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>))</span>
<span id="cb18-7">    plt.imshow(augmented_image[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].numpy())</span>
<span id="cb18-8">    plt.title(label_batch[n].numpy().decode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"utf-8"</span>))</span>
<span id="cb18-9">    plt.axis(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'off'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://sayak.dev/posts/2020-05-10-augmemtation-recipes_files/figure-html/cell-17-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can also make use of Python <code>lambda</code>s to map <code>data_augmentation</code> directly to our <code>tf.data</code> pipeline like so:</p>
<div class="sourceCode" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">train_ds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb19-2">    list_ds</span>
<span id="cb19-3">    .<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(parse_images, num_parallel_calls<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>AUTO)</span>
<span id="cb19-4">    .shuffle(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1024</span>)</span>
<span id="cb19-5">    .batch(BATCH_SIZE)</span>
<span id="cb19-6">    .<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x, y: (data_augmentation(x), y),</span>
<span id="cb19-7">        num_parallel_calls<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>AUTO)</span>
<span id="cb19-8">    .prefetch(AUTO)</span>
<span id="cb19-9">)</span></code></pre></div>
<p>Note that these layers can be also added as a part of your model allowing them to run on GPUs. Based on your compute budget you should decide if you would want to run these layers on the GPU or you would rather have them executed separately on the CPU.</p>
<p>A functional model definition in Keras using this approach may look like so -</p>
<div class="sourceCode" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># You define an input layer with pre-defined shapes</span></span>
<span id="cb20-2">inputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> keras.Input(shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(IMG_SHAPE, IMG_SHAPE, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span>
<span id="cb20-3"></span>
<span id="cb20-4">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data_augmentation(inputs)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Apply random data augmentation</span></span>
<span id="cb20-5">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> feature_extractor_model(x, training<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb20-6">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> GlobalAveragePooling2D()(x)</span>
<span id="cb20-7">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Dropout(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>)(x)  </span>
<span id="cb20-8">outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Dense(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)(x)</span>
<span id="cb20-9"></span>
<span id="cb20-10">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Model(inputs, outputs)</span></code></pre></div>
<p>Now, <code>model</code> should be good to go with - <code>model.fit(train_ds, ...)</code>. A fully worked out example is available <a href="https://colab.research.google.com/drive/17vHSAj7no7RMdJ18MJomTf8twqw1suYC#scrollTo=nhSR8l3OX_sM">here</a>. Note that, performance might get slightly affected when going with this approach since the GPUs will be utilized to run the preprocessing layers as well.</p>
<p>Let’s now think about situations where we may need to use a combination of the image ops of TensorFlow and the layers we just saw. What if we need to plug in custom augmentation operations in the augmentation pipeline? Added on top of it, what if we need to fix the probability at which the augmentation operations would get applied? Data augmentation pipelines are quite central behind the success of recent works like <a href="https://arxiv.org/abs/2002.05709">SimCLR</a>, <a href="https://arxiv.org/abs/1912.02781">Augmix</a>, etc.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>These layers have pre-defined inference-time behaviour. So even if you have included them inside your model itself, it’s totally fine. But if you want them during inference, you would need to set its inference-time behaviour.</p>
</div>
</div>
</section>
<section id="towards-more-complex-augmentation-pipelines" class="level1">
<h1>Towards more complex augmentation pipelines</h1>
<p>In this final approach, we will see how to mix and match between the different stock image ops, and stock image processing layers. Let’s first define a class utilizing the stock image ops with a utility function to apply them at random with a pre-defined probability.</p>
<div id="cell-37" class="cell">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> CustomAugment(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">object</span>):</span>
<span id="cb21-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__call__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, image):        </span>
<span id="cb21-3">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Random flips and grayscale with some stochasticity</span></span>
<span id="cb21-4">        img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._random_apply(tf.image.flip_left_right, image, p<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>)</span>
<span id="cb21-5">        img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._random_apply(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._color_drop, img, p<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>)</span>
<span id="cb21-6">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> img</span>
<span id="cb21-7"></span>
<span id="cb21-8">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _color_drop(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x):</span>
<span id="cb21-9">        image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.rgb_to_grayscale(x)</span>
<span id="cb21-10">        image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.tile(x, [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>])</span>
<span id="cb21-11">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> x</span>
<span id="cb21-12">    </span>
<span id="cb21-13">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _random_apply(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, func, x, p):</span>
<span id="cb21-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> tf.cond(</span>
<span id="cb21-15">          tf.less(tf.random.uniform([], minval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, maxval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tf.float32),</span>
<span id="cb21-16">                  tf.cast(p, tf.float32)),</span>
<span id="cb21-17">          <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span>: func(x),</span>
<span id="cb21-18">          <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span>: x)</span></code></pre></div>
</div>
<p><code>_random_apply</code> is taken from the <a href="https://github.com/google-research/simclr">official SimCLR repository</a>. Now, in order to tie it together with the stock image processing layers, we can still use the <code>Sequential</code> API with a <code>Lambda</code> layer -</p>
<div id="cell-39" class="cell">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Build the augmentation pipeline</span></span>
<span id="cb22-2">data_augmentation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.Sequential([</span>
<span id="cb22-3">    tf.keras.layers.Lambda(CustomAugment()),</span>
<span id="cb22-4">    tf.keras.layers.experimental.preprocessing.RandomRotation(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>)</span>
<span id="cb22-5">])</span></code></pre></div>
</div>
<p>When we verify if it’s indeed correct, we get desired outputs -</p>
<div id="cell-41" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:591}}" data-outputid="0fafb233-22ff-4446-fbf1-919d77f34613">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">image_batch, label_batch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">iter</span>(train_ds))</span>
<span id="cb23-2"></span>
<span id="cb23-3">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>))</span>
<span id="cb23-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> n <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>):</span>
<span id="cb23-5">    ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplot(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb23-6">    augmented_image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data_augmentation(tf.expand_dims(image_batch[n], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>))</span>
<span id="cb23-7">    plt.imshow(augmented_image[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].numpy())</span>
<span id="cb23-8">    plt.title(label_batch[n].numpy().decode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"utf-8"</span>))</span>
<span id="cb23-9">    plt.axis(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'off'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://sayak.dev/posts/2020-05-10-augmemtation-recipes_files/figure-html/cell-21-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Training models when using this approach remains the same as the previous one. Keep in mind that performance can get affected when using this approach.</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ul>
<li><a href="https://www.pyimagesearch.com/2019/06/03/fine-tuning-with-keras-and-deep-learning/">Fine-tuning with Keras and Deep Learning</a></li>
<li><a href="https://keras.io/guides/transfer_learning/">Transfer learning &amp; fine-tuning</a></li>
<li><a href="https://keras.io/examples/vision/image_classification_from_scratch/">Image classification from scratch</a></li>
<li><a href="https://www.tensorflow.org/tutorials/images/data_augmentation">Data augmentation</a></li>
</ul>


</section>

 ]]></description>
  <category>tf.keras</category>
  <category>data_augmentation</category>
  <category>image</category>
  <guid>https://sayak.dev/posts/augmentation-recipes.html</guid>
  <pubDate>Sun, 10 May 2020 00:00:00 GMT</pubDate>
  <media:content url="https://sayak.dev/posts/augmentation.png" medium="image" type="image/png" height="145" width="144"/>
</item>
</channel>
</rss>
