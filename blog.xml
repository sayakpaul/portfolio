<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Sayak Paul</title>
<link>https://sayak.dev/blog.html</link>
<atom:link href="https://sayak.dev/blog.xml" rel="self" type="application/rss+xml"/>
<description>Personal site of Sayak Paul.</description>
<generator>quarto-1.6.40</generator>
<lastBuildDate>Fri, 15 Mar 2024 00:00:00 GMT</lastBuildDate>
<item>
  <title>Streamlining PyPI Releases: A Case Study with 🧨 Diffusers</title>
  <link>https://sayak.dev/posts/streamlined-releases.html</link>
  <description><![CDATA[ 





<p>Releasing a new version of an open-source library is an exhilarating experience. You ship new features, bug fixes, improved documentation, etc., to serve your users and also the mission of your library. Being one of the maintainers of the <a href="https://github.com/huggingface/diffusers/">🧨&nbsp;Diffusers library</a>, I am no exception to this.</p>
<p>Once a release is finalized, it’s usually published on a software repository for the distribution programming language you’re using. Diffusers is a Python library, so <a href="https://pypi.org/">PyPI</a> is our publishing platform.</p>
<p>In this post, I share some of what I learned from trying to streamline the entire process of releasing a new version of the library and then publishing it. If you have similar responsibilities at your workplace or for your personal projects, this post might be helpful for you.</p>
<section id="an-example-release-workflow-manual" class="level2">
<h2 class="anchored" data-anchor-id="an-example-release-workflow-manual">An example release workflow (manual)</h2>
<p>Before we proceed to the other sections of the post, it will be helpful to have a schematic of what constitutes a release. Note that this workflow will vary from library to library, but some principles will still apply. I will take the workflow we follow for Diffusers as an example.</p>
<p>The steps are well laid out in <code>setup.py</code> and can be found <a href="https://github.com/huggingface/diffusers/blob/main/setup.py#L20C1-L78C55">here</a>. Broadly, these are:</p>
<ol type="1">
<li>Prepare the release branch and cut it out from the <code>main</code>.</li>
<li>Run any test on the release branch and wait for them to pass. Fix any failures if needed.</li>
<li>Tag the release branch and push the tag.</li>
<li>Build the package source and wheel.<br>
</li>
<li>Upload the package distribution to the <a href="https://test.pypi.org/">Test PyPI server</a> and run any tests.</li>
<li>Finally, upload to the actual <a href="https://pypi.org/">PyPI server</a>.</li>
</ol>
<p>We identified that steps 1-3 will always require a bit of human intervention and cannot be automated much (props if that’s not the case for you). But steps 3-6 can indeed be automated. These steps require more attention, too:</p>
<ul>
<li>When building the package distribution, one must delete the previous one before starting the build. Otherwise, it can have unintended consequences.</li>
<li>Managing the credentials for the Test PyPI and PyPI servers.</li>
<li>Running any tests after publishing them on the Test PyPI server.</li>
</ul>
<p>These steps would be better automated in your library’s Continuous Integration suite, greatly reducing the mental burden.</p>
</section>
<section id="semi-automating-the-release-workflow" class="level2">
<h2 class="anchored" data-anchor-id="semi-automating-the-release-workflow">Semi-automating the release workflow</h2>
<p>Once we identified the above findings, we prepared a GitHub Actions workflow that gets triggered after a release is tagged and the tag is pushed. Additionally, we configured the workflow to be manually triggerable in case any intervention was needed.</p>
<p>This workflow takes the following steps:</p>
<ol type="1">
<li>Find out the release branch so that it can be checked out for the sequential steps.</li>
<li>Steps 3-6 as outlined in the above section.</li>
</ol>
<p>It’s worth noting that the trigger for this kind of workflow should be figured out to suit what’s best for the given project. In the case of Diffusers, we realized that release steps that come after pushing the release tags can be largely automated. Hence, we went with that trigger.</p>
<p>The workflow file is available <a href="https://github.com/huggingface/diffusers/blob/main/.github/workflows/pypi_publish.yaml">here</a>. When successfully executed, it appears like so:</p>
<p><img src="https://github.com/sayakpaul/portfolio/blob/master/posts/release_graph.png?raw=true" class="img-fluid"></p>
<p>You can find the complete details about the action run <a href="https://github.com/huggingface/diffusers/actions/runs/8283556088">here</a>.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pay attention to the dependencies
</div>
</div>
<div class="callout-body-container callout-body">
<p>The initial workflow was missing a dependency that was needed to run the import tests after Test PyPI publishing. This was fixed in <a href="https://github.com/huggingface/diffusers/pull/7339">this PR</a>. So, please double-check any dependency that might be needed to run the tests after your package has been published on the Test PyPI server.</p>
</div>
</div>
<p>The <a href="https://github.com/huggingface/diffusers/blob/main/.github/workflows/pypi_publish.yaml">workflow</a> doesn’t make use of any pre-built actions (such as <a href="https://github.com/pypa/gh-action-pypi-publish"><code>pypa/gh-action-pypi-publish@v1.6.4</code></a>) for publishing on PyPI. Instead, we decided to just follow what we’d do manually, i.e., use <code>twine</code> to manage the process. If you’re looking to use such an action, <a href="https://github.com/philschmid/easyllm/blob/main/.github/workflows/publish.yaml">this</a> can be a handy reference.</p>
</section>
<section id="publishing-the-release-notes-and-communications" class="level2">
<h2 class="anchored" data-anchor-id="publishing-the-release-notes-and-communications">Publishing the release notes and communications</h2>
<p>The next step in the release process involves publishing the release notes on your repository and tagging it. Once a release is published, team members usually communicate about it internally within an organization and also more broadly with their communities through social media channels.</p>
<p>On the Diffusers team, we take release notes pretty seriously (<a href="https://github.com/huggingface/diffusers/releases/tag/v0.27.0">example notes</a>). This is why we intentionally keep the process of writing the notes and publishing them purely manual. Once a release is published on the repository, a workflow gets automatically triggered to communicate about it to an internal Slack channel. Successful execution of this workflow makes a bot automatically post the message below to a particular Slack channel:</p>
<p><img src="https://github.com/sayakpaul/portfolio/blob/master/posts/bot.png?raw=true" class="img-fluid"></p>
<p>This workflow can be found <a href="https://github.com/huggingface/diffusers/blob/main/.github/workflows/notify_slack_about_release.yml">here</a>.</p>
<p>Both the above steps were introduced in Diffusers through <a href="https://github.com/huggingface/diffusers/pull/7270">this PR</a>. I recommend readers to go through it if they want to incorporate similar changes in their projects.</p>
</section>
<section id="considerations" class="level2">
<h2 class="anchored" data-anchor-id="considerations">Considerations</h2>
<p>I played with the workflows rigorously on a <a href="https://github.com/sayakpaul/blossom">dummy repository</a> before introducing them in Diffusers. This is optional but highly recommended to confidently land similar changes in your actual projects.</p>
<p>We used incoming webhooks on Slack so that the bot could post messages. If you’re configuring something similar, this <a href="https://api.slack.com/messaging/webhooks">official tutorial</a> can be quite useful.</p>


</section>

 ]]></description>
  <category>pypi-releases</category>
  <category>github-actions</category>
  <category>diffusers</category>
  <guid>https://sayak.dev/posts/streamlined-releases.html</guid>
  <pubDate>Fri, 15 Mar 2024 00:00:00 GMT</pubDate>
  <media:content url="https://sayak.dev/posts/pypi-releases-diffusers.png" medium="image" type="image/png" height="72" width="144"/>
</item>
<item>
  <title>Another Quarto Migration Guide for fastpages Users</title>
  <link>https://sayak.dev/posts/quarto-migration.html</link>
  <description><![CDATA[ 





<p>For more than four years, I used <a href="https://fastpages.fast.ai/">fastpages</a> for my personal site, <a href="https://sayak.dev">sayak.dev</a>, because it offered a really easy way to do blogging. When its creator, Hamel, <a href="https://forums.fast.ai/t/fastpages-deprecating-fastpages-in-favor-of-quarto/99095">posted</a> about its deprecation in favor of Quarto, I was anxious. Migrations always make me anxious. But I still continued with fastpages anyway.</p>
<p>But entering into this year, I decided to finally switch to Quarto for apparently no specific reason. I think I just wanted a fresh look for my website but didn’t want something too fancy. Quarto provides a perfect balance between simplicity and visual aesthetics. So, I knew I would use Quarto eventually I migrate from fastpages.</p>
<p>Hamel provides an excellent Quarto migration guide for fastpages users <a href="https://nbdev.fast.ai/tutorials/blogging.html">here</a>. However, I had to take care of a few additional things to make it all work and successfully migrate sayak.dev to Quarto. In this post, I want to provide a detailed account of what I did and some personal preferential bits in the hope that it will be beneficial for someone else. Let’s get started 💫</p>
<section id="bit-0-go-through-the-original-migration-guide" class="level3">
<h3 class="anchored" data-anchor-id="bit-0-go-through-the-original-migration-guide">Bit 0: Go through the original migration guide</h3>
<p>This post assumes that you’re here because you’re migrating your fastpages website to Quarto. So, it’s a must that you first read <a href="https://nbdev.fast.ai/tutorials/blogging.html">Hamel’s migration guide</a> in detail.</p>
</section>
<section id="bit-1-install-nbdev-from-source" class="level3">
<h3 class="anchored" data-anchor-id="bit-1-install-nbdev-from-source">Bit 1: Install <code>nbdev</code> from source</h3>
<p>While running the <code>nbdev_migrate --path posts</code> step, you might encounter compatibility problems. This is, luckily, easily mitigated by installing <code>nbdev</code> from source:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> install git+https://github.com/fastai/nbdev</span></code></pre></div>
</section>
<section id="bit-2-use-a-separate-repository-for-dev" class="level3">
<h3 class="anchored" data-anchor-id="bit-2-use-a-separate-repository-for-dev">Bit 2: Use a separate repository for dev</h3>
<p>I found it it’s better and more convenient to use a separate repository while doing the initial developments related to the migration. For example, my website code is hosted at https://github.com/sayakpaul/portfolio/, but I maintained another repository during the initial development.</p>
<p>This especially comes in handy for setting up the GitHub Actions, previewing the site with GitHub Pages, etc. This way, you can also ensure that everything looks hunky-dory before you move the contents to the actual serving repository (“portfolio” in my case).</p>
</section>
<section id="bit-3-connect-with-github-actions-and-pages" class="level3">
<h3 class="anchored" data-anchor-id="bit-3-connect-with-github-actions-and-pages">Bit 3: Connect with GitHub Actions and Pages</h3>
<p>Most of us (if not all) want to automate the website publishing process after a change is pushed to the serving repository. So, we should definitely set up a workflow that does this for us. Refer to the <a href="https://quarto.org/docs/publishing/github-pages.html#github-action">official Quarto documentation</a> to use the recommended worflow file. It covers most use cases and it worked like a charm in my case. I didn’t have to touch a single thing in the workflow. Refer <a href="https://quarto.org/docs/publishing/github-pages.html#source-branch">here</a> for connecting your site with GitHub Pages.</p>
</section>
<section id="bit-4-pay-attention-to-branch-names" class="level3">
<h3 class="anchored" data-anchor-id="bit-4-pay-attention-to-branch-names">Bit 4: Pay attention to branch names</h3>
<p>If you used another repository as suggested in “Bit 2”, be careful about the branch names when moving the contents over to the actual repository. You need to configure a repository branch name in the following places, for example:</p>
<ul>
<li>The publishing <a href="https://github.com/sayakpaul/portfolio/blob/b51a1d390c26d3bfef49e40c5723a30f5b6ee9f0/.github/workflows/publish.yml#L4">worfkflow</a></li>
<li>The “_quarto.yml” <a href="https://github.com/sayakpaul/portfolio/blob/b51a1d390c26d3bfef49e40c5723a30f5b6ee9f0/_quarto.yml#L23">configuration</a></li>
</ul>
<p>This is important to note because fastpages defaults to “master” while recent GitHub repositories default to “main”.</p>
</section>
<section id="bit-5-careful-copying" class="level3">
<h3 class="anchored" data-anchor-id="bit-5-careful-copying">Bit 5: Careful copying</h3>
<p><em>(Only applicable if you used another repository as per “Bit 2”)</em></p>
<p>When you’re satisfied with the migrated content, you may wish to copy over all the elements in the original repository. Some things I found to be useful to keep in mind while performing this step (assuming both the backup and original repositories are locally cloned):</p>
<ul>
<li>Clear out the original repository contents, first: <code>rm -rf * .gitignore .github</code>. This will remove all the files and folders along with the <code>.gitignore</code> file. You can add any other hidden files to the mix as needed. But don’t accidentally delete the <code>.git</code> file.</li>
<li>Copy over the contents from your backup repository: <code>cp -r backup_repo/* original_repo/ &amp;&amp; cp -r backup_repo/.github original_repo/</code>. Finally, copy the <code>.gitignore</code> file: <code>cp backup_repo/.gitignore original_repo/</code>. You can copy any additional hidden file as needed, but not the <code>.git</code> file.</li>
</ul>
<p>I know that this step is slightly involved and can potentially be made simpler.</p>
</section>
<section id="bit-6-configuring-a-custom-domain" class="level3">
<h3 class="anchored" data-anchor-id="bit-6-configuring-a-custom-domain">Bit 6: Configuring a custom domain</h3>
<p><em>(Only applicable if you’re serving from a custom domain)</em></p>
<p>I struggled with this step a little. After setting up the CNAME on my repository <a href="https://github.com/sayakpaul/portfolio/blob/master/CNAME">here</a> and configuring the GitHub Pages section as shown <a href="https://huggingface.co/datasets/sayakpaul/sample-datasets/blob/main/github_pages_sayak.png">here</a>, my site was not getting rendered at “sayak.dev”, yielding 404 errors. After some Google searches, I stumbled upon <a href="https://github.com/quarto-dev/quarto-cli/discussions/3249#discussioncomment-4090518">this thread</a>, and it solved the problem!</p>
<p>And that’s it! I found the Quarto resources to be really self-sufficient and the migration process was way smoother than expected, thanks to the amazing resources.</p>


</section>

 ]]></description>
  <category>quarto</category>
  <category>blogging</category>
  <category>fastpages</category>
  <guid>https://sayak.dev/posts/quarto-migration.html</guid>
  <pubDate>Wed, 17 Jan 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Practicing ML in a Non-ML Job</title>
  <link>https://sayak.dev/posts/ml-practice.html</link>
  <description><![CDATA[ 





<p>Many people who aspire to become Machine Learning (ML) practitioners find it particularly difficult to continue to hone relevant skills when they pursue a job that does not involve even a tiny bit of ML. So, if you’re serious about choosing ML as a potential career option, it’s important to ensure you continue to practise what you’re learning along the way. Otherwise, there’d likely be nothing for a recruiter to trust in your candidature which, in turn, minimizes your chances of landing the ML job you always wanted.</p>
<p>I myself am not an exception to this. Back in 2017, when I was working at Tata Consultancy Services Limited (TCS), I didn’t get any assignments involving ML expertise. But I tried to utilize my off-work hours in a way that helped me improve my ML-specific knowledge as well as strengthen my candidature.</p>
<p>So, in this post, I’ll share what I did during those days in the hope of providing some meaningful ways for navigation.</p>
<p><strong>Disclaimer</strong>: The opinions stated in this post are solely mine and they are not meant to demean anyone else’s opinions about the same topic.</p>
<section id="assumptions" class="level2">
<h2 class="anchored" data-anchor-id="assumptions">Assumptions</h2>
<p>The post is best-suited for professionals that have prior experience in coding (preferably in Python) and know their way around the fundamentals of ML. If you’re an absolute beginner then I recommend picking up a book (<a href="https://www.manning.com/books/grokking-machine-learning">an example</a>) or a course (<a href="https://developers.google.com/machine-learning/crash-course">an example</a>) to get started. Also, if you haven’t yet picked up an ML framework (Scikit-Learn, PyTorch, TensorFlow, JAX, etc.), then I highly recommend picking one up.</p>
</section>
<section id="solace-in-uncertainty" class="level2">
<h2 class="anchored" data-anchor-id="solace-in-uncertainty">Solace in Uncertainty</h2>
<p>Set your objectives straight. Ask yourself if you’re absolutely certain about wanting to pursue a career in ML. If so, then are you willing to make the adjustments necessary to attain that at any cost? Although these questions are not specific to the purposes of this post, they help set a mindset to push through uncertain times.</p>
<p>I was hell-bent on taking up a career in ML that helped me to work on myself in those extra hours after work. It didn’t feel like I’m being forced into doing this. I thoroughly enjoyed the process and I trusted it. There are things I still enjoy doing like, reading a new paper, learning about a new concept, implementing it, etc.</p>
</section>
<section id="overwhelm-and-courage-to-learn" class="level2">
<h2 class="anchored" data-anchor-id="overwhelm-and-courage-to-learn">Overwhelm and Courage to Learn</h2>
<p>Feeling overwhelmed especially in the ML domain is common given how vast the field is and how rapidly it is evolving regularly. I see this positively because I know that there are things I don’t yet know and I use it as a learning opportunity to improve my knowledge.</p>
<p>One might wonder, do I learn each and everything that comes out? That’s impossible and likely, isn’t very helpful. So, I like to pick up something from the vault of things that genuinely interest me in ML and start digging deeper. I find it incredibly helpful in boosting my confidence. I also figured that the more I did this, the better I was able to develop a general understanding of a broad number of relevant things.</p>
<p>In a nutshell, treating the feeling of “overwhelm” as a learning opportunity has been quite helpful for me.</p>
</section>
<section id="learn-apply-demo-repeat" class="level2">
<h2 class="anchored" data-anchor-id="learn-apply-demo-repeat">Learn, Apply, (Demo), Repeat</h2>
<p>Learning is not enough. You need to be able to develop evidence that shows you can apply what you’ve learned successfully. I highly recommend reading <a href="https://blog.floydhub.com/emils-story-as-a-self-taught-ai-researcher/">this interview with Emil Wallner</a> who’s an “internet-taught” ML Researcher working as a resident at Google.</p>
<p>Below, I discuss a few things you can do to exercise your ML learnings.</p>
<section id="kaggle" class="level3">
<h3 class="anchored" data-anchor-id="kaggle">Kaggle</h3>
<p>Kaggle is arguably one of the best platforms to develop skills for data preprocessing and applying ML in creative ways to solve unique problems. So, pick an interesting dataset or a competition <em>just for learning purposes</em>. Putting the competitive mindset aside, during my initial years it really helped me to develop a mindset of always learning to facilitate self-improvement. If you commit to it hard enough, you will have developed a bunch of useful skills. Over time, you’ll definitely get better.</p>
<p>Keeping an open mind for learning is important here because expectations of outcomes can quickly derail you. Also, remember that the rate of improvement is not the same for everyone. So, it’s better to just do things that are within your control (for example, learning something), and consistently get better at those.</p>
</section>
<section id="papers-concepts" class="level3">
<h3 class="anchored" data-anchor-id="papers-concepts">Papers / Concepts</h3>
<p>Reading research papers is a common undertaking in ML. It can be rewarding to summarize, implement, and blog about a paper that is impactful and tackles interesting problems. Extending on this theme, you have a number of options:</p>
<ul>
<li><p>You can summarize a paper in your own words and publish it on platforms like <a href="https://medium.com/">Medium</a> or even on <a href="https://github.com/fastai/fastpages">your own blog</a>. It’s also important to get feedback on your work. So, feel free to share your work on Social Media as well as let the authors of the actual paper know about your work. A paper summary is supposed to be a reflection of how you perceived the paper. So, if you have criticisms of a paper, do include those with solid reasoning. If you’re looking for an example, definitely check out <a href="https://medium.com/@nainaakash012">Aakash Kumar Nain’s paper summaries</a>.</p>
<p>Picking a paper could be a non-trivial work especially when there’s always a flood of papers on <a href="https://arxiv.org/">arXiv</a>. I usually follow the blogs of research labs at <a href="https://ai.googleblog.com/">Google</a>, <a href="https://ai.facebook.com/blog">Meta</a>, <a href="https://blog.allenai.org/">AI2</a>, <a href="https://bair.berkeley.edu/">BAIR</a>, etc., to keep myself up-to-date about the work I care about. There’s a good chance you’ll find your niche there. Following the works of the most accomplished researchers from my favorite domains is another practice I incorporate.</p>
<p>In this regard, I highly recommend the following two books that actively cite examples of relevant research papers and also implement them in ways that are practically beneficial: <a href="https://www.oreilly.com/library/view/deep-learning-for/9781492045519/">Deep Learning for Coders with fastai and PyTorch</a> by Jeremy Howard and Sylvain Gugger, <a href="https://www.oreilly.com/library/view/natural-language-processing/9781098103231/">Natural Language Processing with Transformers</a> by Lewis Tunstall, Leandro von Werra, and Thomas Wolf. For developing a general understanding of different areas in ML, I recommend the articles on <a href="https://distill.pub/">Distill Pub</a>. <br><br></p></li>
<li><p>Nowadays, a majority of ML papers come with official open-source implementations in the interest of reproducibility. But some don’t. Regardless of either, it’s a good exercise to try to implement the novel bits of a paper. The <a href="https://github.com/rwightman/pytorch-image-models">timm</a><a href="https://github.com/rwightman/pytorch-image-models">libary</a> is a great example of how paper reimplementations should be structured. <br><br></p></li>
<li><p>Blogging has easily become one of the most effective ways to communicate your understanding of something. This does not need to be just tied to papers, though. You can always pick up an interesting concept and blog about it. Many ML stalwarts keep pressing on why you should blog and here is one such example: <a href="https://medium.com/@racheltho/why-you-yes-you-should-blog-7d2544ac1045">Why you (yes, you) should blog</a> by Rachel Thomas.</p></li>
</ul>
<p>You can also consider making videos on papers, concepts, and so on. If you haven’t already, then definitely get to know <a href="https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfew">Yannic Kilcher</a> who has revolutionized the way forward in this theme.</p>
</section>
<section id="open-source-contributions" class="level3">
<h3 class="anchored" data-anchor-id="open-source-contributions">Open-source Contributions</h3>
<p>From my personal experience, I can confirm that making open-source contributions is one of the most useful ways to stay involved in the domain. All the popular ML Python libraries (Scikit-Learn, PyTorch, TensorFlow, Keras, JAX, Hugging Face Transformers, etc.) are open-source and that provides even more opportunities to learn and grow.</p>
<p>I have a <a href="https://www.youtube.com/watch?v=VsBEFUoESR4&amp;t=3123s">separate presentation</a> on this topic but here, I provide my perspectives for context:</p>
<ul>
<li><p>When you’re contributing to a well-maintained open-source library for the first time there’s a high chance that you’ll learn a few things other than just ML. These include writing unit tests, setting up the local development environment, library building tools, etc. This way, you get first-hand exposure to how software engineering is approached in the ML domain in general.</p>
<p>So, not only do you get to contribute to your favorite open-source library (which is an inexplicable feeling anyway), but you also get to learn skills that are practically quite demanding. Beyond these, you get a chance to interact with experts and get their feedback to improve your work. Additionally, you get to collect objective evidence of your skills - coding, thorough understanding of a critical component and the library, building a library, etc. - all of which are noteworthy.</p>
<p>Note that you’re not alone if you’re feeling lost when you’re just starting to contribute to an open-source library. It happens to most. But when you put your mind toward making your contribution anyway, you get to get better in the process. <br><br></p></li>
<li><p>If you feel you’re not ready yet to make contributions, working on your own open-source projects is another promising avenue to pursue. Take Andrej Karpathy’s <a href="https://github.com/karpathy/minGPT">miniGPT</a> project as an example. Besides being an amazing educational resource for learning about the <a href="https://en.wikipedia.org/wiki/GPT-3">GPT model</a>, it serves as a great reference for implementing many of the foundational blocks of <a href="https://arxiv.org/abs/1706.03762">Transformer</a>-based architectures.</p>
<p>If you’re looking for open-source project ideas then <a href="https://youtu.be/dllfKQKlzvg">my presentation</a> on this topic might be helpful.</p></li>
</ul>
<p>Now that we’ve looked into different ways of being engaged with our independent ML practice, let us take examples of two individuals from the ML community who have followed similar paths in this regard.</p>
</section>
</section>
<section id="references-from-the-community" class="level2">
<h2 class="anchored" data-anchor-id="references-from-the-community">References from the Community</h2>
<p><a href="https://twitter.com/carrigmat">Matt</a> (ML Engineer at Hugging Face) says -</p>
<blockquote class="blockquote">
<p><em>[…] I did a few small projects to get familiar with Keras and then tried reimplementing papers or building examples to contribute to places like <a href="https://github.com/keras-team/keras-contrib">keras-contrib</a> or <a href="https://keras.io/">keras.io</a>.</em></p>
</blockquote>
<p><br></p>
<p><a href="https://twitter.com/algo_diver">Chansung</a> (ML-GDE and MLOps Engineer) says -</p>
<blockquote class="blockquote">
<p><em>[…] Anyways, I actually didn’t plan what to do for the next few years. I just have followed my interests and the joy to participate as a community member. And whenever I make any moves, I found other exciting events are waiting for me. These days, I am really enjoying creating open-source projects and applied ML products, and collaborative projects with you as well.</em></p>
</blockquote>
<p><br></p>
<p>Both of them continue to push their boundaries for self-improvement and are exceptional at what they do.</p>
</section>
<section id="finishing-up" class="level2">
<h2 class="anchored" data-anchor-id="finishing-up">Finishing Up</h2>
<p>The pursuit of betterment doesn’t stop after you land the job you were aspiring for. I continue to benefit from my open-source engagements even after professionally working in the area for some time now. I hope you’re able to take forward the pointers discussed in the post and experiment with them. If you have any suggestions for other interesting ways for independent ML practice please <a href="mailto:spsayakpaul@gmail.com">let me know</a>.</p>
</section>
<section id="acknowledgments" class="level2">
<h2 class="anchored" data-anchor-id="acknowledgments">Acknowledgments</h2>
<p>I want to thank all my wonderful collaborators and mentors who continue to inspire me to be better. I am also thankful to <a href="https://www.instagram.com/neerajanmusic/">Neerajan Saha</a> for proofreading this post.</p>


</section>

 ]]></description>
  <category>ml-practice</category>
  <category>jobs</category>
  <guid>https://sayak.dev/posts/ml-practice.html</guid>
  <pubDate>Fri, 27 May 2022 00:00:00 GMT</pubDate>
</item>
<item>
  <title>First Steps in GSoC</title>
  <link>https://sayak.dev/posts/gsoc-faqs.html</link>
  <description><![CDATA[ 





<p>In this post, I discuss my perspective on two primary questions pertaining to the <a href="https://summerofcode.withgoogle.com/">Google Summer of Code (GSoC) program</a>. Even though my work is centered around Machine Learning (ML), I believe these pointers are domain-agnostic. This is based on <a href="https://blog.tensorflow.org/2021/09/tensorflow-hubs-experience-with-gsoc-2021.html">my experience of mentoring for TensorFlow at GSoC 2021</a>. Please note that these thoughts are of my own and may not reflect what anyone else (including the program organizers and my employer) thinks.</p>
<section id="how-should-i-get-started" class="level2">
<h2 class="anchored" data-anchor-id="how-should-i-get-started">How should I get started?</h2>
<p>First and foremost, it’s important to acknowledge that GSoC requires some amount of open-source experience beforehand. That not only makes your application stronger but also sets you up for the program itself. But beyond everything else, having a genuine passion for contributing to open-source is important and is a key enabling factor. Open-source should be a fun engagement driven by your passion for helping a community. So, ensure you’re chasing the right things.</p>
<ul>
<li><p>Understand what GSoC is, how it works, what are the rules, and study some projects from the previous years by going to GSoC’s official website: ​​<a href="https://summerofcode.withgoogle.com/" class="uri">https://summerofcode.withgoogle.com/</a>.</p>
<p>Here’s <a href="https://developers.google.com/open-source/gsoc/resources/downloads/GSoC2022Presentation.pdf">another resource</a> that might be equally helpful. It’s important that you set your expectations right from the very beginning. Sometimes having conversations with the past GSoC contributors is really helpful in this regard. <br></p></li>
<li><p>Take a look at the <a href="https://summerofcode.withgoogle.com/programs/2022/organizations">organizations</a> taking part in GSoC. You’ll notice that they have all their projects listed for which they are welcoming contributions. <br></p></li>
<li><p>Study all the official resources that are out there for the project you want to contribute to. You may be interested in multiple projects but it helps to laser focus on one so that you can precisely figure out what components you’d want to work on, your timeline, etc. <br></p></li>
<li><p>Get started contributing. Here are some good examples that make it clear what a GSoC contributor should do first:</p>
<ul>
<li><a href="https://ardupilot.org/dev/docs/gsoc.html">ArduPilot</a></li>
<li><a href="https://gsoc.gnome.org/">GNOME</a></li>
<li><a href="https://pcp.io/gsoc/contributors.html">Performance Co-Pilot</a></li>
<li><a href="https://robocomp.github.io/web/gsoc/2022/contributor_guidance">RoboComp</a></li>
<li><a href="https://www.mediawiki.org/wiki/Google_Summer_of_Code/Participants#Application_process_steps">Wikimedia Foundation</a> <br><br></li>
</ul></li>
<li><p>Sometimes a project may not require having prior contribution experience but having it is almost always better. <a href="https://www.tensorflow.org/hub/overview">TensorFlow Hub</a> (TF-Hub) is one such example where you’re generally asked to code a ML model, (optionally) train it, and host the model on TF-Hub thereby making it easier for the community to use the model. <br><br></p></li>
<li><p>Lastly, if you haven’t worked with a version control system before definitely spend time doing that. Git is a good example of such a system and <a href="https://www.udacity.com/course/version-control-with-git--ud123">here’s</a> a good course from Udacity that can be helpful.</p></li>
</ul>
</section>
<section id="what-makes-a-proposal-great" class="level2">
<h2 class="anchored" data-anchor-id="what-makes-a-proposal-great">What makes a proposal great?</h2>
<p>Firstly, I’d like to acknowledge that the answers to this question should be subjective. That said, I think there are certain common aspects that are shared by all great GSoC proposals.</p>
<ul>
<li><p>GSoC is about building things. So including your experience that reflects the same immediately catches the eye. You build that experience over a period of time, it’s not something built overnight. That way, your experience speaks about a few things: consistency, technical depth, punctuality, communication, etc. Let me provide some examples. <br><br> Say, you wanted to contribute to <a href="https://github.com/keras-team/keras-cv/">KerasCV</a> by adding a new layer(s) to it. If you can show that you’ve already worked on something that reflects the experience relevant to the contribution, it puts you in a better position than someone without that experience.</p>
<p>Similarly, if you wanted to contribute a model to TF-Hub, it helps to show that you’ve experience implementing models and relevant things such as layers, blocks, etc. <br></p></li>
<li><p>When you’re talking about an experience in your proposal be sure to back it with verifiable links. Without that, the mention becomes practically void. <br></p></li>
<li><p>Don’t just mention the components of the project you’d like to work on. Include all the nitty-gritty of that – why you’d like to work on them and why it’s useful, what your approaches will be, etc. If you anticipate some edge cases or blockers include them too. This speaks volumes about your maturity. <br></p></li>
<li><p>Keep your proposal grammatically correct and easily understandable. This helps you communicate your proposal better. Remember that it’s your responsibility to ensure that your proposal was communicated in an expected way. <br><br></p></li>
<li><p>Sometimes, applications come with incomplete sentences, inconsistency in sentence casing, without punctuations, etc. This is an anti-pattern. Try hard to ensure your proposal doesn’t have those things. This may readily reduce the seriousness of your proposal and the work you put into it. <br><br></p></li>
<li><p>Include a realistic timeline that covers the project deliverables and includes enough time for you and the mentors to communicate effectively. Unexpected things can happen all the time so, it helps to also include some extra time to dedicate to those situations.</p></li>
</ul>
<p>Sometimes, a project may welcome ideas from the contributors. If you’d like to propose something that’s already not enlisted in a project, be sure to reach out to the project mentor to discuss the feasibility of your idea before working on the proposal.</p>
</section>
<section id="additional-notes" class="level2">
<h2 class="anchored" data-anchor-id="additional-notes">Additional notes</h2>
<p>During <a href="https://summerofcode.withgoogle.com/archive/2021/organizations/6649841832165376">GSoC 2021</a>, I had the opportunity to work with Aditya Kane and Vausdev Gupta as their mentor. Here are their GSoC proposals:</p>
<ul>
<li><a href="https://docs.google.com/document/d/1h9kZCywWWveekUFH1SS5rBZINK-W9SDb/edit">Aditya</a></li>
<li><a href="https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/assets/milestone.pdf">Vasudev</a></li>
</ul>
<p>I’m fortunate to be mentoring for TensorFlow at GSoC 2022 as well. If you’re curious, I have a <a href="https://www.youtube.com/watch?v=VsBEFUoESR4&amp;t=3123s">presentation</a> that discusses how open-source can enable different possibilities in the ML world. If you’re looking for an example of a non-ML proposal, then <a href="https://drive.google.com/file/d/1c-nqgm54pIvm_YQKJ4SohLUV6iKGGYH6/view">Anubhav Singh’s proposal</a> is a great example.</p>
<p>Additionally, we penned down our mentorship experience in <a href="https://blog.tensorflow.org/2021/09/tensorflow-hubs-experience-with-gsoc-2021.html">this blog post</a> that may provide additional context.</p>


</section>

 ]]></description>
  <category>gsoc</category>
  <category>open-source</category>
  <guid>https://sayak.dev/posts/gsoc-faqs.html</guid>
  <pubDate>Sun, 13 Mar 2022 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Publishing ConvNeXt Models on TensorFlow Hub</title>
  <link>https://sayak.dev/posts/convnext-tfhub.html</link>
  <description><![CDATA[ 





<p>I recently added 15 different variants of the <a href="https://arxiv.org/abs/2201.03545">ConvNeXt architecture</a> to <a href="https://tfhub.dev/sayakpaul/collections/convnext/1">TensorFlow Hub</a> (TF-Hub). This post is a reflection of what had to be done to get to that point. First, we’ll discuss the implementation of ConvNeXt in Keras and how the original pre-trained parameters were ported into these models. We’ll then talk about TF-Hub’s ConvNeXt collection and what it offers.</p>
<p>I hope this post is useful for anyone willing to contribute models to TF-Hub as doing it the right way can be a good amount of work.</p>
<section id="about-convnext" class="level1">
<h1>About ConvNeXt</h1>
<p>ConvNeXt models were proposed by Liu et al.&nbsp;in <a href="https://arxiv.org/abs/2201.03545">A ConvNet for the 2020s</a>. ConvNeXt models are composed of standard layers such as depthwise convolutions, layer normalization, etc., and use standard network topologies. They don’t use self-attention or any hybrid approaches, unlike the recent architectures such as <a href="https://arxiv.org/abs/2010.11929">Vision Transformers</a>, <a href="https://arxiv.org/abs/2106.04803">CoAtNet</a>, etc. The authors start with a base architecture and gradually refine it to match some of the design choices of <a href="https://arxiv.org/abs/2103.14030">Swin Transformers</a>. In the process, they developed a family of models named ConvNeXt achieving performance on the <a href="https://www.image-net.org/">ImageNet-1k dataset</a> with efficiency. For details, check out the <a href="https://arxiv.org/abs/2201.03545">original paper</a>.</p>
<img src="https://github.com/sayakpaul/portfolio/raw/master/posts/convnext_tfhub/convnext.png" class="img-fluid">
<center>
<b>Figure 1</b>: ConvNeXt performance (source: original paper).
</center>
</section>
<section id="implementation-and-weight-porting" class="level1">
<h1>Implementation and weight porting</h1>
<p>The ConvNeXt models are fairly easy to implement especially with the <a href="https://github.com/facebookresearch/ConvNeXt">official PyTorch codebase available</a> for reference. As mentioned before, these models can be implemented using the standard components provided in most of the major deep learning frameworks such as JAX, PyTorch, and TensorFlow.</p>
<p>ConvNeXt models use the following block structure with layer scaling as introduced in <a href="https://arxiv.org/abs/2103.17239">Going deeper with image transformers</a> by Touvron et al.</p>
<center>
<figure class="figure">
<img src="https://github.com/sayakpaul/portfolio/raw/master/posts/convnext_tfhub/convnext_block.png" class="figure-img">
</figure>
</center>
<center>
<b>Figure 2</b>: ConvNeXt block (source: original paper).
</center>
<p>The skip connection is controlled with <a href="https://arxiv.org/abs/1603.09382">Stochastic Depth</a> to induce regularization <em>during</em> training. Different ConvNeXt variants correspond to different depths along with different channels used in each of the stages. For example, the “tiny” variant uses the following setup:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb1-1">depths <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>]</span>
<span id="cb1-2">dims <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">96</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">192</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">384</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">768</span>]</span></code></pre></div>
<p>If you plan to populate the implemented models with the original parameters then it helps to align the architecture implementation with the official one as much as possible. Since I went with this approach I tried closely following <a href="https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py">the official implementation</a>. My final implementation is available in <a href="https://github.com/sayakpaul/ConvNeXt-TF/blob/main/models/convnext_tf.py">this script</a>. Note that, it does not yet include the <a href="https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext_isotropic.py">isotropic ConvNeXt models</a>.</p>
<p>Coming to the weight porting part, this is usually the most interesting part because there’s no standard recipe that’d work for all the models. You’ll need to think about how to best align the original model parameters with your implementation.</p>
<p>A ConvNeXt model is divided into three main parts: (1) stem which directly operates on the input image, (2) downsample blocks that reduce the resolution of feature maps as the network progresses, and (3) stages that apply the ConvNeXt blocks shown above. This is why I organized my weight porting script such that it has a correspondence between these different parts with the original parameters. Here is an example:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> layer <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> stem_block.layers:</span>
<span id="cb2-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(layer, tf.keras.layers.Conv2D):</span>
<span id="cb2-3">        layer.kernel.assign(</span>
<span id="cb2-4">            tf.Variable(param_list[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].numpy().transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>))</span>
<span id="cb2-5">        )</span>
<span id="cb2-6">        layer.bias.assign(tf.Variable(param_list[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].numpy()))</span>
<span id="cb2-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(layer, tf.keras.layers.LayerNormalization):</span>
<span id="cb2-8">        layer.gamma.assign(tf.Variable(param_list[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>].numpy()))</span>
<span id="cb2-9">        layer.beta.assign(tf.Variable(param_list[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>].numpy()))</span></code></pre></div>
<p>The most difficult bit was figuring out how to properly populate the weights of the convolutional layers in TensorFlow from PyTorch. In an earlier implementation, I was simply using <code>transpose()</code>. The resulting models were giving poorer performance than expected. <a href="https://in.linkedin.com/in/vasudevgupta7">Vasudev</a> helped me figure out the correct transposition of the weight axes and the models were then coming out as expected. More about the evaluation of these models in a moment.</p>
<p>Once the weights were ported successfully, the next task was to verify if the outputs of the intermediate layers matched with their original counterparts. One minor detail to note here is that the outputs of layers are <strong>not</strong> the same as their parameters. So, even if you check if the parameters of your implemented model and the original model are matching, their outputs could still mismatch. This mainly happens because of mismatches between the layer configurations of your model and the original one.</p>
<p>The final model conversion script is available <a href="https://github.com/sayakpaul/ConvNeXt-TF/blob/main/convert.py">here</a>.</p>
</section>
<section id="evaluation-of-the-models" class="level1">
<h1>Evaluation of the models</h1>
<p>To be more certain, it’s also important to check the evaluation metrics of the converted models on the datasets used during training. In this case, we need to use the top-1 accuracy of the models on the ImageNet-1k dataset (validation set).</p>
<p>To set up this evaluation, I developed <a href="https://github.com/sayakpaul/ConvNeXt-TF/blob/main/i1k_eval/eval.ipynb">this notebook</a> where I closely followed <a href="https://github.com/facebookresearch/ConvNeXt/blob/main/datasets.py">the preprocessing used in the official codebase</a> for inference. The following table reflects the top-1 accuracies of the converted models along with the original scores reported <a href="https://github.com/facebookresearch/ConvNeXt/#results-and-pre-trained-models">here</a>.</p>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
</style>
<table class="tg">
<thead>
<tr>
<th class="tg-c3ow">
<span style="font-weight:bold">Name</span>
</th>
<th class="tg-c3ow">
<span style="font-weight:bold">Original acc@1</span>
</th>
<th class="tg-c3ow">
<span style="font-weight:bold">Keras acc@1</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-c3ow">
convnext_tiny_1k_224
</td>
<td class="tg-c3ow">
82.1
</td>
<td class="tg-c3ow">
81.312
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_small_1k_224
</td>
<td class="tg-c3ow">
83.1
</td>
<td class="tg-c3ow">
82.392
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_base_1k_224
</td>
<td class="tg-c3ow">
83.8
</td>
<td class="tg-c3ow">
83.28
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_base_1k_384
</td>
<td class="tg-c3ow">
85.1
</td>
<td class="tg-c3ow">
84.876
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_large_1k_224
</td>
<td class="tg-c3ow">
84.3
</td>
<td class="tg-c3ow">
83.844
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_large_1k_384
</td>
<td class="tg-c3ow">
85.5
</td>
<td class="tg-c3ow">
85.376
</td>
</tr>
<tr>
<td class="tg-c3ow">
</td>
<td class="tg-c3ow">
</td>
<td class="tg-c3ow">
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_base_21k_1k_224
</td>
<td class="tg-c3ow">
85.8
</td>
<td class="tg-c3ow">
85.364
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_base_21k_1k_384
</td>
<td class="tg-c3ow">
86.8
</td>
<td class="tg-c3ow">
86.79
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_large_21k_1k_224
</td>
<td class="tg-c3ow">
86.6
</td>
<td class="tg-c3ow">
86.36
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_large_21k_1k_384
</td>
<td class="tg-c3ow">
87.5
</td>
<td class="tg-c3ow">
87.504
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_xlarge_21k_1k_224
</td>
<td class="tg-c3ow">
87.0
</td>
<td class="tg-c3ow">
86.732
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_xlarge_21k_1k_384
</td>
<td class="tg-c3ow">
87.8
</td>
<td class="tg-c3ow">
87.68
</td>
</tr>
</tbody>
</table>
<p><code>Keras acc@1</code> refers to the scores of my implementation. Differences in the results are primarily because of the differences in the library implementations, especially how image resizing is implemented in PyTorch and TensorFlow. My evaluation logs are available at <a href="https://tensorboard.dev/experiment/odN7OPCqQvGYCRpJP1GhRQ/">this URL</a>. I’d like to thank <a href="https://twitter.com/gusthema">Gus</a> from the TF-Hub team for the productive discussions during this phase.</p>
</section>
<section id="publishing-on-tf-hub" class="level1">
<h1>Publishing on TF-Hub</h1>
<p>With the models converted as expected, I was now tasked with publishing them on TF-Hub. These models can be categorized into two different variants: (1) off-the-shelf classifiers and (2) feature extractors used for downstream tasks. This means that the 15 model variants that I had converted would actually amount to 30 models.</p>
<p>Whenever I publish models on TF-Hub, I try to accompany each model with the following:</p>
<ul>
<li>Documentation that includes references of the models, how it was exported, etc.</li>
<li>Colab Notebook showing the model usage.</li>
</ul>
<p>Doing these things (especially the documentation part) for 30 models can be quite cumbersome. <a href="https://www.linkedin.com/in/willi-gierke/">Willi</a> from the TF-Hub team supported me in automatically generating documentation for 30 models. The script is available <a href="https://github.com/sayakpaul/ConvNeXt-TF/blob/main/hub_utilities/generate_doc.py">here</a>. This script was basically generated from a documentation template and can be used for generating documentation when publishing more than one model. Additionally, I worked on a <a href="https://github.com/sayakpaul/ConvNeXt-TF/blob/main/hub_utilities/export_to_hub.py">script</a> that can archive the TensorFlow SavedModels in a way accepted by TF-Hub.</p>
<p>I hope these scripts will be beneficial for anyone planning to contribute models to TF-Hub.</p>
<p>As of today, all 30 models are <a href="https://tfhub.dev/sayakpaul/collections/convnext/1">available on TF-Hub</a>. They come with Colab Notebooks and documentation so that it’s easier to get started. Moreover, these TF-Hub models are not black-box SavedModels. You can load them as <code>tf.keras.Model</code> objects for further inspection. Here’s an example:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb3-1">model_gcs_path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gs://tfhub-modules/sayakpaul/convnext_tiny_1k_224/1/uncompressed"</span></span>
<span id="cb3-2">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.models.load_model(model_gcs_path)</span>
<span id="cb3-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(model.summary(expand_nested<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>))</span></code></pre></div>
</section>
<section id="wrapping-up" class="level1">
<h1>Wrapping up</h1>
<p>That’s it for the post. We discussed a standard workflow that I use to publish models on TF-Hub and the difficulties that can arise during the process. I hope you’ve found it to be worthy of your time and thank you for reading!</p>
</section>
<section id="acknowledgements" class="level1">
<h1>Acknowledgements</h1>
<ul>
<li><a href="https://github.com/vasudevgupta7">Vasudev</a> for helping with transposition bug</li>
<li><a href="https://twitter.com/gusthema">Gus</a> for fruitful discussions</li>
<li><a href="https://www.linkedin.com/in/willi-gierke/">Willi</a> for helping with publishing</li>
<li><a href="https://developers.google.com/programs/experts/">ML-GDE program</a> for providing Google Cloud Platform credits</li>
</ul>


</section>

 ]]></description>
  <category>tensorflow</category>
  <category>keras</category>
  <category>cnns</category>
  <category>imagenet-1k</category>
  <category>convnext</category>
  <guid>https://sayak.dev/posts/convnext-tfhub.html</guid>
  <pubDate>Thu, 03 Feb 2022 00:00:00 GMT</pubDate>
  <media:content url="https://sayak.dev/posts/convnext_tfhub/convnext_header.png" medium="image" type="image/png" height="93" width="144"/>
</item>
<item>
  <title>Distributed Training in TensorFlow with AI Platform &amp; Docker</title>
  <link>https://sayak.dev/posts/distributed-training.html</link>
  <description><![CDATA[ 





<p>Jupyter Notebooks are a great way to present your code offering a good level of interactivity, transparency, and reproducibility. However, operating with a Jupyter Notebook environment can get very challenging if you are working your way through large-scale training workflows as is common in deep learning.</p>
<p>If you are conducting large-scale training it is likely that you are using a powerful remote machine via SSH access. So, even if you are not using Jupyter Notebooks, problems like SSH pipe breakage, network teardown, etc. can easily occur. Consider using a powerful virtual machine on Cloud as your remote. The problem gets far worse when there’s a connection loss but you somehow forget to turn off that virtual machine to stop consuming its resources. You get billed for practically <em>nothing</em> when the breakdown happens until and unless you have set up some amount of alerts and fault tolerance.</p>
<p>To resolve these kinds of problems, we would want to have the following things in the pipeline:</p>
<ul>
<li>A training workflow that is fully managed by a secure and reliable service with high availability.</li>
<li>The service should automatically provision and de-provision the resources we would ask it to configure allowing us to only get charged for what’s been truly consumed.</li>
<li>The service should also be very flexible. It must not introduce too much technical debt into our existing pipelines.</li>
</ul>
<p>In this post, we are going to consider all of these factors and will implement them using a service called <a href="https://cloud.google.com/ai-platform">AI Platform</a> (provided by GCP) and <a href="https://www.docker.com/">Docker</a>. We will use TensorFlow and Keras to handle <strong>distributed training</strong> to develop an image classification model capable of classifying cats and dogs. Apart from deep learning-related knowledge, a bit of familiarity would be needed to fully understand this post.</p>
<p>All the code presented throughout the post <a href="https://github.com/sayakpaul/Distributed-Training-in-TensorFlow-2-with-AI-Platform">can be found here</a>. We won’t be covering the entire codebase, instead, we will focus on the most important bits.</p>
<p>If you are like me, who have lost sleep over the very thought of the aforementioned problem, you will likely find this tutorial a good starting point to get around it.</p>
<section id="environment-setup" class="level1">
<h1>Environment setup</h1>
<p>You will need to have Docker, command-line GCP (Google Cloud Platform) tools like <code>gcloud</code>, and TensorFlow (2.x) installed if you are on a local machine. But if you have a <a href="https://cloud.google.com/billing/docs/how-to/modify-project">billing-enabled GCP project</a> it’s possible to get started <em>without</em> any significant setup.</p>
<p>We will use a cheap <a href="https://cloud.google.com/ai-platform-notebooks">AI Platform Notebook</a> instance as our staging machine which we will use to build our custom Docker image, push it to <a href="https://cloud.google.com/container-registry">Google Container Registry (GCR)</a>, and submit a <code>training</code> job to AI Platform. Additionally, we will use this instance to create <a href="https://www.tensorflow.org/tutorials/load_data/tfrecord">TensorFlow Records</a> (TFRecords) from the original dataset (<a href="https://www.tensorflow.org/datasets/catalog/cats_vs_dogs">Cats vs.&nbsp;Dogs</a> in this case) and upload them to a GCS Bucket. AI Platform notebooks come pre-configured with many useful Python libraries, Linux packages like <code>docker</code>, and also the command-line GCP tools like <code>gcloud</code>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>I used an <code>n1-standard-4</code> instance with TensorFlow 2.4 as the base image which costs <strong>$0.141 hourly</strong>.</p>
</div>
</div>
</section>
<section id="notes-on-the-task-data-pipeline-and-training" class="level1">
<h1>Notes on the task, data pipeline, and training</h1>
<p><strong>Task</strong></p>
<p>As mentioned earlier, we will be training an image classification model on the Cats vs.&nbsp;Dogs dataset which is a moderate-sized dataset. The learning problem is a <a href="https://developers.google.com/machine-learning/glossary#binary_classification">binary classification</a> task.</p>
<p><strong>Data pipeline</strong></p>
<p>For setting up our data pipeline, we will first create shards of TFRecords from the original dataset. Each of the shards will contain batches of preprocessed images and their labels. This has an advantage. When we would load these shards back for training, we won’t need to do any preprocessing giving us a slight performance boost. Figure 1 demonstrates our TFRecords’ creation workflow.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/sayakpaul/portfolio/raw/master/posts/distributed_training/data.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: Schematics of our TFRecord’s creation process.</figcaption>
</figure>
</div>
<center>
<b>Figure 1</b>: Schematics of our TFRecord’s creation process.
</center>
<p>As you might have already noticed that we have also thrown in another component in the mix – a GCS Bucket. We would need to store our data on a Google Cloud Storage (GCS) Bucket since the training code won’t be executed locally. We could have used other bucket services (like <a href="https://aws.amazon.com/s3/">AWS S3</a>) here but TensorFlow has very unified integrations with GCS Buckets, hence. We will be using the same GCS Bucket to store our trained model and also TensorBoard logs. The total <a href="https://cloud.google.com/storage/pricing">cost</a> to store all of these will be about <strong>$1.20</strong>.</p>
<p>You are welcome to check out the corresponding code <a href="https://github.com/sayakpaul/Distributed-Training-in-TensorFlow-2-with-AI-Platform/blob/main/trainer/create_tfrecords.py">here</a>. In order to streamline the TFRecords’ creation and upload process we will make use of a little <a href="https://www.shellscript.sh/">shell script</a>:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">echo</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Uploading TFRecords to Storage Bucket..."</span></span>
<span id="cb1-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">echo</span> gs://<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">${BUCKET_NAME}</span></span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">python</span> ../trainer/create_tfrecords.py</span>
<span id="cb1-5"></span>
<span id="cb1-6"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">gsutil</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-m</span> cp <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-r</span> train_tfr gs://<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">${BUCKET_NAME}</span></span>
<span id="cb1-7"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">gsutil</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-m</span> cp <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-r</span> validation_tfr gs://<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">${BUCKET_NAME}</span></span>
<span id="cb1-8"></span>
<span id="cb1-9"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">gsutil</span> ls <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-lh</span> gs://<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">${BUCKET_NAME}</span></span></code></pre></div>
<p>After creating the TFRecords we simply copy them over to a previously created GCS Bucket. You can create one by executing the following: <code>gsutil mb ${BUCKET_NAME}</code>.</p>
<p><strong>Training</strong></p>
<p>As for the training pipeline, we will follow the steps below: - Load the TFRecords from GCS using CPU in a parallelized way using <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset">tf.data.Dataset.map()</a> and feed batches of data to our model. For performance, we will also <a href="https://www.tensorflow.org/guide/data_performance#prefetching">prefetch</a> several future batches of data so that our model does not have to wait for the data to consume. Our data loader is present here in <a href="https://github.com/sayakpaul/Distributed-Training-in-TensorFlow-2-with-AI-Platform/blob/main/trainer/data_loader.py">this script</a>. - We will be using a pre-trained model to unleash the power of <a href="https://ruder.io/transfer-learning/">transfer learning</a>. In particular, we will be using the <a href="https://arxiv.org/pdf/1608.06993">DenseNet121</a> model that is available inside <a href="https://www.tensorflow.org/api_docs/python/tf/keras/applications">tf.keras.applications</a>. - We will be training our model inside the <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy">tf.distribute.MirroredStrategy</a> scope for distributed training. This strategy is applicable when we have a single host containing multiple GPUs. We will also be using <a href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html">mixed-precision training</a> to speed up the process. The code for realizing this is <a href="https://github.com/sayakpaul/Distributed-Training-in-TensorFlow-2-with-AI-Platform/blob/main/trainer/model_training.py">here</a>.</p>
<p>The training will take place on a remote machine fully managed by AI Platform.</p>
<p>So far, we have discussed the utilities for creating TFRecords, loading them, and building and training our model. Here’s how the code is structured in the <a href="https://github.com/sayakpaul/Distributed-Training-in-TensorFlow-2-with-AI-Platform">GitHub repository</a> mentioned at the beginning of the post:</p>
<pre class="bashell"><code>├── Dockerfile
├── README.md
├── config.yaml
├── scripts
│   ├── train_cloud.sh
│   ├── train_local.sh
│   └── upload_tfr.sh
└── trainer
    ├── config.py
    ├── create_tfrecords.py
    ├── data_loader.py
    ├── model_training.py
    ├── model_utils.py
    ├── task.py
    └── tfr_utils.py</code></pre>
<p>Next, we will be reviewing how Docker fits into all these. From there on, we will have all the recipes set up to kickstart model training.</p>
</section>
<section id="fitting-in-docker" class="level1">
<h1>Fitting in Docker</h1>
<p>To submit custom training jobs to AI Platform, we need to package our code inside a Docker image. So, let’s start with that.</p>
<p>To build a Docker image, we first need to define a <code>Dockerfile</code> specifying how it should itself up. <a href="https://cloud.google.com/container-registry">Google Container Registry (GCR)</a> provides CUDA-configured TensorFlow containers that we can use to build custom ones. In our case, we extend one such container. Our <a href="https://github.com/sayakpaul/Distributed-Training-in-TensorFlow-2-with-AI-Platform/blob/main/Dockerfile"><code>Dockerfile</code></a> looks like so:</p>
<pre class="bashell"><code># Use an existing CUDA-configured TensorFlow container
FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-4
WORKDIR /root

# Update TensorFlow to the latest version (2.4.1 at the
# time of writing).
RUN pip install -U tensorflow

# Copies the trainer code to the docker image.
COPY trainer/config.py ./trainer/config.py
COPY trainer/data_loader.py ./trainer/data_loader.py
COPY trainer/model_utils.py ./trainer/model_utils.py
COPY trainer/model_training.py ./trainer/model_training.py
COPY trainer/task.py ./trainer/task.py

# Set up the entry point to invoke the trainer.
ENTRYPOINT ["python"]
CMD ["trainer/task.py"]</code></pre>
<p>After we have defined the <code>Dockerfile</code>, we can proceed to build it and do a round of model training by locally running it.</p>
</section>
<section id="building-and-locally-running-our-container" class="level1">
<h1>Building and locally running our container</h1>
<p>We will be using GCR to manage the lifecycle of our container. To build a Docker container, one must provide a correct Image URI (Uniform Resource Identifier) and it depends on the platform you are using for managing your container. In our case, that is GCR.</p>
<p>For GCR, the format of the image goes like the following: <code>gcr.io/${PROJECT_ID}/${IMAGE_REPO_NAME}:${IMAGE_TAG}</code>, where <code>PROJECT_ID</code> is the ID of your GCP project and <code>IMAGE_REPO_NAME</code> and <code>IMAGE_TAG</code> are identifiers.</p>
<p>We then build our image and locally run it:</p>
<pre class="bashell"><code>$ docker build -f Dockerfile -t ${IMAGE_URI} ./
$ docker run ${IMAGE_URI} \
    trainer/task.py --bucket ${BUCKET_NAME} \
    --train-pattern ${TRAIN_FILES} \
    --valid-pattern ${VALIDATION_FILES}</code></pre>
<p>To make the process cleaner, we can create a shell script and put all the instructions inside it. You can follow <a href="https://github.com/sayakpaul/Distributed-Training-in-TensorFlow-2-with-AI-Platform/blob/main/scripts/train_local.sh">this one</a> to get an idea.</p>
<p>The first time it’s run, it’s going to take a while. But after that, all the consequent runs will use the cached resources to speed up the build. The local Docker daemon (<code>dockerd</code>) will first read our <code>Dockerfile</code> and after getting to the entry point, it will parse all the command-line arguments we provided to <a href="https://github.com/sayakpaul/Distributed-Training-in-TensorFlow-2-with-AI-Platform/blob/main/trainer/task.py"><code>task.py</code></a>. <code>task.py</code> just takes all the command-line arguments and starts the model training. <code>TRAIN_FILES</code> and <code>VALIDATION_FILES</code> are patterns to the TFRecords residing inside a GCS Bucket and they look like so -</p>
<pre class="bashell"><code>TRAIN_FILES=gs://${BUCKET_NAME}/train_tfr/*.tfrec
VALIDATION_FILES=gs://${BUCKET_NAME}/validation_tfr/*.tfrec</code></pre>
<p>If everything goes well, then, after a while, you should be able to see that our model has started training:</p>
<img src="https://github.com/sayakpaul/portfolio/raw/master/posts/distributed_training/docker_build.png" class="img-fluid">
<center>
<b>Figure 2</b>: Docker build.
</center>
<img src="https://github.com/sayakpaul/portfolio/raw/master/posts/distributed_training/local_run.png" class="img-fluid">
<center>
<b>Figure 3</b>: Local training logs.
</center>
<p>The local Docker run is a way for us to ensure our code is running fine without any hiccups. So, it’s advisable to stop the local run after you have ensured the model is able to start training. With this, we are now ready to push our custom Docker image to GCR, and submit a training job to AI Platform.</p>
</section>
<section id="submitting-a-training-job" class="level1">
<h1>Submitting a training job</h1>
<p>For this step, we need to add two more lines of code: * After building our Docker image, we need to push it to GCR so that AI Platform can pull it to run model training. * Submit a training job to AI Platform.</p>
<p>So, let’s put these pieces together:</p>
<pre class="bashell"><code># Build and push the docker image
$ docker build -f Dockerfile -t ${IMAGE_URI} ./
$ docker push ${IMAGE_URI}

# Submit job
$ gcloud ai-platform jobs submit training ${JOB_NAME} \
    --region ${REGION} \
    --master-image-uri ${IMAGE_URI} \
    --config ./config.yaml \
    -- \
    trainer/task.py --bucket ${BUCKET_NAME} \
    --train-pattern ${TRAIN_FILES} \
    --valid-pattern ${VALIDATION_FILES}</code></pre>
<p>Reviewing what’s going on with the <code>gcloud</code> command, we have:</p>
<ul>
<li><p><code>region</code>, that informs AI Platform about the region to be used for the training process. This very region is also going to be used to provision resources such as GPUs. If GPUs are to be used then it’s important to pass a region that has that support. You can know the regions that have this support from <a href="https://cloud.google.com/ai-platform/training/docs/using-gpus#gpu-regions">here</a>.</p></li>
<li><p><code>master-image-uri</code> is the URI of our custom Docker image.</p></li>
<li><p>Via <code>config</code>, we provide a specification of the kind of machine we want to use for training. This specification is provided using a <a href="https://docs.ansible.com/ansible/latest/reference_appendices/YAMLSyntax.html">YAML</a> file and ours looks like so:</p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb7-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">trainingInput</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="cb7-2"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">scaleTier</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> CUSTOM</span></span>
<span id="cb7-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">  # Configure a master worker with 2 V100 GPUs</span></span>
<span id="cb7-4"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">masterType</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> n1-standard-8</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> # Specify the base machine type</span></span>
<span id="cb7-5"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">masterConfig</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="cb7-6"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">    </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">acceleratorConfig</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="cb7-7"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">      </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">count</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb7-8"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">      </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">type</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> NVIDIA_TESLA_V100</span></span></code></pre></div>
<p>The advantage of using specifications like this lies in the flexibility it provides. The <code>gcloud ai-platform jobs submit training</code> command has a <code>scale-tier</code> option through which we can <a href="https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training">pass a pre-defined machine configuration</a>. But let’s say we want to train using multiple machines - 1 master, 3 workers, and 3 parameter servers each having different GPU and CPU configurations. The pre-defined values won’t cut here and this is where we can take the advantage of custom specifications. You can check <a href="https://cloud.google.com/ai-platform/training/docs/using-gpus#gpu-enabled-machine-types">here</a> to know the different machine types and configurations that can be provided to AI Platform.</p></li>
</ul>
<p><br></p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>We are using V100 GPUs because they come with Tensor cores and that is a must-have to take the advantage of mixed-precision training. We could have used other GPUs like T4, A100 as well that fit this criterion.</p>
</div>
</div>
<p><br></p>
<p>We have already discussed the part that follows <code>config</code> so we will not be reviewing that here. If the job submission is successful you should see an entry for it on the <a href="https://console.cloud.google.com/ai-platform/jobs">GCP console</a>:</p>
<img src="https://github.com/sayakpaul/portfolio/raw/master/posts/distributed_training/job_list.png" class="img-fluid">
<center>
<b>Figure 4</b>: AI Platform training job list.
</center>
<p>On the extreme right, you would notice an option called <strong>View Logs</strong> that lets us monitor our training. It’s incredibly useful to have all of your training logs stored somewhere safe <em>without</em> making any effort. Logging for an AI Platform <code>training</code> job is managed by <a href="https://cloud.google.com/logging">Cloud Logging</a>. Here’s how mine looks like:</p>
<img src="https://github.com/sayakpaul/portfolio/raw/master/posts/distributed_training/logs_viewer.png" class="img-fluid">
<center>
<b>Figure 5</b>: Training logs. Notice the neat search filter query.
</center>
<p>After training is complete, we can verify if all the necessary artifacts were stored inside our GCS Bucket:</p>
<img src="https://github.com/sayakpaul/portfolio/raw/master/posts/distributed_training/gcs_files.png" class="img-fluid">
<center>
<b>Figure 6</b>: SavedModel file and TensorBoard logs.
</center>
<p>In our <a href="https://github.com/sayakpaul/Distributed-Training-in-TensorFlow-2-with-AI-Platform/blob/main/trainer/model_training.py">training script</a>, we had set up the TensorBoard callback to keep track of the training progress. You can check one such log <a href="https://tensorboard.dev/experiment/AWPrJesPSxyCX0GSmJMk1A/">here online on tensorboard.dev</a>. Inspecting into it, we can see that our model’s been trained well, as the validation accuracy has stabilized:</p>
<img src="https://github.com/sayakpaul/portfolio/raw/master/posts/distributed_training/tensorboard.png" class="img-fluid">
<center>
<b>Figure 7</b>: Accuracy plot.
</center>
<p>As an effective practitioner, It’s important to be aware of the costs and ensure maximization of resource utilization. Now that we were able to successfully complete our model training, let’s discuss these aspects in the next and final section of the post.</p>
</section>
<section id="delving-deep-into-training-costs-and-resource-utilization" class="level1">
<h1>Delving deep into training costs and resource utilization</h1>
<p>AI Platform provides a number of useful metrics for the <code>training</code> jobs. Each job has a separate dashboard that makes it super easy to keep track of its statistics such as total training time, average resource utilization, etc.</p>
<p>First, we have high-level information about the job:</p>
<img src="https://github.com/sayakpaul/portfolio/raw/master/posts/distributed_training/job_high_level.png" class="img-fluid">
<center>
<b>Figure 8</b>: High-level information about a training job.
</center>
<p>We can see that the job takes about 22 minutes to complete, and this includes the provisioning of resources, completing the model training, and de-provisioning the resources. We then see the total ML units consumed to run our job. The cost for this translates to:</p>
<p><strong>1.79</strong> (Consumed ML units) <img src="https://latex.codecogs.com/png.latex?%5Ctimes"> <strong>USD 0.49</strong> = <strong>USD 0.8771</strong></p>
<p>You can refer to <a href="https://cloud.google.com/ai-platform/training/pricing#ml-units">this document</a> that details the cost calculation scheme. GCP also provides a handy estimated cost calculator that you can find <a href="https://cloud.google.com/products/calculator">here</a>.</p>
<p>So far our costs are: <strong>USD 0.141</strong> (AI Platform Notebook) + <strong>USD 1.20 (GCS)</strong> + <strong>USD 0.8771</strong> (<code>training</code> job) = <strong>USD 2.2181</strong>. Let’s compare this to an AI Platform Notebook instance equipped with the similar configurations as the one we used for training:</p>
<img src="https://github.com/sayakpaul/portfolio/raw/master/posts/distributed_training/notebook_pricing.png" class="img-fluid">
<center>
<b>Figure 9</b>: Cost for an AI Platform Notebook with 2 V100 GPUs with n1-standard-8.
</center>
<p>Coming to CPU utilization, we have some room for improvement it seems:</p>
<img src="https://github.com/sayakpaul/portfolio/raw/master/posts/distributed_training/cpu_util.png" class="img-fluid">
<center>
<b>Figure 10</b>: CPU utilization of our training resources.
</center>
<p>The overall GPU utilization has a few spikes which might need some more inspections in the future:</p>
<img src="https://github.com/sayakpaul/portfolio/raw/master/posts/distributed_training/gpu_util.png" class="img-fluid">
<center>
<b>Figure 11</b>: GPU utilization of our training resources.
</center>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>We have covered quite a lot of ground in this post. I hope by now you have an idea of how to combine tools like Docker, AI Platform to manage your large-scale training workflows in a more cost-effective and scalable way. As a next step, you could take the trained model from AI Platform and deploy the model using it. AI Platform <a href="https://cloud.google.com/ai-platform/prediction/docs"><code>predict</code> jobs</a> make it easier to expose models via REST API-like services that are fully managed by AI Platform offering things like autoscaling, authorization, monitoring, etc. If you’d like to try it out yourself, I encourage you to check out the code of this post on <a href="https://github.com/sayakpaul/Distributed-Training-in-TensorFlow-2-with-AI-Platform">GitHub</a>. You are also welcome to checkout <a href="https://github.com/tensorflow/cloud">TensorFlow Cloud</a> that provides a set of tools making it easier to perform large-scale training with GCP.</p>
</section>
<section id="acknowledgements" class="level1">
<h1>Acknowledgements</h1>
<p>I am thankful to <a href="https://twitter.com/kweinmeister?lang=en">Karl Weinmeister</a> for his comments on the initial draft of this post. Also, thanks to the <a href="https://developers.google.com/community/experts">ML-GDE program</a> for providing generous GCP support without which I couldn’t have executed the experiments.</p>


</section>

 ]]></description>
  <category>tensorflow</category>
  <category>keras</category>
  <category>distributed-training</category>
  <category>ai-platform</category>
  <category>docker</category>
  <category>mlops</category>
  <guid>https://sayak.dev/posts/distributed-training.html</guid>
  <pubDate>Tue, 06 Apr 2021 00:00:00 GMT</pubDate>
  <media:content url="https://sayak.dev/posts/distributed_training.png" medium="image" type="image/png" height="113" width="144"/>
</item>
<item>
  <title>Building and Installing OpenCV 4.5.0 on an M1 Mac</title>
  <link>https://sayak.dev/posts/install-opencv-m1.html</link>
  <description><![CDATA[ 





<p>This post shows how to build and install OpenCV 4.5.0 on a MacBook Pro that comes with an <a href="https://www.apple.com/in/mac/m1/">M1 chip</a>. Yes, you guessed it right - as of <strong>January 01, 2021</strong>, there’s no pre-compiled OpenCV binary compatible with this MacBook Pro variant. So, open up a terminal and get started!</p>
<p>Here’s a brief summary of the configuration of my MacBook -</p>
<p><img src="https://i.ibb.co/XDZZQ4t/image.png" class="img-fluid"></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The following steps should run well on an M1 MacBook Air too.</p>
</div>
</div>
<section id="install-xcode-and-homebrew" class="level2">
<h2 class="anchored" data-anchor-id="install-xcode-and-homebrew">Install Xcode and Homebrew</h2>
<p>We start by executing <code>sudo xcodebuild -license</code> from a terminal.</p>
<p>When you execute the above command, you would need to accept the Xcode license. Then, in order to make use of Apple command line tools, we need to install it - <code>sudo xcode-select --install</code>.</p>
<p>Homebrew manages packages on a Mac. In order to install it execute the following - <code>/usr/bin/ruby -e "%(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"</code>.</p>
<p>You would want to add the command brew after the installation is complete. To do so, execute the following - <code>nano ~/.zshrc</code></p>
<p>Then insert <code>export PATH=$PATH:/opt/homebrew/bin</code> into it and press <code>Ctrl + X</code> from your keyboard. Then execute <code>source ~/.zshrc</code> from the terminal.</p>
<p>Note that the exact path to Homebrew might be different for your system, so please double check that.</p>
<p>Next up, we install a few system-level utilities required by OpenCV on a Mac.</p>
</section>
<section id="install-conda" class="level2">
<h2 class="anchored" data-anchor-id="install-conda">Install conda</h2>
<p>My favorite Python virtual environment manager is virtualenv. Unfortunately, it does not play out that well with the new M1 chip. This is mostly because the pip-installable packages often break during their installations on the chip. This is why conda, specifically its <strong>miniforge</strong> distribution is the recommended package manager for a Mac shipped with M1. You can install it from <a href="https://github.com/conda-forge/miniforge#miniforge3">here</a>. This installs <strong>Python 3.8</strong>.</p>
<p>After the installation is complete, please create a new Python virtual environment by executing <code>conda create --name &lt;environment_name&gt;</code>. Then activate it by running <code>conda activate  &lt;environment_name&gt;</code>.</p>
<p>Running <code>conda install -y python==3.8.6</code> will install a few common Python packages for you. I highly recommend running this.</p>
</section>
<section id="install-numpy" class="level2">
<h2 class="anchored" data-anchor-id="install-numpy">Install NumPy</h2>
<p>NumPy is needed by OpenCV. So, we need to install it before we build and install OpenCV. Apple provides a <code>numpy</code> wheel that is compatible with the M1 chip. Follow the steps below to install it -</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> wget https://github.com/apple/tensorflow_macos/releases/download/v0.1alpha0/tensorflow_macos-0.1alpha0.tar.gz</span>
<span id="cb1-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> tar xvf tensorflow_macos-0.1alpha0.tar.gz</span>
<span id="cb1-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> cd tensorflow_macos/arm64</span>
<span id="cb1-4"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> pip install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--upgrade</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--no-dependencies</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--force</span> numpy-1.18.5-cp38-cp38-macosx_11_0_arm64.whl </span>
<span id="cb1-5"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> cd ~</span></code></pre></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Be sure to activate your conda environment before doing the pip-install.</p>
</div>
</div>
</section>
<section id="compile-opencv" class="level2">
<h2 class="anchored" data-anchor-id="compile-opencv">Compile OpenCV</h2>
<p>First, let’s download the OpenCV and OpenCV extended module files and prepare them for compilation.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> wget <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-O</span> opencv.zip https://github.com/opencv/opencv/archive/4.5.0.zip</span>
<span id="cb2-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> wget <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-O</span> opencv_contrib.zip https://github.com/opencv/opencv_contrib/archive/4.5.0.zip</span>
<span id="cb2-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> unzip opencv.zip</span>
<span id="cb2-4"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> unzip opencv_contrib.zip</span>
<span id="cb2-5"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> cd opencv-4.5.0</span>
<span id="cb2-6"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> mkdir build <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">cd</span> build</span></code></pre></div>
<p>Now, we are all set to fire the <code>cmake</code> command that would build OpenCV for us. Let’s review it briefly -</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> cmake <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-2">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-DCMAKE_SYSTEM_PROCESSOR</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>arm64 <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-3">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-DCMAKE_OSX_ARCHITECTURES</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>arm64 <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-4">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-DWITH_OPENJPEG</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OFF <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-5">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-DWITH_IPP</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OFF <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-6">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> CMAKE_BUILD_TYPE=RELEASE <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-7">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> CMAKE_INSTALL_PREFIX=/usr/local <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-8">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> OPENCV_EXTRA_MODULES_PATH=/Users/sayakpaul/Downloads/opencv_contrib-4.5.0/modules <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-9">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> PYTHON3_EXECUTABLE=/Users/sayakpaul/miniforge3/envs/dev/bin/python3 <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-10">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> BUILD_opencv_python2=OFF <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-11">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> BUILD_opencv_python3=ON <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-12">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> INSTALL_PYTHON_EXAMPLES=ON <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-13">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> INSTALL_C_EXAMPLES=OFF <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-14">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> OPENCV_ENABLE_NONFREE=ON <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-15">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> BUILD_EXAMPLES=ON ..</span></code></pre></div>
<p>As per <a href="https://github.com/opencv/opencv/issues/18049#issuecomment-671878454">this issue comment</a>, <code>DCMAKE_SYSTEM_PROCESSOR</code>, <code>DCMAKE_OSX_ARCHITECTURES</code>, <code>DWITH_OPENJPEG</code>, and <code>DWITH_IPP</code> are needed to be set during the compilation step. Also, please pay attention to the following arguments - <code>OPENCV_EXTRA_MODULES_PATH</code> and <code>PYTHON3_EXECUTABLE</code>. For these two arguments, you would want to first determine the paths and then supply them accordingly.</p>
<p>Now, before you run the above <code>cmake</code> command, activate the conda environment you created in an earlier step (<code>conda activate &lt;environment_name&gt;</code>) if you haven’t already. The compilation took <em>~3 minutes</em> for me and it should produce outputs like so -</p>
<p><img src="https://i.ibb.co/YdpBSh0/image.png" class="img-fluid"></p>
<p>Next, we launch the make command - <code>make -j8</code>. With all the eight cores (<code>j8</code> stands for eight cores here) chugging along, this step took <em>~8 minutes</em> for me. You can adjust the <code>j</code> option with respect to the hardware available. After it’s done you should get an output like so -</p>
<p><img src="https://i.ibb.co/yFJq4jJ/image.png" class="img-fluid"></p>
<p>The final step here is to execute - <code>sudo make install</code>. It should take just a few seconds to complete execution. Upon successful completion, you should get an output like so -</p>
<p><img src="https://i.ibb.co/Pzzmxy4/image.png" class="img-fluid"></p>
</section>
<section id="sym-link-opencv-4-on-macos-to-virtual-environment-site-packages" class="level2">
<h2 class="anchored" data-anchor-id="sym-link-opencv-4-on-macos-to-virtual-environment-site-packages">Sym-link OpenCV 4 on macOS to virtual environment <code>site-packages</code></h2>
<p>To do this, we first need to locate the <code>.so</code> file generated during the compilation step. We can do this with the <code>mdfind</code> command -</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> mdfind cv2.cpython</span>
<span id="cb4-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">/.../opencv-4.5.0/build/lib/python3/cv2.cpython-38-darwin.so</span></span>
<span id="cb4-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">/usr/local/lib/python3.8/site-packages/cv2/python-3.8/cv2.cpython-38-darwin.so</span></span>
<span id="cb4-4"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">...</span></span></code></pre></div>
<p>Please note that I obfuscated some parts of the outputs for privacy reasons. In the above output, we can see the absolute locations for the <code>.so</code> files that were generated. Now, we need to execute the following to sym-link one of the <code>.so</code> files in our current Python virtual environment -</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> cd /.../miniforge3/envs/dev/lib/python3.8/site-packages</span>
<span id="cb5-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> ln <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-s</span> /usr/local/lib/python3.8/site-packages/cv2/python-3.8/cv2.cpython-38-darwin.so cv2.so</span></code></pre></div>
<p>Please double-check the paths before executing the commands.</p>
<p>And that’s it!</p>
<p>You can test the installation by executing the following -</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> conda activate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span>environment_name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">(</span><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">you</span> haven<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'t already)</span></span>
<span id="cb6-2"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">% python</span></span>
<span id="cb6-3"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">&gt;&gt;&gt; import cv2</span></span>
<span id="cb6-4"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">&gt;&gt;&gt; cv2.__version__</span></span></code></pre></div>
<p>It should print <code>'4.5.0'</code>.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://www.pyimagesearch.com/2018/08/17/install-opencv-4-on-macos/">Install OpenCV 4 on macOS</a></li>
<li><a href="https://towardsdatascience.com/tensorflow-2-4-on-apple-silicon-m1-installation-under-conda-environment-ba6de962b3b8">TensorFlow 2.4 on Apple Silicon M1 : installation under Conda environment</a></li>
</ul>


</section>

 ]]></description>
  <guid>https://sayak.dev/posts/install-opencv-m1.html</guid>
  <pubDate>Fri, 01 Jan 2021 00:00:00 GMT</pubDate>
  <media:content url="https://sayak.dev/posts/opencv_m1.png" medium="image" type="image/png" height="87" width="144"/>
</item>
<item>
  <title>A Battle of Text Detectors for Mobile Deployments: CRAFT vs. EAST</title>
  <dc:creator>Sayak Paul</dc:creator>
  <dc:creator>Tulasi Ram Laghumavarapu</dc:creator>
  <link>https://sayak.dev/posts/optimizing-text-detectors.html</link>
  <description><![CDATA[ 





<p>In the <a href="https://tulasi.dev/craft-in-tflite">previous post</a>, we saw how to convert the pre-trained <a href="https://arxiv.org/pdf/1904.01941">CRAFT</a> model from PyTorch to TensorFlow Lite (TFLite) and run inference with the converted TFLite model. In this post, we will be comparing the TFLite variants of the CRAFT model to another text detection model - <a href="https://arxiv.org/abs/1704.03155">EAST</a>. The objective of this post is to provide a comparative study between these two models with respect to various deployment-specific pointers such as inference latency, model size, performance on dense text regions, and so on. Text detection continues to be a very important use-case across many verticals. So we hope this post will serve as a systematic guide for developers that are interested to explore on-device text detection models.</p>
<p>Precisely, we will be comparing the two models on the basis of the following pointers which we think are very crucial when it comes to deploying them out in the wild -</p>
<ul>
<li>Visual Inspection of Performance</li>
<li>Model Size</li>
<li>Inference Latency</li>
<li>Memory Usage</li>
</ul>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you are interested to know about the conversion process and inference pipelines of the models, please refer to these notebooks - <a href="https://github.com/tulasiram58827/craft_tflite/tree/main/colabs">CRAFT</a> and <a href="https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/EAST_TFLite.ipynb">EAST</a>. The pre-converted models are available on TensorFlow Hub - <a href="https://tfhub.dev/tulasiram58827/lite-model/craft-text-detector/dr/1">CRAFT</a> and <a href="https://tfhub.dev/sayakpaul/lite-model/east-text-detector/dr/1">EAST</a>.</p>
</div>
</div>
<section id="benchmark-setup" class="level2">
<h2 class="anchored" data-anchor-id="benchmark-setup">Benchmark Setup</h2>
<p>We used the <a href="https://www.tensorflow.org/lite/performance/measurement">TensorFlow Lite Benchmark tool</a> in order to gather results on inference latency and memory usage of the models with <strong>Redmi K20 Pro</strong> as the target device. We chose a mobile device for this purpose because text detection is a pretty prevalent recipe of many mobile applications such as <a href="https://play.google.com/store/apps/details?id=com.google.ar.lens&amp;hl=en_IN&amp;gl=US">Google Lens</a>.</p>
<p>In order to make the comparisons fair, we consider the two models with three different image resolutions - 320x320, 640x416, and 1200x800. For each of these resolutions, we consider two different <a href="https://www.tensorflow.org/lite/performance/post_training_quantization">post-training quantization schemes</a> - dynamic-range and float16. <em>The CRAFT model conversion is not yet supported in the integer variant, hence we do not consider integer quantization (but the EAST model does support it)</em>.</p>
</section>
<section id="visual-inspection-of-performance" class="level2">
<h2 class="anchored" data-anchor-id="visual-inspection-of-performance">Visual Inspection of Performance</h2>
<p>In this setting, we run both of the models and their different variants (dynamic-range and float16 quantized) on a sample image that has dense text regions, and then we visualize the results. We observed that both of these models perform fairly well on images having lighter text regions. Here’s the sample image we used for the purpose -</p>
<p><img src="https://i.ibb.co/KVKnnct/image.png" class="img-fluid"></p>
<center>
<small>Image is taken from the <a href="https://rrc.cvc.uab.es/?ch=13">SROIE dataset</a>.</small><br>
</center>
<p>Time to detect some texts!</p>
<section id="craft---320x320-dynamic-range-float16" class="level3">
<h3 class="anchored" data-anchor-id="craft---320x320-dynamic-range-float16">CRAFT - 320x320 Dynamic-Range &amp; float16</h3>
<p>In the dynamic-range quantization setting, we can see the model misses out on some text blocks.</p>
<p><img src="https://i.ibb.co/RBX8XDn/image-w-593-h-442-rev-1-ac-1-parent-19-Qb-WABWc-E3n-SLPE6zqm-Tw6b-Vxkxee-YUw-Om-KTDn-Dz8k-Q.png" class="img-fluid"></p>
<center>
<small>Inference results from the 320x320 dynamic-range and float16 quantized CRAFT models.</small><br>
</center>
<p>With increased numerical precision i.e.&nbsp;float16, we can clearly see quite a bit of improvement in the results. It’s important to note that this improvement comes at the cost of increased model size.</p>
<p>Next up, we apply the same steps to the EAST model.</p>
</section>
<section id="east---320x320-dynamic-range-float16" class="level3">
<h3 class="anchored" data-anchor-id="east---320x320-dynamic-range-float16">EAST - 320x320 Dynamic-Range &amp; float16</h3>
<p>EAST apparently performs better than CRAFT under dynamic-range quantization. If we look closely, it appears that the CRAFT model produces far fewer overlaps in the detections compared to EAST. On the other hand, the EAST model is able to detect more text blocks. When developing practical applications with text detectors, it often becomes a classic case of <em>precision-recall</em> trade-offs like the one we are currently seeing. So, you would want to consider the application-specific needs in order to decide the level of trade-off to be achieved there.</p>
<p><img src="https://i.ibb.co/qsCMC5N/image-w-624-h-520-rev-37-ac-1-parent-19-Qb-WABWc-E3n-SLPE6zqm-Tw6b-Vxkxee-YUw-Om-KTDn-Dz8k-Q.png" class="img-fluid"></p>
<center>
<small>Inference results from the 320x320 dynamic-range and float16 quantized EAST models.</small><br>
</center>
<p>With increased precision, the above-mentioned points still hold, i.e.&nbsp;the number of overlaps being way higher for the EAST model than they are in the CRAFT equivalent. In this setting (float16 quantization), superiority in the performance of the CRAFT model is quite evident in regards to the EAST model.</p>
<p>As different applications may use different image resolutions we decided to test the performance of the models on larger dimensions as well. This is what we are going to see next.</p>
</section>
<section id="craft---640x416-dynamic-range-float16" class="level3">
<h3 class="anchored" data-anchor-id="craft---640x416-dynamic-range-float16">CRAFT - 640x416 Dynamic-Range &amp; float16</h3>
<p>On an increased resolution, the CRAFT model performs pretty well -</p>
<p><img src="https://i.ibb.co/VxbyWch/image-w-624-h-568-rev-38-ac-1-parent-19-Qb-WABWc-E3n-SLPE6zqm-Tw6b-Vxkxee-YUw-Om-KTDn-Dz8k-Q.png" class="img-fluid"></p>
<center>
<small>Inference results from the 640x416 dynamic-range and float16 quantized CRAFT models.</small><br>
</center>
<p>The float16 version of this resolution is a slam dunk (rightfully leaving behind the barcode which is not a piece of text).</p>
</section>
<section id="east---640x416-dynamic-range-float16" class="level3">
<h3 class="anchored" data-anchor-id="east---640x416-dynamic-range-float16">EAST - 640x416 Dynamic-Range &amp; float16</h3>
<p>The performance of the EAST model under these settings are very equivalent to CRAFT -</p>
<p><img src="https://i.ibb.co/ynBbrFZ/image-w-597-h-612-rev-36-ac-1-parent-19-Qb-WABWc-E3n-SLPE6zqm-Tw6b-Vxkxee-YUw-Om-KTDn-Dz8k-Q.png" class="img-fluid"></p>
<center>
<small>Inference results from the 640x416 dynamic-range and float16 quantized EAST models.</small><br>
</center>
<p>With float16 quantization and 640x416 as the resolution, the CRAFT model is a clear winner. Notice that the EAST model is still unable to discard the barcode part which might be an important point to note for some applications.</p>
<p>Time to inspect the results for our final and highest resolution - 1280x800.</p>
</section>
<section id="craft---1280x800-dynamic-range-float16" class="level3">
<h3 class="anchored" data-anchor-id="craft---1280x800-dynamic-range-float16">CRAFT - 1280x800 Dynamic-Range &amp; float16</h3>
<p>Under dynamic-range quantization, the results look okayish. The model misses out on a number of text blocks but the only ones that it detects appear to be neat.</p>
<p><img src="https://i.ibb.co/QMDpH9M/image-w-624-h-453-rev-34-ac-1-parent-19-Qb-WABWc-E3n-SLPE6zqm-Tw6b-Vxkxee-YUw-Om-KTDn-Dz8k-Q.png" class="img-fluid"></p>
<center>
<small>Inference results from the 1280x800 dynamic-range and float16 quantized CRAFT models.</small><br>
</center>
<p>The results from the float16 variant are tremendous (as you probably have guessed by now).</p>
</section>
<section id="east---1280x800-dynamic-range-float16" class="level3">
<h3 class="anchored" data-anchor-id="east---1280x800-dynamic-range-float16">EAST - 1280x800 Dynamic-Range &amp; float16</h3>
<p>At this resolution, the EAST model seems to be performing well too -</p>
<p><img src="https://i.ibb.co/xYHfXXn/image-w-624-h-483-rev-29-ac-1-parent-19-Qb-WABWc-E3n-SLPE6zqm-Tw6b-Vxkxee-YUw-Om-KTDn-Dz8k-Q.png" class="img-fluid"></p>
<center>
<small>Inference results from the 1280x800 dynamic-range and float16 quantized EAST models.</small><br>
</center>
<p>With float16 quantization as well, the CRAFT model beats EAST in terms of the detection quality.</p>
</section>
</section>
<section id="model-size" class="level2">
<h2 class="anchored" data-anchor-id="model-size">Model Size</h2>
<p>When it comes to deploying models to mobile devices model size becomes a really important factor. You may not want to have a heavy model that would, in turn, make your mobile application bulky. Moreover, <a href="https://support.google.com/googleplay/android-developer/answer/113469#apk">Playstore</a> and <a href="https://developer.apple.com/forums/thread/12455">AppStore</a> also have size restrictions on the applications one can host there.</p>
<p>On the other hand, heavier models tend to be slower. If your application cannot have increased inference latency then you would want to have the model size as low as possible.</p>
<p>The following figure shows the size of the CRAFT and EAST models -</p>
<p><img src="https://i.ibb.co/tX7bknk/nyrm-wh-z-itikr9-cnyl6-z1-fq3.png" class="img-fluid"></p>
<center>
<small>Model (TFLite variants) sizes of CRAFT and EAST.</small><br>
</center>
<p>The dynamic-range quantized versions of both the models are in a well-acceptable range with respect to size. However, the float16 variants may still be a bit heavier for some applications.</p>
</section>
<section id="inference-latency" class="level2">
<h2 class="anchored" data-anchor-id="inference-latency">Inference Latency</h2>
<p>Inference latency is also one of the major factors for mobile-based deployments especially when your applications might require instantaneous predictions. We are going to show a comparison between all the settings we considered in the visual inspection section.</p>
<p>To reiterate we performed the benchmarks for this section on a Redmi K20 Pro using 4 threads. In the following figures, we present inference latency of different variants of the CRAFT and EAST models.</p>
<p><img src="https://i.ibb.co/1GyPgR6/ylz3-vh2l-ownf4av-amai-w0j-oz.png" class="img-fluid"></p>
<center>
<small>Inference latency of different variants of the CRAFT model.</small><br>
</center>
<p><img src="https://i.ibb.co/ySBsQvs/z-q-o-zf7cl-hu-tfh-ou-a7-yscgm.png" class="img-fluid"></p>
<center>
<small>Inference latency of different variants of the EAST model.</small><br>
</center>
<p>As expected, with increased resolution the inference latency also increases. Inference latency is also quite lower for all the variants of the EAST model compared to CRAFT. Earlier we saw how a quantization affects model performance under a particular resolution. As stated earlier, when using these models inside a mobile application, the “<em>Size vs.&nbsp;Performance</em>” trade-off becomes extremely vital.</p>
<blockquote class="blockquote">
<p>important: The results for the float16 1280x800 CRAFT model could not be obtained on our target device.</p>
</blockquote>
</section>
<section id="memory-usage" class="level2">
<h2 class="anchored" data-anchor-id="memory-usage">Memory Usage</h2>
<p>In section, we shed light on the total memory allocated for the models while running the TensorFlow Lite Benchmark tool. Knowing about the memory usage of these models helps us plan application releases accordingly as not all the mobile phones may support extensive memory requirements. So based on this information, you may want to set some device requirements for your application using these models. On the other hand, if you would want your application to be as device-agnostic as possible then you may want to maintain separate models according to their size and memory usage.</p>
<p>In this case, also, we are going to consider all the settings we had considered in the previous sections. The following figures give us a sense of the memory footprint left behind by the models -</p>
<p><img src="https://i.ibb.co/TrnZ9vX/webp-net-resizeimage.png" class="img-fluid"></p>
<center>
<small>Memory footprint of different variants of the CRAFT model.</small><br>
</center>
<p><img src="https://i.ibb.co/3szkpK0/hfp-jmc4-nej-lloj-bc2-q-nz515y.png" class="img-fluid"></p>
<center>
<small>Memory footprint of different variants of the EAST model.</small><br>
</center>
<p>Detection performance-wise, CRAFT was a winner in many cases but if we factor in for inference latency and memory footprint the situation might need reconsideration. In other words, the best performing (with respect to a certain task, detection in this case) model may not always be the best candidate for deployments.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>The results for the float16 1280x800 CRAFT model could not be obtained on our target device.</p>
</div>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In this post, we presented a comparative study between two text detection models - CRAFT and EAST. We went beyond their task-specific performance and considered various essential factors that one needs to consider when deploying these models. At this point, you might have felt the need to consider another important factor of these models - <em>FPS information of the models on real-time videos</em>. Please check out <a href="https://github.com/farmaker47/OCR_with_Keras">this repository</a> to get a handle on how to approach that development.</p>
</section>
<section id="contribution" class="level2">
<h2 class="anchored" data-anchor-id="contribution">Contribution</h2>
<p><a href="https://www.linkedin.com/in/tulasi-ram-laghumavarapu-aba672103/">Tulasi</a> worked on the CRAFT model while Sayak worked on the EAST model. For the purpose of this post, Tulasi focused on gathering all the relevant information for doing the comparisons while Sayak focused on the writing part.</p>
<p>Thanks to <a href="https://twitter.com/khanhlvg">Khanh LeViet</a> from the TFLite team for reviewing the post.</p>


</section>

 ]]></description>
  <guid>https://sayak.dev/posts/optimizing-text-detectors.html</guid>
  <pubDate>Fri, 27 Nov 2020 00:00:00 GMT</pubDate>
  <media:content url="https://sayak.dev/posts/text_detector_benchmark.png" medium="image" type="image/png" height="131" width="144"/>
</item>
<item>
  <title>Optimizing MobileDet for Mobile Deployments</title>
  <link>https://sayak.dev/posts/mobiledet-optimization.html</link>
  <description><![CDATA[ 





<p>This year researchers from the University of Wisconsin-Madison and Google published their work on <a href="https://arxiv.org/abs/2004.14525">MobileDet</a>. MobileDet presents an architectural philosophy for designing object detectors specifically targeted toward running on mobile accelerators like DSP, EdgeTPU, and so on. MobileDet yields significant improvement over architectures MobileNetV2+SSDLite and MobileNetV3+SSDLite on the <a href="https://cocodataset.org/">COCO object detection task</a> with the same accelerated inference time. Long story cut short, if you are planning to use object detection models in mobile applications MobileDets may be an extremely good choice.</p>
<p>One fantastic thing about modern-day research is most of the time, the code and essential artifacts (like the trained models) are available publicly. MobileDet is no exception; the authors released their code and pre-trained models in <a href="https://github.com/tensorflow/models/tree/master/research/object_detection">TensorFlow Object Detection (TFOD) API</a>. The model files come in three different variants -</p>
<ul>
<li>Optimized for mobile CPU</li>
<li>Optimized for EdgeTPU</li>
<li>Optimized for DSP</li>
</ul>
<p>Each of these variants includes the pre-trained checkpoints, a TensorFlow Lite (TFLite) compatible model graph, a TFLite model file, a configuration file, and a graph proto. The models were pre-trained on the COCO dataset.</p>
<p>In this post, I am going to be revisiting the TFLite conversion from the pre-trained model checkpoints along with some of the non-trivial things that come up during the process. It is basically an extension of <a href="https://twitter.com/khanhlvg?lang=en">Khanh LeViet</a> and my findings we shared over <a href="https://github.com/ml-gde/e2e-tflite-tutorials/issues/21">this GitHub thread</a>.</p>
<p>The code discussed throughout this post is available <a href="https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/MobileDet_Conversion_TFLite.ipynb">here as a Colab Notebook</a>.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you want to train MobileDet models on your own dataset you may find <a href="https://github.com/sayakpaul/E2E-Object-Detection-in-TFLite/tree/master/colab_training">these notebooks</a> useful. They show you how to prepare the dataset, fine-tune a MobileDet model with the dataset, and optimize the fine-tuned model with TFLite.</p>
</div>
</div>
<section id="why-yet-another-post-on-model-conversion" class="level2">
<h2 class="anchored" data-anchor-id="why-yet-another-post-on-model-conversion">Why yet another post on model conversion?</h2>
<p>Fair question. After all, there are so many great examples and tutorials that show how to use the <a href="https://www.tensorflow.org/lite/performance/post_training_quantization"><u>post-training quantization APIs</u></a> in TFLite to perform the model conversion. MobileDet models in the TFOD API repository were trained in TensorFlow (TF) 1. If you ever wanted to use the latest TFLite converter to do the conversion, that may not be immediately approachable.</p>
<p>Besides, there are certain caveats to the EdgeTPU and DSP variants. They come in two precision formats - <code>uint8</code> and <code>float32</code>. The models in <code>uint8</code> precision were trained using <a href="https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html"><u>quantization aware training</u></a> (QAT) while the <code>float32</code> models were not. During QAT fake quantization nodes get inserted into a model’s computation graph. So, the models trained using QAT usually require some extra care during the TFLite conversion process as we’ll see in a moment.</p>
<p>If we wanted to convert a single shot detector (SSD) based model to TFLite then we first need to generate a frozen graph first that is compatible with the TFLite operator set (as per these guides - <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md"><u>TF1</u></a> and <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md"><u>TF2</u></a>). The TFOD API team provides stock scripts (<a href="https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_ssd_graph.py"><u>TF1</u></a>, <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_graph_tf2.py"><u>TF2</u></a>) for this. Both of these scripts add optimized postprocessing operations to the model graph. Now, these operations are not yet supported in int8 precision. So, if you ever wanted to convert these pre-trained checkpoints using <a href="https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization"><u>full integer quantization</u></a>, what would have been your approach?</p>
<p>By now, hopefully, I have been able to convince you that this post is not just about regular model conversion in TFLite. The situations we’ll be going through over the next sections may be helpful for your production TFLite models as well.</p>
</section>
<section id="the-hassle-free-conversions" class="level2">
<h2 class="anchored" data-anchor-id="the-hassle-free-conversions">The hassle-free conversions</h2>
<p>Before we build our way toward the fun stuff, let’s start with the conversions that won’t cost us a night’s sleep. Conversions based on <a href="https://www.tensorflow.org/lite/performance/post_training_quant"><u>dynamic-range</u></a> and <a href="https://www.tensorflow.org/lite/performance/post_training_float16_quant">float16</a> quantization would come under this category.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>The EdgeTPU and DSP variants of MobileDet are meant to run on the respective hardware accelerators. These accelerators need a model to be in full integer precision. So converting the EdgeTPU and DSP variants with dynamic-range and <code>float16</code> quantization does not have any practical usage.</p>
</div>
</div>
<p>So, for dynamic-range and <code>float16</code> quantization based conversions, we will be using the CPU variant only. This variant is available <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md#mobile-models">here</a> as <code>ssd_mobiledet_cpu_coco</code>. Once the model bundle is untar’d we get the following files -</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">├──</span> model.ckpt-400000.data-00000-of-00001</span>
<span id="cb1-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">├──</span> model.ckpt-400000.index</span>
<span id="cb1-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">├──</span> model.ckpt-400000.meta</span>
<span id="cb1-4"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">├──</span> model.tflite</span>
<span id="cb1-5"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">├──</span> pipeline.config</span>
<span id="cb1-6"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">├──</span> tflite_graph.pb</span>
<span id="cb1-7"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">└──</span> tflite_graph.pbtxt</span></code></pre></div>
<p><code>model.ckpt-*</code> files are the pre-trained checkpoints on the COCO dataset. If you train a MobileDet object detection model on your own dataset, you will have your own model checkpoint files. The <code>tflite_graph.pb</code> file is a frozen inference graph that is compatible with the TFLite operator set, which was exported from the pre-trained model checkpoints. <code>model.tflite</code> file is a TFLite model that was converted from the <code>tflite_graph.pb</code> frozen graph.</p>
<p>In case if you ever train a MobileDet model on your dataset, here’s how you’d get the TFLite frozen graph file (based on <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md"><u>this guide</u></a> mentioned above) -</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> PIPELINE_CONFIG=<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"checkpoint_name/pipeline.config"</span></span>
<span id="cb2-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> CKPT_PREFIX=<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"checkpoint_name/model.ckpt-400000"</span></span>
<span id="cb2-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> OUTPUT_DIR=<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"tflite_graph"</span></span>
<span id="cb2-4"> </span>
<span id="cb2-5"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> python models/research/object_detection/export_tflite_ssd_graph.py <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb2-6">   <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--pipeline_config_path</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$PIPELINE_CONFIG</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb2-7">   <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--trained_checkpoint_prefix</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$CKPT_PREFIX</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb2-8">   <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--output_directory</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$OUTPUT_DIR</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb2-9">   <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--add_postprocessing_op</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>true</span></code></pre></div>
<p>You can see a fully worked out example in the <a href="https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/MobileDet_Conversion_TFLite.ipynb"><u>Colab Notebook</u></a> mentioned above. If everything goes well, then you should have the frozen graph file exported in <code>OUTPUT_DIR</code>. Let’s now proceed to the TFLite model conversion part.</p>
<p>Here’s how the dynamic-range quantization would look like in TensorFlow 2 -</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">converter <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(</span>
<span id="cb3-2">    graph_def_file<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model_to_be_quantized,</span>
<span id="cb3-3">    input_arrays<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'normalized_input_image_tensor'</span>],              </span>
<span id="cb3-4">    output_arrays<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess'</span>,</span>
<span id="cb3-5">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess:1'</span>,</span>
<span id="cb3-6">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess:2'</span>,</span>
<span id="cb3-7">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess:3'</span>],</span>
<span id="cb3-8">   input_shapes<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'normalized_input_image_tensor'</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">320</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">320</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>]}</span>
<span id="cb3-9">)</span>
<span id="cb3-10">converter.optimizations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [tf.lite.Optimize.DEFAULT]</span>
<span id="cb3-11">tflite_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> converter.convert()</span></code></pre></div>
<p>A note about some of the parameters and their values from the above code listing -</p>
<ul>
<li><p><code>model_to_be_quantized</code> corresponds to the frozen graph file.</p></li>
<li><p><code>input_arrays</code> and <code>input_shapes</code> are set accordingly with respect to the frozen graph file. As we can see in the figure below that these values have been set correctly.</p>
<p><img src="https://i.ibb.co/F4xGRJB/image2.png" class="img-fluid"></p></li>
<li><p><code>output_arrays</code> is set according to the instructions provided in <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_ssd_graph.py"><u>this guide</u></a>. Those operations represent four arrays: <code>detection_boxes</code>, <code>detection_classes</code>, <code>detection_scores</code>, and <code>num_detections</code>, usually a mandate for any object detector out there.</p></li>
</ul>
<p>The rest of the parts in the code listing should be familiar to you if you already know about the typical post-training quantization process in TFLite. For <code>float16</code> quantization, all the things would remain the same; we just need to add this line before calling <code>convert()</code> - <code>converter.target_spec.supported_types = [tf.float16]</code>.</p>
<p>The dynamic-range quantized model is <strong>4.3 MB</strong> in size and <code>float16</code> one is <strong>8.2 MB</strong>. Later, we will see how fast this model would run on actual mobile devices with and without different accelerators.</p>
</section>
<section id="the-trickier-tflite-conversions-for-mobiledet" class="level2">
<h2 class="anchored" data-anchor-id="the-trickier-tflite-conversions-for-mobiledet">The trickier TFLite conversions for MobileDet</h2>
<p>In this section, we will be dealing with the full integer quantization for the three different variants of MobileDet. Full integer quantization is usually more involved than the other quantization formats supported by TFLite.</p>
<section id="representative-dataset" class="level3">
<h3 class="anchored" data-anchor-id="representative-dataset">Representative dataset</h3>
<p>Our first step toward doing full integer quantization is preparing a representative dataset. It is required to calibrate the activation ranges so that the quantized model is able to retain the original model performance as much as possible. For the purpose of this post, I sampled 100 images from the <a href="https://cocodataset.org/#download"><u>COCO training dataset</u></a> (<code>train2014</code> split). In my experience, 100 samples as the representative dataset have always been sufficient. I have hosted these images <a href="https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite/releases/tag/v0.9.0"><u>here</u></a> in case you are interested to use them.</p>
<p>The following code listing denotes a generator function that produces a preprocessed image to the TFLite converter -</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">rep_ds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.data.Dataset.list_files(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train_samples/*.jpg"</span>)</span>
<span id="cb4-2">HEIGHT, WIDTH <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">320</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">320</span></span>
<span id="cb4-3"> </span>
<span id="cb4-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> representative_dataset_gen():</span>
<span id="cb4-5">   <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> image_path <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> rep_ds:</span>
<span id="cb4-6">       img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.io.read_file(image_path)</span>
<span id="cb4-7">       img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.io.decode_image(img, channels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb4-8">       img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.convert_image_dtype(img, tf.float32)</span>
<span id="cb4-9">       resized_img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.resize(img, (HEIGHT, WIDTH))</span>
<span id="cb4-10">       resized_img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> resized_img[tf.newaxis, :]</span>
<span id="cb4-11">       <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">yield</span> [resized_img]</span></code></pre></div>
<p><strong>Note</strong> that these preprocessing steps should be in sync with the actual preprocessing steps that would apply before running inference with your TFLite model. In case if you are interested to know about more complex representative dataset generators you may find <a href="https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/Magenta_arbitrary_style_transfer_model_conversion.ipynb"><u>this notebook</u></a> useful.</p>
<p>Also, note that dynamic-range and <code>float16</code> quantization of the EdgeTPU and DSP variants don’t have much of practical usage. The next section is going to be solely about full integer quantization of these different variants and the nitty-gritty to take into consideration for the conversion process.</p>
</section>
<section id="dealing-with-fake-quantization-nodes-during-conversion" class="level3">
<h3 class="anchored" data-anchor-id="dealing-with-fake-quantization-nodes-during-conversion">Dealing with fake quantization nodes during conversion</h3>
<p>The figure below represents a portion of the <code>uint8</code> EdgeTPU model computation graph. The nodes highlighted in red are inserted by the QAT mechanism. You would notice the same kind of nodes in the <code>uint8</code> DSP model computation graph as well.</p>
<p><img src="https://i.ibb.co/B2qXzsf/image1.png" class="img-fluid"></p>
<p>Now, these nodes have some important implications that we need to consider during the conversion process -</p>
<ul>
<li>During QAT the activation ranges are already approximated i.e.&nbsp;QAT resembles post-training quantization during training and adjusts the activation ranges accordingly. So, we don’t need to provide a representative dataset for a full integer quantization based conversion.</li>
<li>These fake nodes are generally in integer precision. So, setting an optimization option (<code>converter.optimizations</code>) might lead to inconsistencies.</li>
<li>In order to convert the <code>uint8</code> models with full integer quantization, we need to set the input and output data type of the TFLite models to integer precision (typically <code>uint8</code> or <code>int8</code>). As per <a href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/lite/TFLiteConverter#attributes"><u>this documentation</u></a>, we also need to specify the <code>quantized_input_stats</code> parameter during the conversion process. This is needed in order for the converted TFLite model to map the quantized input values to real values. More details are available <a href="https://www.tensorflow.org/lite/performance/quantization_spec"><u>here</u></a>.</li>
</ul>
<p>So, how do we realize all of these in code?</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">converter <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(</span>
<span id="cb5-2">   graph_def_file<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model_to_be_quantized,</span>
<span id="cb5-3">   input_arrays<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'normalized_input_image_tensor'</span>],</span>
<span id="cb5-4">   output_arrays<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess'</span>,</span>
<span id="cb5-5">       <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess:1'</span>,</span>
<span id="cb5-6">       <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess:2'</span>,</span>
<span id="cb5-7">       <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess:3'</span>],</span>
<span id="cb5-8">   input_shapes<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'normalized_input_image_tensor'</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">320</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">320</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>]}</span>
<span id="cb5-9">)</span>
<span id="cb5-10">converter.inference_input_type <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.uint8</span>
<span id="cb5-11">converter.quantized_input_stats <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"normalized_input_image_tensor"</span>: (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>)}</span>
<span id="cb5-12">tflite_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> converter.convert()</span></code></pre></div>
<p>If you’re thinking this does not look all that gory compared to the above code listing - it does not have to be! The tooling should help you do these things seamlessly. But catching these details during your project development may not be trivial. Note that we don’t specify <code>converter.inference_output_type</code>. Hold your breath, we will come to this in a moment.</p>
<p>After successful execution, we get two full integer quantized models - EdgeTPU one is <strong>4.2 MB</strong> and the DSP one is <strong>7.0 MB</strong>.</p>
</section>
<section id="integer-quantization-for-cpu-variants-and-float32-precision-models" class="level3">
<h3 class="anchored" data-anchor-id="integer-quantization-for-cpu-variants-and-float32-precision-models">Integer quantization for CPU variants and float32 precision models</h3>
<p>The variants that don’t contain fake quantization nodes (CPU and all the models in <code>float32</code> precision) have a <em>relatively</em> simpler conversion process. Recollect that the EdgeTPU and DSP variants come in two different precisions - <code>uint8</code> and <code>float32</code>. For example, here’s how it would be for the <code>float32</code> precision models -</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">converter.representative_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> representative_dataset_gen</span>
<span id="cb6-2">converter.inference_input_type <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.uint8</span>
<span id="cb6-3">converter.optimizations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [tf.lite.Optimize.DEFAULT]</span></code></pre></div>
<p>Note that we are specifying a representative dataset here because the <code>float32</code> precision models weren’t trained using QAT. For the CPU variant model, the lines of code would slightly change -</p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">converter.inference_input_type <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.uint8</span>
<span id="cb7-2">converter.quantized_input_stats <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"normalized_input_image_tensor"</span>: (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>)}</span>
<span id="cb7-3">converter.optimizations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [tf.lite.Optimize.DEFAULT]</span></code></pre></div>
<p>Honestly, I found this configuration by trial and error. I observed that if I specify a representative dataset then it hurts the predictions of the converted model. Also, I found out that specifying <code>converter.quantized_input_stats</code> helped improve the predictions of the converted model.</p>
<p>We don’t specify <code>converter.inference_output_type</code> in this case as well. Let’s get to it now.</p>
</section>
<section id="dealing-with-non-integer-postprocessing-ops-during-conversion" class="level3">
<h3 class="anchored" data-anchor-id="dealing-with-non-integer-postprocessing-ops-during-conversion">Dealing with non-integer postprocessing ops during conversion</h3>
<p>Remember that frozen graph exporter scripts provided by the TFOD API team add optimized postprocessing operations to the graph. These operations are not supported in integer precision yet. So, even if you wanted to specify <code>converter.inference_output_type</code> as <code>tf.uint8</code> you’ll likely get the following error -</p>
<pre><code>RuntimeError: Unsupported output type UINT8 for output tensor 'TFLite_Detection_PostProcess' of type FLOAT32.</code></pre>
<p>This is why we did not set the <code>converter.inference_output_type</code> parameter.</p>
<p>This should resolve all the problems you may run into if you ever wanted to convert the MobileDet models offered by the TFOD API team. In the last two sections, we’ll see these converted models in action and how fast they can perform on respective hardware accelerators.</p>
</section>
</section>
<section id="show-me-some-results" class="level2">
<h2 class="anchored" data-anchor-id="show-me-some-results">Show me some results</h2>
<p>For the CPU variant model, its <code>float16</code> quantized TFLite provided decent results -</p>
<p><img src="https://i.ibb.co/k6c93CC/image3.png" class="img-fluid"></p>
<p>On Colab, the inference time is about <strong>92.36 ms</strong> for this particular model. I experimented with different threshold values for filtering out the weak predictions and a threshold of <strong>0.3</strong> yielded the best results. These results are pretty consistent across the several different models we talked about.</p>
<p>A major point to note here for the EdgeTPU and DSP variants, their converted counterparts would be much slower on Colab since they were specifically optimized for different hardware accelerators.</p>
<p>You are encouraged to play with the different converted models using the Colab Notebook mentioned above and see these results for yourself.</p>
</section>
<section id="model-benchmarks" class="level2">
<h2 class="anchored" data-anchor-id="model-benchmarks">Model benchmarks</h2>
<p>In this section, we’ll address the question - “So, how do I choose one among these many models?” Well, you could manually try them all out and see which performs the best on the runtime of your choice. But a more practical approach to this would be to first benchmark these models on a set of devices using the <a href="https://www.tensorflow.org/lite/performance/measurement"><u>TFLite Benchmark Tool</u></a> and then decide accordingly.</p>
<p>The following table provides a comprehensive summary of the important statistics about the runtime of different TFLite MobileDet models. These results were generated using the TFLite Benchmark Tool mentioned above.</p>
<img src="https://i.ibb.co/jrKshwB/image.png" class="img-fluid">
<center>
<small>* Device used - Pixel 4 (Inference timings are reported in milliseconds)</small><br> <small>** As reported <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md">here</a></small>
</center>
<p>We can see that with the proper hardware accelerators, the DSP EdgeTPU variants can really shine. For the CPU variant, on a GPU accelerated runtime the <code>float16</code> quantized TFLite model can bring in additional speed boosts.</p>
<p>A catch here is Pixel devices don’t allow third-party applications to use the Hexagon DSP therefore even if we instruct the Benchmark Tool to make use of that the model would fall back to the CPU for execution. This is why for fair benchmarking results for the DSP variants we should consider running the Benchmark Tool on a device (such as Samsung Galaxy S9+) that has Hexagon DSP and also allows third-party applications to use it.</p>
<img src="https://i.ibb.co/mHkyfpd/image.png" class="img-fluid">
<center>
<small>* Device used - Samsung Galaxy S9+ (Inference timings are reported in milliseconds)</small>
</center>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>To train a custom MobileDet-based object detector you can refer to <a href="https://github.com/sayakpaul/E2E-Object-Detection-in-TFLite/tree/master/colab_training">these notebooks</a>.</p>
</div>
</div>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>In this post, we discussed some of the intricate problems one may run into while converting different variants of the MobileDet model in TFLite. One aspect about TFLite that I really like is how it provides the tooling needed to deal with practical problems like this.</p>
<p>I am thankful to Khanh for thoroughly guiding me while writing this post. Thanks to <a href="https://sg.linkedin.com/in/martinandrews">Martin Andrews</a> for suggesting textual edits.</p>


</section>

 ]]></description>
  <category>tflite</category>
  <category>model-optimization</category>
  <category>mobiledet</category>
  <guid>https://sayak.dev/posts/mobiledet-optimization.html</guid>
  <pubDate>Tue, 29 Sep 2020 00:00:00 GMT</pubDate>
  <media:content url="https://sayak.dev/posts/benchmark.png" medium="image" type="image/png" height="62" width="144"/>
</item>
<item>
  <title>The Maker Philosophy with ML APIs</title>
  <link>https://sayak.dev/posts/mlapis-maker.html</link>
  <description><![CDATA[ 





<p>In this post, I discuss how I used several Google Cloud Platform (GCP) APIs to turn two ideas into small prototypes. It includes my thought process, the problems I ran into while developing the prototypes, and my approach toward tackling them. All the code discussed in the post is available in <a href="https://github.com/sayakpaul/GCP-ML-API-Demos"><u>this repository</u></a>.</p>
<p>As a Machine Learning (ML) Practitioner, I advocate for having an understanding of the underlying principles of the models and other stuff that I use. This understanding has many extents. Sometimes, it involves minimally implementing models, and sometimes it may not involve the from-scratch implementation. When it does not involve the implementation part and when the model is readily available, I prefer to put such models directly to use and get a sense of their broader capabilities.</p>
<p>With libraries like TensorFlow, PyTorch, and Scikit-Learn, realizing this usage has never been easier. As all of these libraries are open-source, you could easily get access to the low-level primitives of their model APIs whenever you’d like. It may require you to have a sufficient amount of experience with the library you’d use. But as a Machine Learning Practitioner, one cannot skip this practice. It’s important to have a good grip over a particular Machine Learning library given the domain of choice (structured tabular dataset, images, texts, audios, for example).</p>
<p>On the other hand, APIs that offer ML as a service, allow non-ML folks to incorporate the power of Machine Learning in their applications very easily. This way developers can prototype ideas faster than ever. Some would argue that <a href="https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/"><u>leaky abstractions</u></a> can hit sooner than expected and it can be particularly <a href="https://karpathy.github.io/2019/04/25/recipe/"><u>very miserable in Machine Learning</u></a>. Nonetheless, if you are more on the applied side of things and don’t want to worry about this aspect, that’s perfectly fine.</p>
<p>I wanted to revisit this idea through the lens of an ML Practitioner. More precisely, I wanted to build a series of short demos utilizing the <a href="https://cloud.google.com/products/ai"><u>Cloud ML APIs offered by Google Cloud Platform</u></a>. The premise here is if I have an idea for an ML project, I wanted to see how quickly I can develop a <a href="https://en.wikipedia.org/wiki/Proof_of_concept"><u>PoC</u></a> around it.</p>
<section id="the-ideation-phase" class="level1">
<h1>The ideation phase</h1>
<p>Let me quote <strong>Emil Wallner</strong> from <a href="https://blog.floydhub.com/emils-story-as-a-self-taught-ai-researcher/"><u>this interview</u></a> -</p>
<blockquote class="blockquote">
<p><em>It’s important to collect objective evidence that you can apply machine learning.</em></p>
</blockquote>
<p>With regard to successful ML practice, this statement couldn’t have been more appropriate. Machine Learning has affected almost every industry in some way, it has changed the way we develop and perceive software. Coming up with an ML application idea that’s not already there or implemented is actually pretty hard.</p>
<p>So, I ideated the prototypes drawing inspiration from what is already available. For example, <a href="https://twitter.com/dalequark"><u>Dale</u></a> and <a href="https://twitter.com/kazunori_279"><u>Kaz</u></a> of Google built <a href="https://www.linkedin.com/feed/update/urn:li:activity:6706930343590584320/"><u>this uber-cool demo</u></a> that lets you transform a PDF into an audiobook. I really wanted to build something similar but in a more minimal capacity – something that could solely run on a Colab Notebook.</p>
<p>I decided to revisit some of the GCP ML APIs that I already knew, <a href="https://cloud.google.com/vision"><u>Vision</u></a>, <a href="https://cloud.google.com/text-to-speech"><u>Text-to-Speech</u></a> APIs, for example. As someone that is already working in the field of Computer Vision, I was inclined to do something that involves it. So here are some initial ideas that came to mind after spending a considerable amount of time with the different API documentation available on GCP:</p>
<ul>
<li><p>A pipeline that takes a short video clip, detects the entities present in the video and generates an audio clip dictating detected entity labels. This allowed me to spend some time with GCP’s <a href="https://cloud.google.com/video-intelligence"><u>Video Intelligence API</u></a>.</p></li>
<li><p>A pipeline that takes an <a href="https://arxiv.org/"><u>arXiv paper</u></a> and generates an audio clip of the paper abstract. This was inspired by the demo that Dale and Kaz had already built.</p></li>
</ul>
<p>Note that if you are already experienced with the Vision and Text-to-Speech APIs then these may seem very trivial.</p>
</section>
<section id="the-mental-model" class="level1">
<h1>The mental model</h1>
<p>After these ideas, I designed a bunch of visual workflows demonstrating the steps required to realize these ideas along with the right tooling. Here’s an example -</p>
<p><img src="https://i.ibb.co/64Kw8wW/workflow-1.png" class="img-fluid"></p>
<p>I also like to refer to these workflows as <em>mental models</em>. Additionally, it helps me to figure out the major dependencies and steps that may be required for the work so that I can plan accordingly. I discuss the importance of developing mental models in <a href="https://blog.floydhub.com/structuring-and-planning-your-machine-learning-project/#building-a-mental-image-of-the-execution-flow"><u>this blog post</u></a>.</p>
<p>(You might have noticed that the above model is a bit different from the first initial idea - I added a logo detection block in there as well.)</p>
<p>Here is another workflow I developed for the second idea I mentioned above:</p>
<p><img src="https://i.ibb.co/6P5Sjnx/workflow-2.png" class="img-fluid"></p>
<p>This is slightly different from the initial idea I had. In fact, it does not even incorporate anything related to the Vision API. If I only wanted to deal with arXiv papers, I thought using the <a href="https://arxiv.org/help/api"><u>arXiv API</u></a> (I used the <a href="https://pypi.org/project/arxiv/"><u>arXiv Python library</u></a>) would be a far more reasonable option here since it already provides important information about an arXiv paper such as its categories, abstract, last updated date, and so on.</p>
<p>Finally, I wanted to combine the Vision and Text-to-Speech APIs for the second idea I had. In their demos, Dale and Kaz used <a href="https://cloud.google.com/automl-tables"><u>AutoML Tables</u></a> to train a model capable of classifying a paragraph of text into the following categories - “body”, “header”, “caption” and “others”. But I wanted to see if I can bypass this additional training step <em>to filter out the abstract block of a paper and perform optical character recognition (OCR) locally.</em> So, I came up with the following workflow -</p>
<p><img src="https://i.ibb.co/VgwV9v9/workflow-3.png" class="img-fluid"></p>
<p>As we can see I am using two Python libraries additionally -</p>
<ul>
<li><p><code>pdf2image</code> - as the name suggests, it is for converting a PDF file to PNG.</p></li>
<li><p><code>pytesseract</code> - this is for performing OCR locally on an image.</p></li>
</ul>
<p>In the next sections, I’ll discuss the problems I faced while implementing these workflows in code, and how I went about approaching the solutions.</p>
</section>
<section id="building-a-short-video-descriptor" class="level1">
<h1>Building a short video descriptor</h1>
<p>In the following texts, we will go over the main ingredients that turned out to be important while developing the prototypes. This will include some code along with the motivation to justify their inclusion.</p>
<p>For the first two workflows, it was mostly about reading the documentation carefully and figuring out the right APIs to use. GCP provides first-class documentation for these APIs with bindings available in many different languages as you can see in the figure below -</p>
<p><img src="https://i.ibb.co/CMRMwMS/image2.png" title="Source: https://cloud.google.com/video-intelligence/docs/analyze-labels" class="img-fluid"></p>
<p>I repurposed these code snippets for the workflows. The <a href="https://googleapis.dev/python/videointelligence/latest/index.html"><u>Python binding</u></a> of the Video Intelligence API is simple to use -</p>
<p>You first instantiate the client and instruct what all you are interested in performing -</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">video_client <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> videointelligence.VideoIntelligenceServiceClient()</span>
<span id="cb1-2">features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [videointelligence.enums.Feature.LABEL_DETECTION]</span></code></pre></div>
<p>It provides a bag of different features like entity detection, logo recognition, text recognition, object tracking, and so on. Here I am only interested in performing entity detection on a per-segment basis. A user usually specifies segments if they are interested to only analyze a part of their videos. I didn’t specify any segments, and in that case, the Video Intelligence API handles the entire video as a segment. The API also allows you to perform label detection on more granular levels, i.e.&nbsp;on both shot and frame levels.</p>
<p>After the initialization, it was only a matter of a few keystrokes till I made my first video annotation request -</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Specify the mode in which label detection is to be performed</span></span>
<span id="cb2-2">mode <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> videointelligence.enums.LabelDetectionMode.SHOT_AND_FRAME_MODE</span>
<span id="cb2-3">config <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> videointelligence.types.LabelDetectionConfig(label_detection_mode<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>mode)</span>
<span id="cb2-4">context <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> videointelligence.types.VideoContext(label_detection_config<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>config)</span>
<span id="cb2-5"> </span>
<span id="cb2-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Make the request</span></span>
<span id="cb2-7">operation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> video_client.annotate_video(</span>
<span id="cb2-8">    input_uri<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>gcs_path, features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>features, video_context<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>context)</span></code></pre></div>
<p>Here I am supplying a GCS bucket path of the video I wanted to infer on. Processing the results of the operation is also straightforward -</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Process video/segment level label annotations</span></span>
<span id="cb3-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Get the first response, since we sent only one video.</span></span>
<span id="cb3-3">segment_labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> operation.result.annotation_results[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].segment_label_annotations</span>
<span id="cb3-4">video_labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb3-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> (i, segment_label) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(segment_labels):</span>
<span id="cb3-6">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Video label description: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(segment_label.entity.description))</span>
<span id="cb3-7">    video_labels.append(segment_label.entity.description)</span></code></pre></div>
<p>After I got the entity labels on the entire video the next task was to use the Text-to-Speech API to generate an audio clip. For that, I simply followed the <a href="https://cloud.google.com/text-to-speech/docs/ssml-tutorial"><u>official tutorial</u></a> and reused the code.</p>
<p>The logo detection pipeline is almost similar with some very minor changes. In case you want to catch all the details please follow this <a href="https://colab.research.google.com/github/sayakpaul/GCP-ML-API-Demos/blob/master/Video_Intelligence_TTS.ipynb"><u>Colab Notebook</u></a>.</p>
<p>I tested the entire workflow on the following video and you can see the outputs right below it -</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/8mUIvDtxS_s" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">Processing</span> video for label annotations:</span>
<span id="cb4-2"></span>
<span id="cb4-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">Finished</span> processing.</span>
<span id="cb4-4"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">Video</span> label description: sidewalk</span>
<span id="cb4-5"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">Video</span> label description: street</span>
<span id="cb4-6"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">Video</span> label description: public space</span>
<span id="cb4-7"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">Video</span> label description: pedestrian</span>
<span id="cb4-8"></span>
<span id="cb4-9"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">Processing</span> video for logo detection:</span>
<span id="cb4-10"></span>
<span id="cb4-11"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">Finished</span> processing.</span></code></pre></div>
<p>As for the audio clip, it got came out pretty nice -</p>
<audio controls="">
<source src="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/labels.mp3" type="audio/mpeg">
</audio>
<p>Speed-wise the entire pipeline executed pretty quickly.</p>
<p>I had some previous experience working with videos, so I was able to get an idea of what was going under the hood for the video-related activities but for speech, I plan to get to that probably in the next summer (?)</p>
<p>A potential extension of this demo could be developed to aid blind people to navigate their ways when they are outside. I developed this demo keeping this mind, hence you won’t see any visual results.</p>
</section>
<section id="detecting-cropping-and-reading-an-arxiv-summary" class="level1">
<h1>Detecting, cropping, and reading an arXiv summary</h1>
<p>I presented with two different workflows for the second idea i.e.&nbsp;get the abstract of an arXiv paper and generate an audio clip of it. The workflow involving the arxiv Python library wasn’t problematic at all, so I am not going to discuss it in detail. You can always check out <a href="https://colab.research.google.com/github/sayakpaul/GCP-ML-API-Demos/blob/master/Abstract_Parser.ipynb"><u>this fully worked out Colab Notebook</u></a> in case you are interested.</p>
<p>The other workflow is a bit more involved. In there, I wanted to take an arXiv paper in PDF format, use the Vision API to get blocks of texts from it, and then locate the abstract from there like so -</p>
<p><img src="https://i.ibb.co/QkCGCxm/image1.png" class="img-fluid"></p>
<p>But that’s not it. I also wanted to perform OCR locally on the text blocks. This essentially allowed me to reduce the number of calls to the Vision API and thereby saving me some $. The final piece of the puzzle was to take the local OCR results and generate an audio clip. If you saw the <a href="https://cloud.google.com/text-to-speech/docs"><u>Text-to-Speech documentation</u></a>, you probably noticed that it is really not a big deal.</p>
<p>So, to realize this workflow here’s what I did (<a href="https://colab.research.google.com/github/sayakpaul/GCP-ML-API-Demos/blob/master/Abstract_Locator_Reader.ipynb"><u>Colab Notebook</u></a>) -</p>
<ul>
<li><p>As I am only interested in dealing with the abstract of a paper, I first converted the entire PDF-formatted paper to PNG and serialized only the first page. I used the <code>pdf2png</code> library for this.</p></li>
<li><p>Next, I used the Vision API to make a <code>document_text_detection()</code> request for getting the dense text blocks. The code for this is again, very straightforward -</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">client <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> vision.ImageAnnotatorClient()</span>
<span id="cb5-2">bounds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb5-3"></span>
<span id="cb5-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> io.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(image_file, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rb'</span>) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> image_file:</span>
<span id="cb5-5">    content <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> image_file.read()</span>
<span id="cb5-6"></span>
<span id="cb5-7">image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> types.Image(content<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>content)</span>
<span id="cb5-8">response <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> client.document_text_detection(image<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>image)</span>
<span id="cb5-9">document <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> response.full_text_annotation</span>
<span id="cb5-10"></span>
<span id="cb5-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Segregate the blocks</span></span>
<span id="cb5-12"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> page <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> document.pages:</span>
<span id="cb5-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> block <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> page.blocks:</span>
<span id="cb5-14">        bounds.append(block.bounding_box)</span></code></pre></div></li>
<li><p>Then I used the example presented <a href="https://cloud.google.com/vision/docs/fulltext-annotations"><u>here</u></a> to draw the bounding boxes on the input image which we saw earlier. I also reused these bounding boxes to segregate different blocks as inferred by the Vision API.</p></li>
<li><p>I am not going to get into the gory details of how I did the segregation. The catch here is for dense text block detection, Vision API returns <em>polygon coordinates</em> and <em>not</em> rectangular coordinates. So, I had to take polygon crops to segregate the different text blocks. (Thanks to <a href="https://stackoverflow.com/questions/22588074/polygon-crop-clip-using-python-pil"><u>this StackOverflow thread</u></a>.)</p></li>
<li><p>After the segregation part, I used <code>pytesseract</code> to perform OCR on the segregated text blocks. In <code>pytesseract</code> it’s literally doable with <code>text = pytesseract.image_to_string(image_block)</code>.</p></li>
<li><p>Now, an abstract cannot be just a single character (if the OCR was performed correctly). So I only considered those OCR’d texts where the character length is greater than 1000.</p></li>
<li><p>Even with this kind of thresholding, you’d end up with multiple text blocks where this criterion holds. To counter this, I first sorted the OCR’d text blocks with respect to their character lengths and checked if a text block contained only one or no reference to citations. If this criterion was matched then the text block is returned as the abstract.</p>
<p>Here’s how I coded it up:</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">texts_sorted <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sorted</span>(texts, key<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>)</span>
<span id="cb6-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> text <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> texts_sorted:</span>
<span id="cb6-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> text.split()[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].isupper() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;</span> text.count(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"["</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:</span>
<span id="cb6-4">        abstract <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> text</span></code></pre></div>
<p>The upper case criterion is there to ensure an abstract always starts with an uppercase letter.</p>
<p>I am aware that these handcrafted rules can get broken for many instances. But I wanted to explore this possibility anyway.</p></li>
<li><p>To make sure the Text-to-Speech API does not account for any citation I filtered out the raw text to escape them - <code>raw_lines = re.sub("[[\s*\d*\,*]*]", "", raw_lines)</code>.</p></li>
</ul>
<p>And that’s it! After a number of trial and error rounds, I was able to get a decent output.</p>
<p><img src="https://i.ibb.co/cQpHY9h/Screen-Shot-2020-09-25-at-10-18-11-AM.png" class="img-fluid"></p>
<audio controls="">
<source src="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/resnet_abstract.mp3" type="audio/mpeg">
</audio>
</section>
<section id="final-thoughts" class="level1">
<h1>Final thoughts</h1>
<p>Throughout this post, we went over two different ideas that are good prototype candidates for Machine Learning. We saw how easy it is to see these ideas in actions with different ML APIs. We saw how to make these different APIs work together to solve a given problem. Now, if you are feeling excited enough, you can dive deeper into the different ML tasks we saw: <strong>detection</strong> and <strong>classification</strong>, for example. Also note that even if one is using these APIs, it’s important to be able to process the API responses properly for the project at hand.</p>
<p>I would like to leave you with <a href="https://cloud.google.com/solutions/"><u>this amazing resource</u></a> provided by GCP. It includes detailed solution walkthroughs of real-world problem scenarios across a wide range of different industry verticals. They also show how to make the best use of different GCP services.</p>
<p><em>I would like to thank <a href="https://twitter.com/kweinmeister?lang=en"><u>Karl Weinmeister</u></a> for reviewing this post and for sharing his valuable feedback. Also, thanks to the <a href="https://developers.google.com/programs/experts/"><u>GDE program</u></a> for providing the GCP credit support which made these demos possible.</em></p>


</section>

 ]]></description>
  <category>gcp</category>
  <category>vision</category>
  <category>apis</category>
  <guid>https://sayak.dev/posts/mlapis-maker.html</guid>
  <pubDate>Fri, 25 Sep 2020 00:00:00 GMT</pubDate>
</item>
<item>
  <title>A few favorite recipes in computer vision &amp; deep learning</title>
  <link>https://sayak.dev/posts/2020-08-02-favorite-recipes-vision.html</link>
  <description><![CDATA[ 





<p>A few days ago from the time of writing this blog post I tweeted -</p>
<blockquote class="blockquote">
<p>twitter: https://twitter.com/RisingSayak/status/1285866356592635904?s=20</p>
</blockquote>
<p><br></p>
<p>In this blog post, I will expand on this tweet to convey why these are my favorite recipes among other things.</p>
<p>The training frameworks I mentioned can be classified into two broad categories -</p>
<ul>
<li><strong>supervised learning</strong> (Supervised Contrastive Learning [1] and BigTransfer [2])</li>
<li><strong>self-supervised learning</strong> (SimCLRv2 [only the <em>SimCLR</em> part]).</li>
</ul>
<p><br></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>that SimCLR [3] and SimCLRv2 [4] are two separate works.</p>
</div>
</div>
<section id="so-why-self-supervised-learning-anyway" class="level1">
<h1>So, why self-supervised learning anyway?</h1>
<p>The field of self-supervised visual representation learning is progressing pretty fast. With recent advancements, the deep learning community has started to consider it as an alternative to the fully supervised models for tasks like image classification, object detection, image segmentation, etc. If you are unfamiliar with self-supervised learning check out <a href="https://www.fast.ai/2020/01/13/self_supervised/">this blog post</a> by Jeremy Howard. So, <strong><em>why all the fuss around self-supervised visual representation learning?</em></strong></p>
<p>This is because a self-supervised learning framework can benefit from <strong><em>unlabeled data</em></strong>. Essentially, you would frame a supervised learning task from a large unlabeled corpus and then train a model to learn that task. You see we are <strong><em>not</em></strong> using any explicit label information here. Instead, we are using the given data to form a supervised learning task, this is why it is self-supervised. You would then take the representations from the model (preferably from the encoder part of the model) and use them for downstream tasks. Representations learned using self-supervised learning frameworks like SimCLRv2, SwAV [5] transfer quite well to downstream (vision) tasks even with very less labeled data.</p>
<p><a href="https://colinraffel.com/">Colin Raffel</a> beautifully summed up the recent progress in the field of self-supervised learning for computer vision -</p>
<blockquote class="blockquote">
<p>twitter: https://twitter.com/colinraffel/status/1289315020199743488?s=20</p>
</blockquote>
<p>BYOL [6] and SwAV have even beaten SimCLR -</p>
<p><img src="https://github.com/sayakpaul/portfolio/raw/master/posts/byol_swav.png" title="We see that training frameworks like BYOL and SwAV have already beaten SimCLR and all of this (including the SimCLR) has happened in 2020 itself! (Sources: BYOL and SwAV papers respectively [6][5])" class="img-fluid"></p>
<p>One could argue that this is in comparison with SimCLR but not SimCLRv2. Well, that is because SimCLRv2 is not just about self-supervised learning, it is more than that -</p>
<p><img src="https://github.com/sayakpaul/portfolio/raw/master/posts/simclr_v2.png" title="Source: SimCLRv2 [4]" class="img-fluid"></p>
<p>This blog post, however, would only focus on the SimCLR part i.e.&nbsp;the left-most part of the figure above. If you are interested to know more about SimCLRv2, feel free to watch <a href="https://www.youtube.com/watch?v=2lkUNDZld-4&amp;lc">this video</a> by Yannic Kilcher.</p>
</section>
<section id="why-simclrv2" class="level1">
<h1>Why SimCLR(v2)?</h1>
<p>Among all of the techniques that have been mentioned in the post so far, SimCLR is by far the most favorite of mine. This is because of its simplicity and the promise to shine more as one would scale up the base architecture and introduce more (unlabeled data). Moreover, framework wise SimCLR is very consistent with the recipes I would want to see in an effective visual representation learning framework. One of these primary recipes is to maximize the agreement between the semantically similar images. SwAV is also capable of doing this, in fact, it is currently the state-of-the-art (as of August 2020) in this domain. But simplicity wise SimCLR beats SwAV big time.</p>
<p>At a very high-level, SimCLR takes two different views of the same image and tries to maximize the agreement between these two views while minimizing the agreement between the views coming from other images. These different views are obtained by applying augmentation operations like random-resized crops, horizontal flips, color distortions, etc. Representations learned using this framework (and any self-supervised visual representation learning framework in general) can be used in different flavors -</p>
<ul>
<li>You may have loads of unlabeled data and limited labeled data for your problem. You could use the unlabeled data and incorporate SimCLR for obtaining effective representations and use them to a downstream task where the limited labeled data might be required.</li>
<li>Representations obtained from datasets like ImageNet using SimCLR can be used in regular transfer learning settings.</li>
</ul>
<p><img src="https://camo.githubusercontent.com/d92c0e914af70fe618cf3ea555e2da1737d84bc4/68747470733a2f2f312e62702e626c6f6773706f742e636f6d2f2d2d764834504b704539596f2f586f3461324259657276492f414141414141414146704d2f766146447750584f79416f6b4143385868383532447a4f67457332324e68625877434c63424741735948512f73313630302f696d616765342e676966.png" title="Source: SimCLR blog post [7]" class="img-fluid"></p>
<p>As we can see in SimCLR, the loss function (normalized temperature-scaled cross-entropy loss) operates directly on the features computed by the projection head (MLP part). This makes SimCLR a compute-intensive framework.</p>
<p>On the other hand, SwAV operates by assigning the encoded representations of different views of the same image to clusters. The clusters are being assigned by keeping a <strong><em>differentiable</em></strong> codebook for the prototypes of the different types of images present in the given dataset. Training wise, SwAV tries to maximize the agreement between the clusters of semantically similar images. Operating on the clusters rather than the encoded representations is a lesser compute-intensive task.</p>
<p><img src="https://camo.githubusercontent.com/a6d39dcc04416c84bd23f5e2ae87dceb40c6ef8b/68747470733a2f2f646c2e666261697075626c696366696c65732e636f6d2f64656570636c75737465722f616e696d617465642e676966.png" title="Source: GitHub repository of SwAV" class="img-fluid"></p>
<p>SwAV might appear as a simpler framework than SimCLR but there are a number of different pieces to look after here:</p>
<ul>
<li>Maintaining a differentiable prototype bank</li>
<li>Optimal transportation of the representations to form soft codes using the <a href="https://en.wikipedia.org/wiki/Sinkhorn%27s_theorem">Sinkhorn-Knopp algorithm</a></li>
<li>Multi-crop data augmentation policy</li>
<li>Swapped prediction problem</li>
</ul>
<p>When working in practical scenarios we often need to maintain a trade-off between technical feasibility and performance. To me, SimCLR cuts it through in terms of technical feasibility.</p>
<p><strong>Update</strong>: Along with <a href="https://twitter.com/ayushthakur0">Ayush</a> and generous amount of help from Mathilde Caron (first author of SwAV) we were finally able to <a href="https://wandb.ai/authors/swav-tf/reports/Unsupervised-visual-representation-learning-with-SwAV--VmlldzoyMjg3Mzg">minimally implement SwAV</a> after realizing the improvements it brings to the table.</p>
</section>
<section id="returning-to-supervised-learning" class="level1">
<h1>Returning to supervised learning</h1>
<p>Given the almightly prowess of the self-supervised learning frameworks why even bother about supervised regimes?</p>
<section id="labels-contrastive-loss-win-win" class="level2">
<h2 class="anchored" data-anchor-id="labels-contrastive-loss-win-win">Labels + contrastive loss = win-win</h2>
<p>Supervised Contrastive Learning addresses a very important point about the self-supervised learning frameworks like SimCLR. In SimCLR, the positive pairs are generated by taking different views of the same image and the negative pairs are then randomly sampled from the other images present in a batch.</p>
<p><img src="https://github.com/sayakpaul/portfolio/raw/master/posts/positive_pairs.png" title="Examples of positive pairs (Source: Exploring SimCLR [9])" class="img-fluid"></p>
<p>Here are some examples of negative pairs -</p>
<p><img src="https://github.com/sayakpaul/portfolio/raw/master/posts/negative_pairs.png" title="Source: Exploring SimCLR [9]" class="img-fluid"></p>
<p>As discussed in the Supervised Contrastive Learning paper, this method of contrasting two different views of the same image can result in false negatives i.e.&nbsp;the samples belonging to the same class might get mapped differently in the embedding space. There’s no way for us to properly mitigate this issue without having access to the original labels. Hence, I mentioned if you have loads of labeled images, it’s better to use Supervised Contrastive Learning to capture meaningful representations.</p>
<p><img src="https://github.com/sayakpaul/portfolio/raw/master/posts/scl_embeddings.png" title="Source: Supervised Contrastive Learning [1]" class="img-fluid"></p>
<p>Supervised Contrastive Learning extends how we train supervised models by introducing a two stage training framework. In the first stage, it uses the label information in the contrastive loss to learn to map the encoded representations effectively. In the second stage, it train a linear model on top of these encoded representations for the given supervised training objective.</p>
<p>In practice this works quite well -</p>
<p><img src="https://github.com/sayakpaul/portfolio/raw/master/posts/scl_results.png" title="Source: Supervised Contrastive Learning [1]" class="img-fluid"></p>
<p>You might be feeling very tempted to try out this on your labeled dataset. Truth be told - Supervised Contrastive Learning is also compute-intensive even for relatively small datasets. So, if you don’t have the training budget you might need to reconsider this option.</p>
<p>Maybe you have a relatively smaller labeled dataset and you just want to be able to fine-tune a good enough architecture which is still very useful for many practical scenarios. This is where BigTransfer can really shine.</p>
</section>
</section>
<section id="returning-to-supervised-transfer-learning" class="level1">
<h1>Returning to supervised transfer learning</h1>
<p>BigTransfer is from the family of classic supervised pre-training and transfer the learned representations to downstream tasks. ImageNet has been the choice for this kind of pre-training for a long time. But in BigTransfer, the authors use larger datasets such as ImageNet-21k, JFT along with ImageNet. In order to facilitate these larger datasets they scale up the model architectures along with longer pre-training. Their result speaks for itself -</p>
<p><img src="https://github.com/sayakpaul/portfolio/raw/master/posts/bit_results.png" title="Source: BigTransfer blog post [10]" class="img-fluid"></p>
<p>To eliminate the dependence on batch statistics the authors make use of Group Normalization and Weight Standardization. Personally, I really liked this recipe because using overall large batch sizes to train larger models at scale is a common choice and using Batch Normalization there could have easily affected the performance of the models during the downstream tasks.</p>
<p>For fine-tuning, the authors propose a heuristics-based BiT-HyperRule which provides instructions on what augmentation policy to use, how many steps to train for, what learning rate schedule to use, etc.</p>
<p>![](https://github.com/sayakpaul/portfolio/raw/master/posts/bit_hyperrule.png ” BiT Hyper-rule (Source: BigTransfer blog post [10])“)</p>
<p>I found this strategy to be simple enough to be implemented and practised.</p>
</section>
<section id="resources" class="level1">
<h1>Resources</h1>
<p>If you are interested to apply these techniques in your own works you may find the following resources to be helpful -</p>
<ul>
<li><a href="https://colab.research.google.com/github/google-research/simclr/blob/master/colabs/finetuning.ipynb">A Colab Notebook</a> by the authors of SimCLRv2 that shows how to fine-tune with SimCLRv2.</li>
<li><a href="https://bit.ly/2UVZtm7">A report</a> by Sweta Shaw and myself that walks through Supervised Contrastive Learning along with Colab Notebooks.</li>
<li><a href="https://blog.tensorflow.org/2020/05/bigtransfer-bit-state-of-art-transfer-learning-computer-vision.html">A tutorial</a> on BigTransfer by the authors of BigTransfer.</li>
</ul>
<p>Here is a list of some other interesting works on transfer learning for computer vision -</p>
<blockquote class="blockquote">
<p>twitter: https://twitter.com/RisingSayak/status/1285119290895548417</p>
</blockquote>
</section>
<section id="acknowledgments" class="level1">
<h1>Acknowledgments</h1>
<p>Amit’s <a href="https://amitness.com/2020/03/illustrated-simclr/">visual guide on SimCLR</a> and <a href="https://sthalles.github.io/simple-self-supervised-learning/">Thalles’s exploratory analysis on SimCLR</a> helped me in developing a thorough understanding of SimCLR.</p>
<p>Thanks to Yannic Kilcher for his explanation videos on <a href="https://www.youtube.com/watch?v=2lkUNDZld-4&amp;lc">SimCLRv2</a> and <a href="https://www.youtube.com/watch?v=MpdbFLXOOIw">Supervised Contrastive Learning</a>. Those made the learning process smoother.</p>
<p>Thanks to Ting Chen (SimCLR author) for providing me with additional pointers on self-supervised learning in general.</p>
<p>Thanks to Jeremy Howard for his <a href="http://fast.ai">fast.ai</a> lectures that continue to help me in approaching deep learning recipes with more common sense and practicality.</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ol type="1">
<li>Khosla, Prannay, et al.&nbsp;“Supervised Contrastive Learning.” ArXiv:2004.11362 [Cs, Stat], Apr.&nbsp;2020. <a href="http://arxiv.org/">arXiv.org</a>, <a href="http://arxiv.org/abs/2004.11362">http://arxiv.org/abs/2004.11362</a>.</li>
<li>Kolesnikov, Alexander, et al.&nbsp;“Big Transfer (BiT): General Visual Representation Learning.” ArXiv:1912.11370 [Cs], May 2020. <a href="http://arxiv.org/">arXiv.org</a>, <a href="http://arxiv.org/abs/1912.11370">http://arxiv.org/abs/1912.11370</a>.</li>
<li>Chen, Ting, Simon Kornblith, Mohammad Norouzi, et al.&nbsp;“A Simple Framework for Contrastive Learning of Visual Representations.” ArXiv:2002.05709 [Cs, Stat], June 2020. <a href="http://arxiv.org/">arXiv.org</a>, <a href="http://arxiv.org/abs/2002.05709">http://arxiv.org/abs/2002.05709</a>.</li>
<li>Chen, Ting, Simon Kornblith, Kevin Swersky, et al.&nbsp;“Big Self-Supervised Models Are Strong Semi-Supervised Learners.” ArXiv:2006.10029 [Cs, Stat], June 2020. <a href="http://arxiv.org/">arXiv.org</a>, <a href="http://arxiv.org/abs/2006.10029">http://arxiv.org/abs/2006.10029</a>.</li>
<li>Caron, Mathilde, et al.&nbsp;“Unsupervised Learning of Visual Features by Contrasting Cluster Assignments.” ArXiv:2006.09882 [Cs], July 2020. <a href="http://arxiv.org/">arXiv.org</a>, <a href="http://arxiv.org/abs/2006.09882">http://arxiv.org/abs/2006.09882</a>.</li>
<li>Grill, Jean-Bastien, et al.&nbsp;“Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning.” ArXiv:2006.07733 [Cs, Stat], June 2020. <a href="http://arxiv.org/">arXiv.org</a>, <a href="http://arxiv.org/abs/2006.07733">http://arxiv.org/abs/2006.07733</a>.</li>
<li>“Advancing Self-Supervised and Semi-Supervised Learning with SimCLR.” Google AI Blog, http://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html.</li>
<li>Facebookresearch/Swav. 2020. Facebook Research, 2020. GitHub, https://github.com/facebookresearch/swav.</li>
<li>Exploring SimCLR: A Simple Framework for Contrastive Learning of Visual Representations - Thalles’ Blog. https://sthalles.github.io/simple-self-supervised-learning/.</li>
<li>BigTransfer (BiT): State-of-the-Art Transfer Learning for Computer Vision. https://blog.tensorflow.org/2020/05/bigtransfer-bit-state-of-art-transfer-learning-computer-vision.html.</li>
</ol>


</section>

 ]]></description>
  <category>visual-representation-learning</category>
  <category>self-supervised-learning</category>
  <category>computer-vision</category>
  <guid>https://sayak.dev/posts/2020-08-02-favorite-recipes-vision.html</guid>
  <pubDate>Sun, 02 Aug 2020 00:00:00 GMT</pubDate>
  <media:content url="https://sayak.dev/posts/montage_vrl.png" medium="image" type="image/png" height="107" width="144"/>
</item>
<item>
  <title>Using TensorRT for accelerated deep learning inference</title>
  <link>https://sayak.dev/posts/tftrt-optimization.html</link>
  <description><![CDATA[ 





<p>If you see the way deep learning research has progressed over the years, it has always been guided by the need of the hour. If I were to develop a chronology out of it, it would be something like - train better model -&gt; train them faster -&gt; get them good at generalizing well, and so on. With a stern increase in the demand for using deep learning more as just another technology stack, there could not have been a better time to think about how do we make our models <strong><em>infer faster</em></strong>. In this post, we are going to see how to use <strong>TensorRT</strong> to perform accelerated inference with TensorFlow (2) models. After all, making predictions with deep learning models is what makes you real 💰 and we would want to make sure that our bucks burned judiciously.</p>
<div id="cell-5" class="cell" data-outputid="1604876a-056a-441e-fb62-e1944ddf5bdf">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">mobilenet_v2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.applications.MobileNetV2(weights<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'imagenet'</span>)</span>
<span id="cb1-2">mobilenet_v2.save(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'mobilenet_v2'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5
14540800/14536120 [==============================] - 0s 0us/step
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
INFO:tensorflow:Assets written to: mobilenet_v2/assets</code></pre>
</div>
</div>
<p>Here, <code>mobilenet_v2</code> is a directory and when you pass a directory to the save function, it serializes the model in the TensorFlow <a href="https://www.tensorflow.org/guide/saved_model"><u>SavedModel</u></a> format. This format makes it easier for us to be able to use it on different platforms - be it on GCP’s AI Platform, be it on TensorFlow JS, be it on TensorFlow Serving, and so on.</p>
<p>Now, there are some basic preprocessing steps to be followed before we can actually feed an image to this model -</p>
<div id="cell-9" class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Prepare the image for prediction</span></span>
<span id="cb3-2">img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.preprocessing.image.load_img(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'elephant.jpg'</span>, target_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">224</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">224</span>))</span>
<span id="cb3-3">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.preprocessing.image.img_to_array(img)</span>
<span id="cb3-4">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.expand_dims(x, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb3-5">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.applications.mobilenet_v2.preprocess_input(x)</span></code></pre></div>
</div>
<p>Here’s how <code>elephant.jpg</code> looks like in case if anyone’s curious -</p>
<div id="cell-11" class="cell" data-outputid="0bc7f8c6-d75d-4cc2-f6f6-8b73630c59c0">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://sayak.dev/posts/2020-07-01-accelerated-inference-trt_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Now, running the prediction and decoding it is just a matter of two lines of code -</p>
<div id="cell-13" class="cell" data-outputid="aec4f524-d843-4426-979b-8c8952005bd9">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Run inference</span></span>
<span id="cb4-2">preds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mobilenet_v2.predict(x)</span>
<span id="cb4-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Predicted:'</span>, tf.keras.applications.mobilenet_v2.decode_predictions(preds, top<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json
40960/35363 [==================================] - 0s 0us/step
Predicted: [('n02504013', 'Indian_elephant', 0.70024925), ('n01871265', 'tusker', 0.2549572), ('n02504458', 'African_elephant', 0.0033761878)]</code></pre>
</div>
</div>
<p>To find out how much time does this model take to predict a given image? Let’s write a short utility function to handle that -</p>
<div id="cell-15" class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> time_my_model(model, data):</span>
<span id="cb6-2">    times <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb6-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>):</span>
<span id="cb6-4">        start_time <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time.time()</span>
<span id="cb6-5">        one_prediction <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.predict(data)</span>
<span id="cb6-6">        delta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (time.time() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> start_time)</span>
<span id="cb6-7">        times.append(delta)</span>
<span id="cb6-8">    mean_delta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array(times).mean()</span>
<span id="cb6-9">    fps <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> mean_delta</span>
<span id="cb6-10">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'average(sec):</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{:.2f}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">,fps:</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{:.2f}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(mean_delta, fps))</span></code></pre></div>
</div>
<p>If we <code>run time_my_model</code> fives times the output would look like so -</p>
<div id="cell-17" class="cell" data-outputid="11cdc83b-7e0d-4d7c-d4a8-80e826155aae">
<div class="cell-output cell-output-stdout">
<pre><code>average(sec):0.06,fps:15.48
average(sec):0.03,fps:32.26
average(sec):0.03,fps:32.48
average(sec):0.03,fps:31.14
average(sec):0.03,fps:31.67</code></pre>
</div>
</div>
<p>Can we further optimize this? We will start the next section with this question.</p>
<section id="optimizing-the-pre-trained-image-classification-model" class="level1">
<h1>Optimizing the pre-trained image classification model</h1>
<p>Note that explaining the different means of optimizing a deep learning model is out of the scop for this post. If you are interested, the following posts are great starting points -</p>
<ul>
<li><p><a href="https://medium.com/tensorflow/high-performance-inference-with-tensorrt-integration-c4d78795fbfe"><u>High performance inference with TensorRT Integration</u></a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=fesdKLTZFBE"><u>Optimizing TensorFlow Models for Serving</u></a></p></li>
</ul>
<p>Let’s now introduce the big elephant in the room - TensorRT. TensorRT is an SDK by NVIDIA for performing accelerated deep learning inference. It utilizes Tensor Cores of an NVIDIA GPU (for example V100, P4, etc.) and performs a number of model optimization steps for including parameter quantization, constant folding, model pruning, layer fusion, etc. You can know more about this SDK from <a href="https://developer.nvidia.com/tensorrt"><u>here</u></a>.</p>
<p>Note that TensorRT will only be able to achieve acceleration when it’s used on supported hardware. For more on this, check out the aforementioned link.</p>
<p>Optimizing the MobileNetV2 model is a three-step process -</p>
<ul>
<li><strong>Setting up the optimization configuration</strong> -</li>
</ul>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(</span>
<span id="cb8-2">    precision_mode<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'FP16'</span>,</span>
<span id="cb8-3">    is_dynamic_op<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
<p>We use the <code>precision_mode</code> argument to specify the numerical precision of the model parameters we would want. In this case it is FP16 (float16). <code>is_dynamic_op</code> argument is set to True so that the shapes would be determined during runtime. Onto the next step.</p>
<pre><code>(`trt` is aliased as `from tensorflow.python.compiler.tensorrt import trt_convert as trt`.)</code></pre>
<ul>
<li><p><strong>Performing the model conversion for optimization</strong> -</p>
<p>As the headline suggests, in this step we actually perform the conversion with the configurations we specified in the previous step to optimize our model. <code>python   converter = trt.TrtGraphConverterV2(       input_saved_model_dir='mobilenet_v2',       conversion_params=params)   converter.convert()</code> For the conversion to take place, we are supplying the pre-trained MobileNetV2 model in the <code>SavedModel</code> format. It’s really nice to see how this format comes to unify different platforms.</p></li>
<li><p><strong>Serializing the optimized model</strong> -</p>
<p>Serializing this optimized model is similar to how we did it for the pre-trained model - <code>python   saved_model_dir_trt = 'mobilenet_v2.trt'   converter.save(saved_model_dir_trt)</code></p></li>
</ul>
<p>Now, how good is this new variant of the model? How accurate will it be? How much faster will it be? We will find those out in a moment. Before that let’s see how to run inference with this optimized model in the next section.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>The above code snippet won’t work in Colab or with any TensorFlow version that doesn’t ship a correctly compiled TensorRT (<a href="https://github.com/tensorflow/tensorflow/issues/57671">discussion</a>). Therefore, it’s recommended to use an NVIDIA Docker container for this purpose. <a href="https://huggingface.co/spaces/sayakpaul/tensorrt-tf">This resource</a> can be useful in this regard.</p>
</div>
</div>
</section>
<section id="running-inference-with-the-optimized-model" class="level1">
<h1>Running inference with the optimized model</h1>
<p>TensorFlow 2.x provides a convenient function <code>tf.saved_model.load</code> to load the models saved in <code>SavedModel</code>. We are only interested in performing inference with the model so we will load the respective signature from the model as a <a href="https://www.tensorflow.org/guide/concrete_function"><u>concrete function</u></a> -</p>
<div id="cell-23" class="cell">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Load the particular signature from the TRT graph</span></span>
<span id="cb10-2">root <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.saved_model.load(saved_model_dir_trt)</span>
<span id="cb10-3">concrete_func <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> root.signatures[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'serving_default'</span>]</span></code></pre></div>
</div>
<p>You can inspect the structure of this function by running <code>concrete_func.structured_outputs</code> and the output would be -</p>
<div id="cell-25" class="cell" data-outputid="9b4f0c46-340c-4a3f-e378-114c5314cb48">
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>{'predictions': TensorSpec(shape=(None, 1000), dtype=tf.float32, name='predictions')}</code></pre>
</div>
</div>
<p>As we would expect, this function will yield a 1000-d vector which is nothing but probabilities distributed across the 1000 different classes of the ImageNet dataset. Also note the key of the above dictionary, it might not be ‘Logits’ always.</p>
<p>Now, to be able to run the inference and decode them in a human-interpretable way, we first need to get the ImageNet dataset labels -</p>
<div id="cell-27" class="cell" data-outputid="afe498dc-3d5d-42af-9bc0-c188d7bed3cf">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Gather the ImageNet labels first and prepare them</span></span>
<span id="cb12-2">labels_path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.utils.get_file(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ImageNetLabels.txt'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt'</span>)</span>
<span id="cb12-3">imagenet_labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(labels_path).read().splitlines())</span></code></pre></div>
</div>
<p>Time for performing the inference -</p>
<div id="cell-29" class="cell" data-outputid="db68e554-02fc-4005-cc03-bd2d4d4c41c7">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Perform inference</span></span>
<span id="cb13-2">labeling <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> concrete_func(tf.constant(x.astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'float32'</span>)))</span>
<span id="cb13-3">activations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.nn.softmax(labeling[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'predictions'</span>])</span>
<span id="cb13-4">imagenet_labels[np.argsort(activations)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,::<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>array(['Indian elephant', 'tusker', 'African elephant', 'bull mastiff',
       'Great Dane'], dtype='&lt;U30')</code></pre>
</div>
</div>
<p>Looks like our optimized model got it right!</p>
<p>While parsing the predictions, we would need to put focus on the key in this case which is ‘predictions’.</p>
</section>
<section id="battle-of-performance" class="level1">
<h1>Battle of performance</h1>
<p>Let’s first recall where we were with our pre-trained MobileNetV2 -</p>
<pre><code>average(sec):0.03,fps:37.22
average(sec):0.03,fps:36.54
average(sec):0.03,fps:36.54
average(sec):0.03,fps:38.93
average(sec):0.03,fps:37.24</code></pre>
<p>Now, to time the performance of our optimized model, we will need to make little adjustments to the utility function we previously wrote. This is mainly because now, we will now be using a concrete function which takes a <code>tf.constant</code>.</p>
<div id="cell-32" class="cell">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> time_trt_model():</span>
<span id="cb16-2">    image_input <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.constant(x.astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'float32'</span>))</span>
<span id="cb16-3">    times <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb16-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>):</span>
<span id="cb16-5">        start_time <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time.time()</span>
<span id="cb16-6">        one_prediction <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> concrete_func(input_1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>image_input)</span>
<span id="cb16-7">        delta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (time.time() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> start_time)</span>
<span id="cb16-8">        times.append(delta)</span>
<span id="cb16-9">    mean_delta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array(times).mean()</span>
<span id="cb16-10">    fps <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> mean_delta</span>
<span id="cb16-11">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'average(sec):</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{:.2f}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">,fps:</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{:.2f}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(mean_delta, fps))</span></code></pre></div>
</div>
<p>For convenience, here’s our x -</p>
<div class="sourceCode" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.preprocessing.image.load_img(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'elephant.jpg'</span>, target_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">224</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">224</span>))</span>
<span id="cb17-2">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.preprocessing.image.img_to_array(img)</span>
<span id="cb17-3">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.expand_dims(x, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb17-4">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.applications.mobilenet_v2.preprocess_input(x)</span></code></pre></div>
<p>Let’s now <code>run time_trt_model()</code> for five times -</p>
<div id="cell-35" class="cell" data-outputid="84c921f6-81d5-4625-f02f-d1ef074410e3">
<div class="cell-output cell-output-stdout">
<pre><code>average(sec):0.00,fps:227.01
average(sec):0.00,fps:279.10
average(sec):0.00,fps:269.89
average(sec):0.00,fps:277.14
average(sec):0.00,fps:219.98</code></pre>
</div>
</div>
<p>That’s quite a bit of improvement, isn’t it? Note that you may observe slower inference in your first call to <code>time_trt_model()</code>. It may happen because of the additional time a GPU takes to set itself up. When running comparisons like this, it’s a good practice to first warm up the base hardware by running a few test iterations on it and then run the actual iterations for comparison. Depending on the GPU you’re using, these numbers can vary (these experiments were performed a <strong>Tesla P100</strong>).</p>
<p>Talking about the memory footprints of both the models, for the pre-trained model we have -</p>
<div id="cell-37" class="cell" data-outputid="ec8a1a3d-69bc-4027-9636-046af7c85eff">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Size of the model files</span></span>
<span id="cb19-2"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!</span>du <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">--</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">all</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>h mobilenet_v2</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>96K mobilenet_v2/variables/variables.data-00000-of-00002
14M mobilenet_v2/variables/variables.data-00001-of-00002
20K mobilenet_v2/variables/variables.index
14M mobilenet_v2/variables
3.9M    mobilenet_v2/saved_model.pb
4.0K    mobilenet_v2/assets
18M mobilenet_v2</code></pre>
</div>
</div>
<p>We have a total of <strong>18 MB</strong> here. For the optimized model, we have -</p>
<div id="cell-39" class="cell" data-outputid="2cba2513-1432-41c9-a78e-73ee796d1d34">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!</span>du <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">--</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">all</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>h mobilenet_v2.trt</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>96K mobilenet_v2.trt/variables/variables.data-00000-of-00002
14M mobilenet_v2.trt/variables/variables.data-00001-of-00002
20K mobilenet_v2.trt/variables/variables.index
14M mobilenet_v2.trt/variables
31M mobilenet_v2.trt/saved_model.pb
4.0K    mobilenet_v2.trt/assets
45M mobilenet_v2.trt</code></pre>
</div>
</div>
<p>Woah! The size has increased in this case! This is because the computational graph of our optimized model has been changed. However, the size of the parameters (<code>mobilenet_v2/variables</code> and <code>mobilenet_v2.trt/variables</code>) of both models is the same.</p>
<p>In real-life situations, as a machine learning practitioner, you will often have to make trade-offs between memory footprints, performance both in terms of accuracy and inference time. So having the knowledge of employing the right tools at the right moment will help you a long way. So, if you are looking for reducing the memory footprint of the model as well as accelerating the inference time, TensorFlow Lite is a good choice. In the bonus section of the post, we are going to discuss it.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>TensorFlow has now introduced a <a href="https://www.tensorflow.org/api_docs/python/tf/experimental/tensorrt/Converter"><code>tf.experimental.tensorrt.Converter</code></a> API with better support for TensorRT.</p>
</div>
</div>
</section>
<section id="using-tensorrt-on-your-custom-models" class="level1">
<h1>Using TensorRT on your custom models</h1>
<p>This section is for you to pick up. It will be a nice weekend project to train a simple model on a custom dataset and compare the performances as we saw in this post. Additionally, it will be interesting to compare different evaluation metrics like accuracy, precision, and recall for the different models (a custom trained mode, its optimized variants). We barely scratched the surface of TensorRT in this post. You are encouraged to experiment with the different arguments that come with the functions we saw in the post and figure out what works the best for your use-case. If your use-case involves embedded devices and mobile phones then <a href="https://www.tensorflow.org/lite">TensorFlow Lite</a> will be another great framework for you to explore.</p>


</section>

 ]]></description>
  <category>tf.keras</category>
  <category>tensorrt</category>
  <category>tensorflow</category>
  <guid>https://sayak.dev/posts/tftrt-optimization.html</guid>
  <pubDate>Wed, 01 Jul 2020 00:00:00 GMT</pubDate>
  <media:content url="https://sayak.dev/posts/speed.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Different data augmentation recipes in tf.keras for image classification</title>
  <link>https://sayak.dev/posts/augmentation-recipes.html</link>
  <description><![CDATA[ 





<p>Data augmentation is a favorite recipe among deep learning practitioners especially for the ones working in the field of computer vision. Data augmentation is a technique used for introducing variety in training data thereby helping to mitigate overfitting.</p>
<p>When using Keras for training image classification models, using the <code>ImageDataGenerator</code> class for handling data augmentation is pretty much a standard choice. However, with TensorFlow, we get a number of different ways we can apply data augmentation to image datasets. In this tutorial, we are going to discuss three such ways. Knowing about these different ways of plugging in data augmentation in your image classification training pipelines will help you decide the best way for a given scenario.</p>
<p>Here’s a brief overview of the different ways we are going to cover:</p>
<ul>
<li>Using the standard ImageDataGenerator class</li>
<li>Using TensorFlow image ops with a TensorFlow dataset</li>
<li>Using Keras’s (experimental) image processing layers</li>
<li>Mix-matching different image ops &amp; image processing layers</li>
</ul>
<p>Let’s get started!</p>
<section id="experimental-setup" class="level1">
<h1>Experimental setup</h1>
<p>We are going to use the flowers dataset to demonstrate the experiments. Downloading the dataset is just as easy as executing the following line of code:</p>
<p><code>flowers</code> contains the path (which in my case is - <code>/root/.keras/datasets/flower_photos</code>) where the dataset got downloaded. The structure of the dataset looks like so -</p>
<pre><code>├── daisy [633 entries]
├── dandelion [898]
├── roses [641]
├── sunflowers [699 entries]
├── tulips [799 entries]
└── LICENSE.txt</code></pre>
<div id="cell-6" class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Get the flowers dataset</span></span>
<span id="cb2-2">flowers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.utils.get_file(</span>
<span id="cb2-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'flower_photos'</span>,</span>
<span id="cb2-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz'</span>,</span>
<span id="cb2-5">    untar<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</div>
<p>Using the standard ImageDataGenerator class For most of the scenarios, the ImageDataGenerator should be good enough. Its flexible API design is really to follow and it makes it easier to work with custom image datasets by providing meaningful high-level abstractions.</p>
<p>We instantiate the ImageDataGenerator class like so -</p>
<div id="cell-10" class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">img_gen <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.preprocessing.image.ImageDataGenerator(</span>
<span id="cb3-2">    rescale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">255</span>,</span>
<span id="cb3-3">    rotation_range<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>,</span>
<span id="cb3-4">    horizontal_flip<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</div>
<p>We specify two augmentation operations and a pixel rescaling operation in there. <code>ImageDataGenerator</code> comes with a handy <code>flow_from_directory</code> method that allows us to read images from a directory and apply the specified operations on the fly during the time of training. Here’s how to instruct the <code>img_gen</code> object to read images from a directory -</p>
<div id="cell-12" class="cell" data-outputid="821f7d34-ab72-4cfc-9a52-3833df630437">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">IMG_SHAPE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">224</span></span>
<span id="cb4-2">BATCH_SIZE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span></span>
<span id="cb4-3"></span>
<span id="cb4-4">img_flow <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> img_gen.flow_from_directory(flowers, </span>
<span id="cb4-5">    shuffle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, </span>
<span id="cb4-6">    batch_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>BATCH_SIZE,</span>
<span id="cb4-7">    target_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(IMG_SHAPE, IMG_SHAPE))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Found 3670 images belonging to 5 classes.</code></pre>
</div>
</div>
<p>We then verify the images and the labels and they are indeed parsed right -</p>
<div id="cell-14" class="cell" data-outputid="4a3e083a-6a12-49e4-8e29-88e79b0f86d1">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">images, labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>(img_flow)</span>
<span id="cb6-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(images.shape, labels.shape)</span>
<span id="cb6-3">show_batch(images, labels)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(32, 224, 224, 3) (32, 5)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://sayak.dev/posts/2020-05-10-augmemtation-recipes_files/figure-html/cell-9-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Training with an ImageDataGenerator instance is extremely straight-forward -</p>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_training_model()</span>
<span id="cb8-2">model.fit(img_flow, ...)</span></code></pre></div>
<p>For a fully worked out example, refer to <a href="https://www.pyimagesearch.com/2019/06/03/fine-tuning-with-keras-and-deep-learning/">this tutorial</a>.</p>
<p>As can be seen in <a href="https://www.tensorflow.org/tutorials/load_data/images#performance">this blog post</a>, <code>ImageDataGenerator</code>’s overall data loading performance can have a significant effect on how fast your model trains. To tackle situations, where you need to maximize the hardware utilization without burning unnecessary bucks, <a href="https://www.tensorflow.org/guide/data">TensorFlow’s data module</a> can be really helpful (comes at some costs).</p>
</section>
<section id="tensorflow-image-ops-with-tf.data-apis" class="level1">
<h1>TensorFlow image ops with tf.data APIs</h1>
<p>The blog post I mentioned in the previous section shows the kind of performance boost achievable with <code>tf.data</code> APIs. But it’s important to note that boost comes at the cost of writing boilerplate code which makes the overall process more involved. For example, here’s how you would load and preprocess your images and labels -</p>
<div id="cell-17" class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> parse_images(image_path):</span>
<span id="cb9-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Load and preprocess the image</span></span>
<span id="cb9-3">    img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.io.read_file(image_path) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># read the raw image</span></span>
<span id="cb9-4">    img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.decode_jpeg(img, channels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># decode the image back to proper format</span></span>
<span id="cb9-5">    img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.convert_image_dtype(img, tf.float32) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># scale the pixel values to [0, 1] </span></span>
<span id="cb9-6">    img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.resize(img, [IMG_SHAPE, IMG_SHAPE]) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># resize the image</span></span>
<span id="cb9-7"></span>
<span id="cb9-8">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Parse the labels</span></span>
<span id="cb9-9">    label <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.strings.split(image_path, os.path.sep)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>]</span>
<span id="cb9-10"></span>
<span id="cb9-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (img, label)</span></code></pre></div>
</div>
<p>You would then write a separate augmentation policy with the <a href="https://www.tensorflow.org/api_docs/python/tf/image">TensorFlow Image ops</a> -</p>
<div id="cell-19" class="cell">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> augment(image, label):</span>
<span id="cb10-2">    img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.rot90(image)</span>
<span id="cb10-3">    img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.flip_left_right(img)</span>
<span id="cb10-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (img, label)</span></code></pre></div>
</div>
<p>To chain the above two together you would first create an initial dataset consisting of only the image paths -</p>
<div id="cell-21" class="cell">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">image_paths <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(paths.list_images(flowers))</span>
<span id="cb11-2">list_ds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.data.Dataset.from_tensor_slices((image_paths))</span></code></pre></div>
</div>
<p>Now, you would read, preprocess, shuffle, augment, and batch your dataset -</p>
<div id="cell-23" class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">AUTO <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.data.experimental.AUTOTUNE</span>
<span id="cb12-2"></span>
<span id="cb12-3">train_ds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb12-4">    list_ds</span>
<span id="cb12-5">    .<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(parse_images, num_parallel_calls<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>AUTO)</span>
<span id="cb12-6">    .shuffle(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1024</span>)</span>
<span id="cb12-7">    .<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(augment, num_parallel_calls<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>AUTO) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># augmentation call</span></span>
<span id="cb12-8">    .batch(BATCH_SIZE)</span>
<span id="cb12-9">    .prefetch(AUTO)</span>
<span id="cb12-10">)</span></code></pre></div>
</div>
<p><code>num_parallel_calls</code> allows you to parallelize the mapping function and <code>tf.data.experimental.AUTOTUNE</code> lets TensorFlow decide the level of parallelism to use dynamically (how cool is that?). prefetch allows loading in the next batch of data well before your model finishes the current epoch of training. It is evident that this process is more involved than the previous one.</p>
<p>Verifying if we constructed the data input pipeline correctly is a vital step before you feed your data to the model -</p>
<div id="cell-25" class="cell" data-outputid="de640bb6-85e6-4905-cce9-4ffa2cf1fa29">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">image_batch, label_batch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">iter</span>(train_ds))</span>
<span id="cb13-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(image_batch.shape, label_batch.shape)</span>
<span id="cb13-3">show_batch(image_batch.numpy(), label_batch.numpy(), image_data_gen<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(32, 224, 224, 3) (32,)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://sayak.dev/posts/2020-05-10-augmemtation-recipes_files/figure-html/cell-14-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The “b”s appear before the class labels because TensorFlow parses the strings as byte-strings. Using train_ds with your model is also just about executing -</p>
<div class="sourceCode" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_training_model()</span>
<span id="cb15-2">model.fit(train_ds, ...)</span></code></pre></div>
<p><a href="https://www.tensorflow.org/tutorials/images/data_augmentation">Here</a> you can find a fully worked out example. <a href="https://www.tensorflow.org/guide/data_performance">Here</a> you can know more about the different performance considerations when using tf.data. There are more image ops available with <strong>TensorFlow Addons</strong> which can found <a href="https://www.tensorflow.org/addons/tutorials/image_ops">here</a>.</p>
<p>Recently, Keras introduced <a href="https://keras.io/api/preprocessing/image/#image_dataset_from_directory"><code>image_dataset_from_directory</code></a> function (only available in <code>tf-nightly</code> at the time of writing this) which takes care of many of the boilerplate code we saw above and still yields pretty good performance. Here’s <a href="https://colab.research.google.com/drive/1umJnCp8tZ7UDTYSQsuWdKRhqbHts38AC">a tutorial</a> that shows how to use it.</p>
<p>Keras has also introduced <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing">a number of image processing layers</a> which can be very useful to build flexible augmentation pipelines using the Sequential API. In the next section, let’s see how.</p>
</section>
<section id="using-kerass-experimental-image-processing-layers" class="level1">
<h1>Using Keras’s (experimental) image processing layers</h1>
<p>Just like you would construct an entire model using the <code>Sequential</code> API, you can now construct very flexible data augmentation pipelines using the newly introduced (although experimental at the time of writing this) image processing layers. If we were to convert the data augmentation operations we have been following in the tutorial so far, building a data augmentation pipeline using this approach would be like so -</p>
<div id="cell-28" class="cell">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">data_augmentation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.Sequential([</span>
<span id="cb16-2">    tf.keras.layers.experimental.preprocessing.RandomFlip(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'horizontal'</span>),</span>
<span id="cb16-3">    tf.keras.layers.experimental.preprocessing.RandomRotation(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>)</span>
<span id="cb16-4">])</span></code></pre></div>
</div>
<p>Before passing your data through this stack of layers <strong>makes sure you haven’t applied any augmentation already</strong>. So, it’s safe to create a separate TensorFlow dataset without mapping the augmentation function like we previously did -</p>
<div id="cell-30" class="cell">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create TensorFlow dataset without any augmentation</span></span>
<span id="cb17-2">train_ds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb17-3">    list_ds</span>
<span id="cb17-4">    .<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(parse_images, num_parallel_calls<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>AUTO)</span>
<span id="cb17-5">    .shuffle(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1024</span>)</span>
<span id="cb17-6">    .batch(BATCH_SIZE)</span>
<span id="cb17-7">    .prefetch(AUTO)</span>
<span id="cb17-8">)</span></code></pre></div>
</div>
<p>Now, we can see how to examine some of the augmented images that would come out of this mini pipeline -</p>
<div id="cell-32" class="cell" data-outputid="f8f269a9-8bd3-492d-a445-212fbc6897c0">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">image_batch, label_batch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">iter</span>(train_ds))</span>
<span id="cb18-2"></span>
<span id="cb18-3">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>))</span>
<span id="cb18-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> n <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>):</span>
<span id="cb18-5">    ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplot(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb18-6">    augmented_image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data_augmentation(tf.expand_dims(image_batch[n], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>))</span>
<span id="cb18-7">    plt.imshow(augmented_image[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].numpy())</span>
<span id="cb18-8">    plt.title(label_batch[n].numpy().decode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"utf-8"</span>))</span>
<span id="cb18-9">    plt.axis(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'off'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://sayak.dev/posts/2020-05-10-augmemtation-recipes_files/figure-html/cell-17-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can also make use of Python <code>lambda</code>s to map <code>data_augmentation</code> directly to our <code>tf.data</code> pipeline like so:</p>
<div class="sourceCode" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">train_ds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb19-2">    list_ds</span>
<span id="cb19-3">    .<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(parse_images, num_parallel_calls<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>AUTO)</span>
<span id="cb19-4">    .shuffle(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1024</span>)</span>
<span id="cb19-5">    .batch(BATCH_SIZE)</span>
<span id="cb19-6">    .<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x, y: (data_augmentation(x), y),</span>
<span id="cb19-7">        num_parallel_calls<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>AUTO)</span>
<span id="cb19-8">    .prefetch(AUTO)</span>
<span id="cb19-9">)</span></code></pre></div>
<p>Note that these layers can be also added as a part of your model allowing them to run on GPUs. Based on your compute budget you should decide if you would want to run these layers on the GPU or you would rather have them executed separately on the CPU.</p>
<p>A functional model definition in Keras using this approach may look like so -</p>
<div class="sourceCode" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># You define an input layer with pre-defined shapes</span></span>
<span id="cb20-2">inputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> keras.Input(shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(IMG_SHAPE, IMG_SHAPE, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span>
<span id="cb20-3"></span>
<span id="cb20-4">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data_augmentation(inputs)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Apply random data augmentation</span></span>
<span id="cb20-5">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> feature_extractor_model(x, training<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb20-6">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> GlobalAveragePooling2D()(x)</span>
<span id="cb20-7">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Dropout(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>)(x)  </span>
<span id="cb20-8">outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Dense(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)(x)</span>
<span id="cb20-9"></span>
<span id="cb20-10">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Model(inputs, outputs)</span></code></pre></div>
<p>Now, <code>model</code> should be good to go with - <code>model.fit(train_ds, ...)</code>. A fully worked out example is available <a href="https://colab.research.google.com/drive/17vHSAj7no7RMdJ18MJomTf8twqw1suYC#scrollTo=nhSR8l3OX_sM">here</a>. Note that, performance might get slightly affected when going with this approach since the GPUs will be utilized to run the preprocessing layers as well.</p>
<p>Let’s now think about situations where we may need to use a combination of the image ops of TensorFlow and the layers we just saw. What if we need to plug in custom augmentation operations in the augmentation pipeline? Added on top of it, what if we need to fix the probability at which the augmentation operations would get applied? Data augmentation pipelines are quite central behind the success of recent works like <a href="https://arxiv.org/abs/2002.05709">SimCLR</a>, <a href="https://arxiv.org/abs/1912.02781">Augmix</a>, etc.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>These layers have pre-defined inference-time behaviour. So even if you have included them inside your model itself, it’s totally fine. But if you want them during inference, you would need to set its inference-time behaviour.</p>
</div>
</div>
</section>
<section id="towards-more-complex-augmentation-pipelines" class="level1">
<h1>Towards more complex augmentation pipelines</h1>
<p>In this final approach, we will see how to mix and match between the different stock image ops, and stock image processing layers. Let’s first define a class utilizing the stock image ops with a utility function to apply them at random with a pre-defined probability.</p>
<div id="cell-37" class="cell">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> CustomAugment(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">object</span>):</span>
<span id="cb21-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__call__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, image):        </span>
<span id="cb21-3">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Random flips and grayscale with some stochasticity</span></span>
<span id="cb21-4">        img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._random_apply(tf.image.flip_left_right, image, p<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>)</span>
<span id="cb21-5">        img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._random_apply(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._color_drop, img, p<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>)</span>
<span id="cb21-6">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> img</span>
<span id="cb21-7"></span>
<span id="cb21-8">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _color_drop(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x):</span>
<span id="cb21-9">        image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.rgb_to_grayscale(x)</span>
<span id="cb21-10">        image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.tile(x, [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>])</span>
<span id="cb21-11">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> x</span>
<span id="cb21-12">    </span>
<span id="cb21-13">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _random_apply(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, func, x, p):</span>
<span id="cb21-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> tf.cond(</span>
<span id="cb21-15">          tf.less(tf.random.uniform([], minval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, maxval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tf.float32),</span>
<span id="cb21-16">                  tf.cast(p, tf.float32)),</span>
<span id="cb21-17">          <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span>: func(x),</span>
<span id="cb21-18">          <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span>: x)</span></code></pre></div>
</div>
<p><code>_random_apply</code> is taken from the <a href="https://github.com/google-research/simclr">official SimCLR repository</a>. Now, in order to tie it together with the stock image processing layers, we can still use the <code>Sequential</code> API with a <code>Lambda</code> layer -</p>
<div id="cell-39" class="cell">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Build the augmentation pipeline</span></span>
<span id="cb22-2">data_augmentation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.Sequential([</span>
<span id="cb22-3">    tf.keras.layers.Lambda(CustomAugment()),</span>
<span id="cb22-4">    tf.keras.layers.experimental.preprocessing.RandomRotation(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>)</span>
<span id="cb22-5">])</span></code></pre></div>
</div>
<p>When we verify if it’s indeed correct, we get desired outputs -</p>
<div id="cell-41" class="cell" data-outputid="0fafb233-22ff-4446-fbf1-919d77f34613">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">image_batch, label_batch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">iter</span>(train_ds))</span>
<span id="cb23-2"></span>
<span id="cb23-3">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>))</span>
<span id="cb23-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> n <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>):</span>
<span id="cb23-5">    ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplot(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb23-6">    augmented_image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data_augmentation(tf.expand_dims(image_batch[n], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>))</span>
<span id="cb23-7">    plt.imshow(augmented_image[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].numpy())</span>
<span id="cb23-8">    plt.title(label_batch[n].numpy().decode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"utf-8"</span>))</span>
<span id="cb23-9">    plt.axis(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'off'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://sayak.dev/posts/2020-05-10-augmemtation-recipes_files/figure-html/cell-21-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Training models when using this approach remains the same as the previous one. Keep in mind that performance can get affected when using this approach.</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ul>
<li><a href="https://www.pyimagesearch.com/2019/06/03/fine-tuning-with-keras-and-deep-learning/">Fine-tuning with Keras and Deep Learning</a></li>
<li><a href="https://keras.io/guides/transfer_learning/">Transfer learning &amp; fine-tuning</a></li>
<li><a href="https://keras.io/examples/vision/image_classification_from_scratch/">Image classification from scratch</a></li>
<li><a href="https://www.tensorflow.org/tutorials/images/data_augmentation">Data augmentation</a></li>
</ul>


</section>

 ]]></description>
  <category>tf.keras</category>
  <category>data_augmentation</category>
  <category>image</category>
  <guid>https://sayak.dev/posts/augmentation-recipes.html</guid>
  <pubDate>Sun, 10 May 2020 00:00:00 GMT</pubDate>
  <media:content url="https://sayak.dev/posts/augmentation.png" medium="image" type="image/png" height="145" width="144"/>
</item>
<item>
  <title>Sharing your work online effectively</title>
  <link>https://sayak.dev/posts/sharing-work.html</link>
  <description><![CDATA[ 





<p>Well, you have put a lot of blood and sweat into writing your latest blog post on Machine Learning. Don’t let your struggle go in vain and let the world know about it. Sharing your blog posts across different channels not only gives you exposure but also may get you tremendous feedback on your work. In my personal experience, the feedback has been super useful for me to improve myself not only as a writer but also as a communicator. There can be times you might have missed out on a super important detail, or you might have unknowingly introduced a snazzy bug in the code listings of your blog – those things could have been caught in the process of feedback interchange.</p>
<p>In this short article, I am going to enlist a few different ways to share your work and get feedback. Note your work can be anything starting from a crucial GitHub PR, to a weekend project. Although the following platforms and communities are mostly limited to Machine Learning, I hope this guide will be useful for tech bloggers in general.</p>
<section id="sharing-on-platformscommunities" class="level2">
<h2 class="anchored" data-anchor-id="sharing-on-platformscommunities">Sharing on platforms/communities</h2>
<p>Before I start the sharing process, I generally create a Google Doc to effectively keep track of where I am sharing my work. This essentially acts as a checklist for all the places I want to share my work on. Here’s the template I follow for creating the Google Doc -</p>
<ul>
<li><p>Link to where the work has been posted.</p></li>
<li><p>Brief description of the work.</p></li>
<li><p>Post table:</p>
<p><img src="https://github.com/sayakpaul/portfolio/blob/master/images/blog_matters.png?raw=true" class="img-fluid"></p></li>
</ul>
<p>I generally keep the description to a maximum of <em>280 characters</em> so that I can use it on Twitter as well.</p>
<p>Now, turning to the platforms and communities, here are some recommendations (in no particular order): - HackerNews (https://news.ycombinator.com/newest) - Made With ML (https://madewithml.com/) - Reddit - <a href="https://www.reddit.com/r/MachineLearning/">r/MachineLearning</a> - <a href="https://www.reddit.com/r/MachinesLearn/">r/MachinesLearn</a> - <a href="https://www.reddit.com/r/learnmachinelearning/">r/learnmachinelearning</a> - <a href="https://www.reddit.com/r/deeplearning/">r/deeplearning</a> - Twitter - Facebook - <a href="https://www.facebook.com/groups/DeepNetGroup/">AIDL</a> - <a href="https://www.facebook.com/groups/MontrealAI/">Montreal AI</a> - <a href="https://www.facebook.com/groups/DeepLearnng/">Deep Learning</a> - Fast.ai Forum (https://forums.fast.ai/) - LinkedIn - Google Groups (depends on the framework used in the work) - discuss@tensorflow.org - tflite@tensorflow.org - tfjs@tensorflow.org - tfx@tensorflow.org</p>
<p>While sharing my work, I find it to be important to always attach a brief description. Additionally, if your work is related to implementing research work, you should definitely include it on <a href="https://paperswithcode.com/">Papers with Code</a>.</p>
</section>
<section id="sharing-to-aid-discussions" class="level2">
<h2 class="anchored" data-anchor-id="sharing-to-aid-discussions">Sharing to aid discussions</h2>
<p>You might be active on online forums like Quora, StackOverflow, and so on. While participating in a discussion in those forums you can make effective use of your work <em>if it is relevant</em>. In these cases, the approach is to not just supply a link to your work, but also to first write about any important pointers relevant to the discussion first, and then supply the link to your work to better aid it. Let’s say there’s a discussion going on the topic of <em>“What is Weight Initialization in Neural Nets?”</em> Here’s how I would approach my comment:</p>
<blockquote class="blockquote">
<p>A neural net can be viewed as a function with learnable parameters and those parameters are often referred to as weights and biases. Now, while starting the training of neural nets these parameters (typically the weights) are initialized in a number of different ways - sometimes, using constant values like 0’s and 1’s, sometimes with values sampled from some distribution (typically a uniform distribution or normal distribution), sometimes with other sophisticated schemes like Xavier Initialization. The performance of a neural net depends a lot on how its parameters are initialized when it is starting to train. Moreover, if we initialize it randomly for each run, it’s bound to be non-reproducible (almost) and even not-so-performant too. On the other hand, if we initialize it with constant values, it might take way too long to converge. With that, we also eliminate the beauty of randomness which in turn gives a neural net the power to reach covergence quicker using gradient-based learning. We clearly need a better way to initialize it. Careful initialization of weights helps us to train them better. To know more, please follow <a href="https://www.wandb.com/articles/the-effects-of-weight-initialization-on-neural-nets">this article of mine</a>.</p>
</blockquote>
<p>Well, that’s it for now. I hope it proves to be useful for you. Please provide any suggestions you may have via the comments. I am thankful to <a href="https://www.linkedin.com/in/alessio-gozzoli-530aa2109/">Alessio</a> of <a href="https://www.floydhub.com/">FloydHub</a> for sharing these tips with me.</p>


</section>

 ]]></description>
  <category>blogs</category>
  <category>sharing</category>
  <guid>https://sayak.dev/posts/sharing-work.html</guid>
  <pubDate>Mon, 20 Apr 2020 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Embedding an image preprocessing function in a tf.keras model</title>
  <link>https://sayak.dev/posts/2020-04-13-embedding-image-preprocessing-functions.html</link>
  <description><![CDATA[ 





<p>In this tutorial, we are going to see how to embed a simple image preprocessing function within a <em>trained model</em> (<code>tf.keras</code>) while exporting it for serving. This is a useful feature to have because it can help us reduce a lot of boilerplate code needed while using any model for serving purposes. With this capability, you get a lot more flexibility and modularity to your model.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>At the time of writing this post, layers under <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/"><code>tf.keras.layers.experimental.preprocessing</code></a> were fairly new. But with time, they have matured enough and I encourage the usage of these layers <em>inside</em> TensorFlow/Keras models.</p>
</div>
</div>
<h2 class="anchored">
Data loading, <strike>preprocessing</strike>, and visualization
</h2>
<p>To keep things simple we will be using the FashionMNIST dataset. Note that these techniques can easily be applied to more complex models as well (with some limitation).</p>
<blockquote class="blockquote">
<p>We are <strong>not</strong> going to preprocess the images before hand. We will let the model do it.</p>
</blockquote>
<div id="cell-6" class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Load data</span></span>
<span id="cb1-2">(x_train, y_train), (x_test, y_test) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.datasets.fashion_mnist.load_data()</span></code></pre></div>
</div>
<div id="cell-7" class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Class labels (don't change the order)</span></span>
<span id="cb2-2">CLASSES <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"T-shirt/top"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Trouser"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Pullover"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Dress"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Coat"</span>,</span>
<span id="cb2-3">               <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Sandal"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Shirt"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Sneaker"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Bag"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Ankle boot"</span>]</span></code></pre></div>
</div>
<div id="cell-8" class="cell" data-outputid="184b8098-f227-492d-cb8d-65e50f832bdf">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Show a few examples from the train set</span></span>
<span id="cb3-2">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>))</span>
<span id="cb3-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>):</span>
<span id="cb3-4">    plt.subplot(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-5">    plt.xticks([])</span>
<span id="cb3-6">    plt.yticks([])</span>
<span id="cb3-7">    plt.grid(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb3-8">    plt.imshow(x_train[i], cmap<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>plt.cm.binary)</span>
<span id="cb3-9">    plt.xlabel(CLASSES[y_train[i]])</span>
<span id="cb3-10">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://sayak.dev/posts/2020-04-13-embedding-image-preprocessing-functions_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="model-building-and-training" class="level2">
<h2 class="anchored" data-anchor-id="model-building-and-training">Model building and training</h2>
<p>We are good to proceed towards building and training a neural network. We will first define a simple preprocessing function to scale the pixel values and then we will embed it into the model using a <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda"><code>Lambda</code> layer</a>. You can replace this anything fancy you would want.</p>
<p>We will use a shallow network architecture so that we can train it quickly.</p>
<div id="cell-11" class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define the preprocessing function</span></span>
<span id="cb4-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># We will embed it in the model later</span></span>
<span id="cb4-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> preprocess_image(image_pixels):</span>
<span id="cb4-4">    img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> image_pixels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">255</span></span>
<span id="cb4-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> img</span>
<span id="cb4-6"></span>
<span id="cb4-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># A humble model</span></span>
<span id="cb4-8"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_training_model():</span>
<span id="cb4-9">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Construct the model using the Functional API</span></span>
<span id="cb4-10">    input_layer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.layers.Input(shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>), name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input_layer"</span>)</span>
<span id="cb4-11">    preproc_layer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.layers.Lambda(preprocess_image, name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"lambda_layer"</span>)(input_layer) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Preprocessing function</span></span>
<span id="cb4-12">    flatten <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.layers.Flatten()(preproc_layer)</span>
<span id="cb4-13">    dense_1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.layers.Dense(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, activation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"relu"</span>)(flatten)</span>
<span id="cb4-14">    dropout <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.layers.Dropout(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>)(dense_1)</span>
<span id="cb4-15">    outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.layers.Dense(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(CLASSES), activation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"softmax"</span>)(dropout)</span>
<span id="cb4-16"></span>
<span id="cb4-17">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create the model</span></span>
<span id="cb4-18">    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.models.Model(input_layer, outputs)</span>
<span id="cb4-19"></span>
<span id="cb4-20">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compile the model and return it</span></span>
<span id="cb4-21">    model.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">compile</span>(optimizer<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'adam'</span>,</span>
<span id="cb4-22">                loss<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sparse_categorical_crossentropy'</span>,</span>
<span id="cb4-23">                metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'accuracy'</span>])</span>
<span id="cb4-24">        </span>
<span id="cb4-25">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> model</span></code></pre></div>
</div>
<div id="cell-12" class="cell" data-outputid="5ceb1291-0c2f-45bc-9faf-4cd29bbabdcd">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Topology of the model</span></span>
<span id="cb5-2">tf.keras.utils.plot_model(get_training_model(), show_shapes<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<div>
<figure class="figure">
<p><img src="https://sayak.dev/posts/2020-04-13-embedding-image-preprocessing-functions_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The <code>Lambda</code> layer is our preprocessing layer.</p>
<div id="cell-14" class="cell" data-outputid="e2150e7d-15c2-46d5-c386-259c91b8f980">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Train the model for 10 epochs</span></span>
<span id="cb6-2">apparel_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_training_model()</span>
<span id="cb6-3">history <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> apparel_model.fit(x_train, y_train, </span>
<span id="cb6-4">    validation_data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(x_test, y_test), </span>
<span id="cb6-5">    epochs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, </span>
<span id="cb6-6">    batch_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10
469/469 [==============================] - 2s 4ms/step - loss: 0.6004 - accuracy: 0.7937 - val_loss: 0.4682 - val_accuracy: 0.8347
Epoch 2/10
469/469 [==============================] - 2s 4ms/step - loss: 0.4246 - accuracy: 0.8495 - val_loss: 0.4089 - val_accuracy: 0.8521
Epoch 3/10
469/469 [==============================] - 2s 4ms/step - loss: 0.3795 - accuracy: 0.8642 - val_loss: 0.3928 - val_accuracy: 0.8564
Epoch 4/10
469/469 [==============================] - 2s 4ms/step - loss: 0.3576 - accuracy: 0.8711 - val_loss: 0.3632 - val_accuracy: 0.8687
Epoch 5/10
469/469 [==============================] - 2s 4ms/step - loss: 0.3407 - accuracy: 0.8762 - val_loss: 0.3593 - val_accuracy: 0.8688
Epoch 6/10
469/469 [==============================] - 2s 4ms/step - loss: 0.3294 - accuracy: 0.8788 - val_loss: 0.3532 - val_accuracy: 0.8721
Epoch 7/10
469/469 [==============================] - 2s 4ms/step - loss: 0.3165 - accuracy: 0.8846 - val_loss: 0.3609 - val_accuracy: 0.8685
Epoch 8/10
469/469 [==============================] - 2s 4ms/step - loss: 0.3084 - accuracy: 0.8859 - val_loss: 0.3503 - val_accuracy: 0.8701
Epoch 9/10
469/469 [==============================] - 2s 4ms/step - loss: 0.2982 - accuracy: 0.8915 - val_loss: 0.3560 - val_accuracy: 0.8713
Epoch 10/10
469/469 [==============================] - 2s 4ms/step - loss: 0.2886 - accuracy: 0.8929 - val_loss: 0.3381 - val_accuracy: 0.8776</code></pre>
</div>
</div>
<p>Now that we have a trained model, we can go ahead and export it and then we will see how to use it on new images for inference.</p>
</section>
<section id="sample-test-image-and-model-export" class="level2">
<h2 class="anchored" data-anchor-id="sample-test-image-and-model-export">Sample test image and model export</h2>
<p>We are getting close. Now that we have a trained model here are the things we would do from here: - Serialize a randomly selected image from the test set.<br>
- Export the model and parse model predictions.</p>
<p>Let’s go.</p>
<section id="step-1-serializing-a-randomly-selected-image-from-the-test-set" class="level3">
<h3 class="anchored" data-anchor-id="step-1-serializing-a-randomly-selected-image-from-the-test-set">Step 1: Serializing a randomly selected image from the test set</h3>
<div id="cell-19" class="cell" data-outputid="60b5fec1-21de-4482-da04-23d71532b2a9">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Select a random image from the test set for serialization</span></span>
<span id="cb8-2">sampe_test_img_id <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.choice(x_test.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb8-3">sampe_test_img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x_test[sampe_test_img_id].squeeze() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Remove the batch dimension</span></span>
<span id="cb8-4">sampe_test_img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (sampe_test_img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">255</span>).astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"int32"</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Scale back to integer</span></span>
<span id="cb8-5"></span>
<span id="cb8-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Verify image label and shape</span></span>
<span id="cb8-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Image class: "</span>,CLASSES[y_test[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(sampe_test_img_id)]])</span>
<span id="cb8-8"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(sampe_test_img.shape)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Image class:  Ankle boot
(28, 28)</code></pre>
</div>
</div>
<div id="cell-20" class="cell" data-outputid="a6284755-df1a-42ee-b68b-00549b695662">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Serialize the image</span></span>
<span id="cb10-2">cv2.imwrite(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sample_image.png"</span>, sampe_test_img)</span></code></pre></div>
</div>
<p>Note that while writing a grayscale image, <a href="https://stackoverflow.com/questions/18870603/in-opencv-python-why-am-i-getting-3-channel-images-from-a-grayscale-image">OpenCV adds the channel dimension of 3 to it</a>. We will need to handle carefully.</p>
<div id="cell-22" class="cell" data-outputid="1ffde99a-9854-4db1-dfe4-f42a756dc4dc">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Make sure the serialized image is good to go</span></span>
<span id="cb11-2">plt.imshow(plt.imread(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sample_image.png"</span>), cmap<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>plt.cm.binary)</span>
<span id="cb11-3">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://sayak.dev/posts/2020-04-13-embedding-image-preprocessing-functions_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="step-2-model-exporting-and-parsing-predictions" class="level3">
<h3 class="anchored" data-anchor-id="step-2-model-exporting-and-parsing-predictions">Step 2: Model exporting and parsing predictions</h3>
<p>Let’s first serialize our model and load it.</p>
<div id="cell-25" class="cell" data-outputid="41ad14a0-c94c-4362-8552-32c109559ce9">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Serialize the model and load it</span></span>
<span id="cb12-2">apparel_model.save(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"apparel_model.h5"</span>)</span>
<span id="cb12-3">restored_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.models.load_model(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"apparel_model.h5"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.</code></pre>
</div>
</div>
<p>This warning is not desirable. When the optimizer is loaded with a fresh state, the model predictions can be erroneous. So, to resolve this problem we will only be serializing the weights of the model with the <code>save_weights()</code> function. There can be other nuances like this when you work with <code>Lambda</code> layers and you can check <a href="https://blog.paperspace.com/working-with-the-lambda-layer-in-keras/#saving-and-loading-a-model-with-a-lambda-layer">this article</a> out to know about them.</p>
<div id="cell-27" class="cell">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">apparel_model.save_weights(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"apparel_model.h5"</span>)</span></code></pre></div>
</div>
<p>We will now initialize a dummy model with the same architecture as the one we trained and we will then load the weights of our trained model into it.</p>
<div id="cell-29" class="cell">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">restored_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_training_model()</span>
<span id="cb15-2">restored_model.load_weights(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"apparel_model.h5"</span>)</span></code></pre></div>
</div>
<p>Now we should be good to go with the predictions part. First, let’s load the image we serialized in step 1. As mentioned before, OpenCV adds 3-channels to grayscale images while saving them. We can take care of this issue with <code>cv2.cvtColor(image_pixels, cv2.COLOR_BGR2GRAY)</code>.</p>
<div id="cell-31" class="cell" data-outputid="d2655f40-476e-4765-d579-7a472410d5c8">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Load the image</span></span>
<span id="cb16-2">image_pixels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cv2.imread(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sample_image.png"</span>)</span>
<span id="cb16-3">image_pixels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cv2.cvtColor(image_pixels, cv2.COLOR_BGR2GRAY)</span>
<span id="cb16-4"></span>
<span id="cb16-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Preview the image</span></span>
<span id="cb16-6">plt.imshow(image_pixels, cmap<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>plt.cm.binary)</span>
<span id="cb16-7">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://sayak.dev/posts/2020-04-13-embedding-image-preprocessing-functions_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-32" class="cell" data-outputid="9ca64126-87b3-4a01-9ce6-0f8d21a23aa2">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Run inference and parse the prediction</span></span>
<span id="cb17-2">class_probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> restored_model.predict(np.expand_dims(image_pixels, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>))[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb17-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Predicted "</span>,CLASSES[np.argmax(class_probabilities)])</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Predicted  Ankle boot</code></pre>
</div>
</div>
<p>We can see that it is working as expected.</p>
<div id="cell-34" class="cell">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Load and *preprocess* data</span></span>
<span id="cb19-2">(x_train, y_train), (x_test, y_test) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.datasets.fashion_mnist.load_data()</span>
<span id="cb19-3">x_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">255</span></span>
<span id="cb19-4">x_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">255</span></span></code></pre></div>
</div>
</section>
</section>
<section id="taking-it-a-step-further-with-concrete-functions-and-savedmodel" class="level2">
<h2 class="anchored" data-anchor-id="taking-it-a-step-further-with-concrete-functions-and-savedmodel">Taking it a step further with concrete functions and <code>SavedModel</code></h2>
<p>The <a href="https://www.tensorflow.org/guide/saved_model"><code>SavedModel</code></a> format is the standard serialization format in TensorFlow 2.x since it communicates very well with the entire TensorFlow ecosystem. Be it GCP AI Platform, be it <code>tf.keras</code>, be it TFLite, etc,, <code>SavedModel</code> format unifies the entire ecosystem. For serializing custom models (developed using <a href="https://www.pyimagesearch.com/2019/10/28/3-ways-to-create-a-keras-model-with-tensorflow-2-0-sequential-functional-and-model-subclassing/">subclassing</a>) <code>SavedModel</code> would be needed as well.</p>
<p>In this section, let’s see how can we do the same i.e.&nbsp;embed a preprocessing function inside a model so that it can be serialized in the <code>SavedModel</code> format.</p>
<section id="step-1-create-a-sequential-model-without-any-preprocessing-layer" class="level3">
<h3 class="anchored" data-anchor-id="step-1-create-a-sequential-model-without-any-preprocessing-layer">Step 1: Create a sequential model <em>without</em> any preprocessing layer</h3>
<div id="cell-38" class="cell">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_training_model_v2():</span>
<span id="cb20-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Construct the model using the Functional API</span></span>
<span id="cb20-3">    input_layer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.layers.Input(shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>), name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input_layer"</span>)</span>
<span id="cb20-4">    flatten <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.layers.Flatten()(input_layer)</span>
<span id="cb20-5">    dense_1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.layers.Dense(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, activation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"relu"</span>)(flatten)</span>
<span id="cb20-6">    dropout <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.layers.Dropout(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>)(dense_1)</span>
<span id="cb20-7">    outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.layers.Dense(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(CLASSES), activation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"softmax"</span>)(dropout)</span>
<span id="cb20-8"></span>
<span id="cb20-9">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create the model</span></span>
<span id="cb20-10">    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.models.Model(input_layer, outputs)</span>
<span id="cb20-11"></span>
<span id="cb20-12">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compile the model and return it</span></span>
<span id="cb20-13">    model.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">compile</span>(optimizer<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"adam"</span>,</span>
<span id="cb20-14">                loss<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb20-15">                metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"accuracy"</span>])</span>
<span id="cb20-16">        </span>
<span id="cb20-17">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> model</span></code></pre></div>
</div>
</section>
<section id="step-2-train-it" class="level3">
<h3 class="anchored" data-anchor-id="step-2-train-it">Step 2: Train it!</h3>
<div id="cell-40" class="cell" data-outputid="b40b564c-d345-493a-ba2b-5f86635c7d10">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Train the model for 10 epochs</span></span>
<span id="cb21-2">apparel_model_v2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_training_model_v2()</span>
<span id="cb21-3">history <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> apparel_model_v2.fit(x_train, y_train, </span>
<span id="cb21-4">    validation_data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(x_test, y_test), </span>
<span id="cb21-5">    epochs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, </span>
<span id="cb21-6">    batch_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10
469/469 [==============================] - 2s 4ms/step - loss: 0.5995 - accuracy: 0.7914 - val_loss: 0.4549 - val_accuracy: 0.8347
Epoch 2/10
469/469 [==============================] - 2s 4ms/step - loss: 0.4200 - accuracy: 0.8501 - val_loss: 0.4094 - val_accuracy: 0.8520
Epoch 3/10
469/469 [==============================] - 2s 4ms/step - loss: 0.3823 - accuracy: 0.8616 - val_loss: 0.3831 - val_accuracy: 0.8635
Epoch 4/10
469/469 [==============================] - 2s 4ms/step - loss: 0.3575 - accuracy: 0.8713 - val_loss: 0.3896 - val_accuracy: 0.8563
Epoch 5/10
469/469 [==============================] - 2s 4ms/step - loss: 0.3405 - accuracy: 0.8758 - val_loss: 0.3569 - val_accuracy: 0.8720
Epoch 6/10
469/469 [==============================] - 2s 4ms/step - loss: 0.3249 - accuracy: 0.8813 - val_loss: 0.3490 - val_accuracy: 0.8733
Epoch 7/10
469/469 [==============================] - 2s 4ms/step - loss: 0.3176 - accuracy: 0.8840 - val_loss: 0.3480 - val_accuracy: 0.8735
Epoch 8/10
469/469 [==============================] - 2s 4ms/step - loss: 0.3055 - accuracy: 0.8878 - val_loss: 0.3355 - val_accuracy: 0.8809
Epoch 9/10
469/469 [==============================] - 2s 4ms/step - loss: 0.2971 - accuracy: 0.8914 - val_loss: 0.3331 - val_accuracy: 0.8792
Epoch 10/10
469/469 [==============================] - 2s 4ms/step - loss: 0.2905 - accuracy: 0.8920 - val_loss: 0.3344 - val_accuracy: 0.8808</code></pre>
</div>
</div>
</section>
<section id="step-3-savedmodel-plunge" class="level3">
<h3 class="anchored" data-anchor-id="step-3-savedmodel-plunge">Step 3: <code>SavedModel</code> plunge</h3>
<p>Okay! Now we are ready to the crux of the section. We will first create a custom model class (inherited from <code>tf.keras.Model</code>) and it will contain two things: - A model that is loaded with the weights of a trained model - A <em>serving function</em> that will contain the preprocessing function along with the necessary signature.</p>
<div id="cell-43" class="cell">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># A custom class for serving</span></span>
<span id="cb23-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> ExportModel(tf.keras.Model):</span>
<span id="cb23-3">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, model):</span>
<span id="cb23-4">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>)</span>
<span id="cb23-5">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model</span>
<span id="cb23-6"></span>
<span id="cb23-7">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@tf.function</span>(input_signature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[tf.TensorSpec([<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>], dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tf.uint8)])</span>
<span id="cb23-8">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> my_serve(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, images):</span>
<span id="cb23-9">        images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.cast(images, tf.float32) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">255</span>        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># pre-processing</span></span>
<span id="cb23-10">        probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model(images)                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># prediction from model</span></span>
<span id="cb23-11">        class_index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.argmax(probabilities, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)   <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># post-processing</span></span>
<span id="cb23-12">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"class_index"</span>: class_index}</span></code></pre></div>
</div>
<p><code>my_serve</code> is our serving function. You can see that is <em>decorated</em> with <code>tf.function</code> and the reason behind doing so is it allows us to embed an arbitrary function in a model’s graph which can later be exported using the <code>SavedModel</code> format.</p>
<p>We can also see - <code>input_signature=[tf.TensorSpec([None, 28, 28], dtype=tf.uint8)]</code>. This is needed in order to indicate which part of the model’s graph would be needed while serving. By specifying <code>tf.TensorSpec([None, 28, 28]</code>, we instruct the function that the inputs should respect this shape - <code>[None, 28, 28]</code> and the <code>dtype</code> argument is self-explanatory.</p>
<p>We will get to why the return type of the function is done in such a way - <code>{"class_index": class_index}</code> in a moment.</p>
<p>If you are interested to know more using <code>SavedModel</code> and different serialization options that come with it, be sure to check <a href="https://www.tensorflow.org/guide/saved_model">this tutorial</a> out.</p>
</section>
<section id="step-4-instantiate-a-dummy-model-and-set-its-weights" class="level3">
<h3 class="anchored" data-anchor-id="step-4-instantiate-a-dummy-model-and-set-its-weights">Step 4: Instantiate a dummy model and set its weights</h3>
<div id="cell-46" class="cell">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Set the weights of this dummy model to the weights of the model we trained</span></span>
<span id="cb24-2">restored_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_training_model_v2()</span>
<span id="cb24-3">restored_model.set_weights(apparel_model_v2.get_weights()) </span></code></pre></div>
</div>
</section>
<section id="step-5-export-the-model-and-run-inference" class="level3">
<h3 class="anchored" data-anchor-id="step-5-export-the-model-and-run-inference">Step 5: Export the model and run inference</h3>
<p>Now, to serialize the model in the <code>SavedModel</code> format we will make use of <a href="https://www.tensorflow.org/api_docs/python/tf/saved_model/save"><code>tf.saved_model.save</code></a>. It can automatically determine which input signature to use for serving for most of the models <em>if the details are available</em>. However, in our case, it won’t be able to do so. So, we will need to explicitly indicate which function to use as the signature while serving.</p>
<div id="cell-49" class="cell" data-outputid="08bc2798-9874-4596-bac6-1375e72ad5e8">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">export_path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"/content/saved_model/1/"</span></span>
<span id="cb25-2">tf.keras.backend.set_learning_phase(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Make sure no weight update happens</span></span>
<span id="cb25-3">serving_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ExportModel(restored_model) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Instantiate a model with the preprocessing function</span></span>
<span id="cb25-4">tf.saved_model.save(serving_model, export_path, signatures<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'serving_default'</span>: serving_model.my_serve})</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>WARNING:tensorflow:Skipping full serialization of Keras layer &lt;__main__.ExportModel object at 0x7f4096b7b358&gt;, because it is not built.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
INFO:tensorflow:Assets written to: /content/saved_model/1/assets</code></pre>
</div>
</div>
<p>By specifying <code>'serving_default': serving_model.my_serve</code> we instructed <code>tf.saved_model.save</code> about which signature to use for serving. Now if we inspect what all were saved, things should seem consistent. For this we are going to use the <a href="https://www.tensorflow.org/guide/saved_model#saved_model_cli"><code>saved_model_cli</code></a> command-line interpreter.</p>
<div id="cell-51" class="cell" data-outputid="d26a3550-47c1-477b-bb0f-0ea34c6929b4">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!</span>saved_model_cli show <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">--</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dir</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>content<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>saved_model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">--</span>tag_set serve <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">--</span>signature_def serving_default</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The given SavedModel SignatureDef contains the following input(s):
  inputs['images'] tensor_info:
      dtype: DT_UINT8
      shape: (-1, 28, 28)
      name: serving_default_images:0
The given SavedModel SignatureDef contains the following output(s):
  outputs['class_index'] tensor_info:
      dtype: DT_INT64
      shape: (-1)
      name: StatefulPartitionedCall:0
Method name is: tensorflow/serving/predict</code></pre>
</div>
</div>
<p>So, we can see that the configuration that is expected from the inputs and the outputs of the serialized model is consistent with what we had instructed. We returned the outputs in form a dictionary (namely <code>class_index</code>) in <code>my_serve</code> and we can see that as well.</p>
<p>We can also do the inspection in Pythonic ways.</p>
<div id="cell-53" class="cell" data-outputid="c8112735-8804-4ae7-815f-1afd6336f4be">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">loaded <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.saved_model.load(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"/content/saved_model/1/"</span>)</span>
<span id="cb29-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(loaded.signatures.keys())) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># This signature will be used while serving</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['serving_default']</code></pre>
</div>
</div>
<div id="cell-54" class="cell" data-outputid="90e5e33c-dc7e-4cfa-c440-20e22a15da33">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Output configuration</span></span>
<span id="cb31-2">infer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> loaded.signatures[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"serving_default"</span>]</span>
<span id="cb31-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(infer.structured_outputs)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{'class_index': TensorSpec(shape=(None,), dtype=tf.int64, name='class_index')}</code></pre>
</div>
</div>
<p>Let’s finally run the inference!</p>
<div id="cell-56" class="cell">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Load the sample image</span></span>
<span id="cb33-2">image_pixels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cv2.imread(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sample_image.png"</span>)</span>
<span id="cb33-3">image_pixels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cv2.cvtColor(image_pixels, cv2.COLOR_BGR2GRAY)</span></code></pre></div>
</div>
<div id="cell-57" class="cell" data-outputid="3d29ef57-803c-4b96-aa37-b712982a822f">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Run inference</span></span>
<span id="cb34-2">CLASSES[infer(tf.constant(image_pixels))[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"class_index"</span>].numpy()[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>'Ankle boot'</code></pre>
</div>
</div>
<p>We can see that the prediction is correct in this case as well. So, when we ran <code>infer = loaded.signatures["serving_default"]</code> we essentially loaded a <a href="https://www.tensorflow.org/guide/concrete_function">concrete function</a> i.e.&nbsp;we loaded <code>my_serve</code>. Remember we assigned the value of <code>serving_default</code> in the beginning of this section?</p>
<p>With <code>infer(tf.constant(image_pixels))</code> we are simply running our input image through the concrete function and we are parsing the output from the dictionary (<code>class_index</code> being the key) it returns .</p>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/01_MNIST_TPU_Keras.ipynb">MNIST on TPU (Tensor Processing Unit) or GPU using tf.Keras and tf.data.Dataset</a></li>
<li><a href="https://www.tensorflow.org/guide/saved_model#exporting_custom_models">Using the SavedModel format</a></li>
</ul>


</section>

 ]]></description>
  <guid>https://sayak.dev/posts/2020-04-13-embedding-image-preprocessing-functions.html</guid>
  <pubDate>Mon, 13 Apr 2020 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
