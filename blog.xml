<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Sayak Paul</title>
<link>https://sayak.dev/blog.html</link>
<atom:link href="https://sayak.dev/blog.xml" rel="self" type="application/rss+xml"/>
<description>Personal site of Sayak Paul.</description>
<generator>quarto-1.8.24</generator>
<lastBuildDate>Mon, 11 Aug 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>Optimizing the Full Stack: Thoughts with Image and Video Generation Models</title>
  <link>https://sayak.dev/posts/flow-optim.html</link>
  <description><![CDATA[ 






<p>For the past few years, the space of synthetic image and video generation has been on the rise. The results have been nothing short of extraordinary, and they continue to get better. At the center of this revolution lies a class of models – flow-matching – known for its unique framework to connect noise to data through a straight line<sup>1</sup> <span class="citation" data-cites="lipman2023flow">[1]</span>.</p>
<p>Typically, these generative models take different forms of input conditions, and natural language is perhaps the most popular one. Being able to generate images and videos just from natural language descriptions is liberating. You may already recall some popular models / organizations in this line: DALL-E 3, Stable Diffusion, Flux, Pika, Midjourney, etc. You may also know them to be “diffusion models” <span class="citation" data-cites="NEURIPS2020_4c5bcfec">[2]</span>. Flow-matching subsumes diffusion as a special case <span class="citation" data-cites="lipman2023flow">[1]</span>. This means that the principles and optimizations discussed for flow-based models are often directly applicable to diffusion models, as they represent a specific instance within the broader flow-matching framework.</p>
<p>Unlike GANs (Generative Adversarial Networks) <span class="citation" data-cites="NIPS2014_f033ed80">[3]</span>, these models are not one-shot. They are typically invoked multiple times over a fixed number of iterations to reach a reasonable output. By design, these steps cannot be parallelized<sup>2</sup>. Therefore, despite extremely convincing results, these models are believed to be notoriously difficult to optimize when it comes to serving.</p>
<p>These models often become standalone practical applications or become a part of some larger application. They become a part of the user experience. For instance, this involves analyzing trade-offs like whether a faster generation speed better serves a user with slightly lower quality, or if the use case demands the highest possible quality even at the cost of higher latency and expense. It also includes considering whether to offer different tiers of models—from small and fast to larger and slower — to best match a user’s specific needs and budget. We discuss perspectives on optimizing these models by keeping such user-facing decisions at the center.</p>
<p>We start by looking at the steps involved in a standard text-to-image generation pipeline. We then analyze the memory and latency costs to build the ground for optimization. We then dive into different approaches that can be taken to not only optimize the speed-memory trade-offs but also the user experiences surrounding these models. Since the post doesn’t account for the fundamentals of flow-matching or the classes of models that implement it, readers are expected to have some level of familiarity with diffusion or flow-matching family of models. <a href="https://www.youtube.com/watch?v=DDq_pIfHqLs">This short video</a> does a great job of introducing the topics.</p>
<p>We will discuss most of the approaches by keeping image generation in mind. Unless explicitly specified, these methods should also apply to video generation. Similarly, the approaches are applicable to both flow and diffusion models. Additionally, we will focus on open models, as closed-source models like Veo and Sora already come with optimized user experiences. This focus allows us to concretely analyze their individual components, reference different strategies, and explore how these techniques can be combined into a more holistic optimization process.</p>
<section id="skeleton-of-a-common-generation-pipeline" class="level2">
<h2 class="anchored" data-anchor-id="skeleton-of-a-common-generation-pipeline">Skeleton of a common generation pipeline</h2>
<div id="fig-overview" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/optimizing_flow/image1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Outline of the text-to-image generation process with Flux.1-Dev.
</figcaption>
</figure>
</div>
<p>Unlike large-language models (LLMs), modern image or video generation models are never single models. They are composed of multiple models. For example, the Flux model <span class="citation" data-cites="blackforestlabs2024announcing">[4]</span> we see in Figure 1 is composed of two text encoders, a Transformer-based <span class="citation" data-cites="NIPS2017_3f5ee243 pmlr-v235-esser24a">[5, 6]</span> flow model i.e., the Flux Transformer, and a decoder.</p>
<p>In the case of text-to-image generation, we first embed the input prompt with text encoder(s). The prompt embeddings and an initial noisy latents (drawn from a Gaussian distribution) become the inputs to the flow model, responsible for denoising the noisy latents iteratively. The flow model is also conditioned on the current iteration it is operating on. Once it is done with the entire course of the iterations, the refined latents are passed to the decoder to obtain the final image pixels <span class="citation" data-cites="Rombach_2022_CVPR">[7]</span>.</p>
<p>Below is the memory footprint of these individual model-level components involved in Flux:</p>
<ul>
<li>Text encoders
<ul>
<li>T5-XXL: 8.87 GB<br>
</li>
<li>CLIP-L: 0.229 GB<br>
</li>
</ul></li>
<li>Transformer: 22.168 GB<br>
</li>
<li>Decoder: 0.156 GB</li>
</ul>
<p>It is worth mentioning that amongst these models, the Transformer (the flow model that is the crux of the entire pipeline) is the most compute-hungry one. Most of the computation in this form of text-to-image generation is spent on this flow model. Unlike LLMs, these models are compute-bound. This means their speed is primarily limited by the ability to perform iterative calculations on high-dimensional spaces representing images, videos, or their latents. This is different from LLMs, which are often memory-bottlenecked from loading the massive weights of their text-based Transformer architectures. Consequently, applying optimization techniques designed for memory-intensive LLMs to compute-bound generative models may yield suboptimal results or even none at all, highlighting the need for tailored strategies. Unless otherwise specified, all optimization techniques discussed are for this flow model.</p>
<p>For image generation, when using the Bfloat16 data-type and placing all these components on the hardware accelerator memory, it takes about 33.828 GBs to go from a prompt to a 1024x1024 image. In terms of generation speed, on an H100 GPU, it is ~7 seconds.</p>
<p>Taking the example of an open and high-quality video model like Wan 2.1 (14B) <span class="citation" data-cites="wan2.1">[8]</span>, the timing and memory get even worse. A 5-second (16 FPS) 720P video takes ~30 minutes to generate.</p>
<p>We’re operating locally. With decent models, each image takes about 7 seconds to generate, and each video takes 30 minutes! If these models were to be operationalized, their generation speed most definitely needs to be improved quite a bit so that they can deliver seamless user experiences.</p>
<p>However, is it just that? I.e., we improve generation speed without sacrificing quality, and we’re done? What can there possibly be beyond this factor? If you have made it this far, thank you! We’re going to find that out next and work our way from there.</p>
<p>Figure&nbsp;2 provides an overview of the different themes we will address.</p>
<div id="fig-techniques" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-techniques-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/optimizing_flow/image2.png" class="img-fluid figure-img" style="width:75.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-techniques-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Different axes of optimization in generative diffusion-based models for images and videos. SLA refers to service-level agreement, which pushes the axes up and right, i.e., fast and reasonably good.
</figcaption>
</figure>
</div>
</section>
<section id="selecting-the-model" class="level2">
<h2 class="anchored" data-anchor-id="selecting-the-model">Selecting the model</h2>
<p>We know the use case(s) we want to serve, but we haven’t settled on a model. This can refer to the base model architecture itself or to different parameterizations of the same base model architecture. This can also refer to selecting a pre-trained checkpoint for a given model architecture. As we will see, selection of a model is a non-trivial aspect of the workflow, and when done correctly, can be quite beneficial. Therefore, unless explicitly specified, the approaches discussed in this section will apply to both training and inference.</p>
<section id="hardware-awareness" class="level3">
<h3 class="anchored">Hardware awareness</h3>
<p>Assuming we know the serving hardware, it makes sense to incorporate <em>hardware awareness</em> while developing the model architecture to maximize throughput while optimizing for quality.</p>
<p>In ModernBERT <span class="citation" data-cites="warner2024smarterbetterfasterlonger">[9]</span>, for example, the authors decided on the dimensions (number of attention heads, number of Transformer blocks, hidden dimension, and expansion factor) of the Transformer block in a way that provided a good balance between downstream performance and hardware utilization.</p>
<p>One way to think about this is by starting with the specifications of the hardware. For example, if the given GPU has tensor cores and if we want to leverage them (and we should), the size of each dimension weight matrix should be a multiple of 64.</p>
<p>Then there is tiling, wherein the iteration space of computation and data is chunked into small and fixed-sized “tiles” so that they can be operated on in parallel by the streaming multiprocessors<sup>3</sup> (SM). If the data cannot be partitioned evenly with respect to the number of processors available, performance can be suboptimal. In ModernBERT, the Transformer block dimensions were also chosen to realize efficient tiling across the number of SMs available. When a pool of different hardware is available (such as various types of GPUs), it makes sense to design an architecture that maximizes hardware utilization collectively. Anthony et al.&nbsp;provide an excellent study on the math behind designing optimal model configurations for available hardware.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Several other works have also used neural architecture search for hardware-aware inference-optimized model design <span class="citation" data-cites="Xiong_2021_CVPR">[10]</span>, <span class="citation" data-cites="anthony2024casecodesigningmodelarchitectures">[11]</span>, and <span class="citation" data-cites="bercovich2025puzzledistillationbasednasinferenceoptimized">[12]</span>. …</p>
<p><strong>A note on efficiency</strong><br>
Efficiency is an important criterion when navigating across this whole spectrum of hardware-aware model architecture design. As studied in various works <span class="citation" data-cites="10.5555/3600270.3602446">[13]</span>, it is possible that the compute-optimal model for a given dataset is smaller than the one currently being used. However, it could also require more training. If the compute-optimal model for the given dataset is smaller, then it could be beneficial from the perspective of efficiency.</p>
<p>It is common to think that small models are more efficient than larger models. However, what is efficiency in this context? Is it the carbon footprint of a model? Is it the memory consumption of a model? Do models with fewer parameters obtain better throughput than models with more parameters?</p>
<p>As thoroughly studied by <span class="citation" data-cites="dehghani2022efficiencymisnomer">[14]</span>, there is no clear trend, as Figure&nbsp;3 clearly illustrates.</p>
<div id="fig-efficiency-misnomer" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-efficiency-misnomer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/optimizing_flow/image3.png" class="img-fluid figure-img" style="width:88.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-efficiency-misnomer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Taken from <a href="https://arxiv.org/abs/2110.12894">Dehghani et al.</a> For particular classes of models, a larger number of parameters doesn’t necessarily mean more FLOPs and better throughput. More examples are available in the paper.
</figcaption>
</figure>
</div>
<p>Therefore, when deriving the efficiency of the architecture, always prefer obtaining three metrics: number of parameters, FLOP, and throughput. In the context of optimization:</p>
<ul>
<li>The <em>number of parameters</em> usually correlates heavily with the memory footprint of the model.<br>
</li>
<li><em>FLOPs</em> provide an idea of the computational costs.<br>
</li>
<li><em>Throughput</em> dictates the real-world performance.</li>
</ul>
<section id="architectural-flexibility" class="level3">
<h3 class="anchored" data-anchor-id="architectural-flexibility">Architectural flexibility</h3>
<p>A more first-principles approach toward optimization would be to try exploiting the loopholes of the problem at hand. For various image and video generation models, we often operate on the latent space (as shown above). For the purpose of our discussions and also to give a taste of real-world applications, we take the example of high-resolution synthesis of images and videos.</p>
<p>For high-resolution synthesis, the latent space can get very memory-hungry and also latency-intensive. For 4K generation, if we were to perform 8x compression on the latent space, we would have <code>(batch_size, num_latent_channels, 512, 512)</code> dimensional latents. If the underlying application prioritizes real-time generation, then this becomes far from ideal.</p>
<p>Even when not operating with high resolutions, for videos, the problem gets even worse. The outputs are now spatio-temporal. This means we need to compute full 3D attention between tokens<sup>4</sup>. For moderate-sized videos (5 seconds long, 512x768 resolution), we might have to deal with <code>(batch_size, num_latent_channels, num_compressed_temporal_channels, 64, 96)</code> dimensional latents.</p>
<p>Works like LTX-Video <span class="citation" data-cites="hacohen2024ltxvideorealtimevideolatent">[15]</span> and SANA <span class="citation" data-cites="xie2025sana">[16]</span> operate on highly compressed latent spaces, thereby reducing the memory requirements, while also improving latency. While operating on highly compressed latent spaces significantly reduces memory and latency, it’s a critical design choice, as excessive compression can lead to information loss in the representation, consequently impacting the fidelity or detail of the generated output. Both LTX-Video and SANA have their own ways to compensate for that. Amongst other things,</p>
<ul>
<li>LTX-Video tasks the decoder to perform both latent-to-pixel conversion and the final denoising step.<br>
</li>
<li>SANA employs specialized blocks (dubbed Mix-FFN) in its Transformer architecture.</li>
</ul>
<p>One can approach architectural flexibility through a slightly different lens, too. Flux was released as a text-to-image generation model. Later, its creators took the same Flux Transformer architecture and expanded it to incorporate structural inputs (Flux Control <span class="citation" data-cites="blackforestlabs2024flux1tools">[17]</span>) and additional image inputs (Flux Kontext <span class="citation" data-cites="blackforestlabs2025flux1kontextdev">[18]</span>). While Flux Control required a single change at the input embedding layer dealing with the noisy latents, Flux Kontext didn’t require any change at all.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>It should be noted that even though the Flux Transformer architecture went through minimal to no changes, its generation pipeline needed changes. These changes were mostly about connecting the other parts of the pipeline (such as the text encoders, the latent-to-pixel decoder, and the pixel-to-latent encoder).</p>
</div>
</div>
<p>At this point, a flexible model architecture that has been developed in a hardware-aware manner should be an excellent start to guide further the subsequent application optimization processes.</p>
</section>
<section id="model-is-decided-what-is-next" class="level2">
<h2 class="anchored" data-anchor-id="model-is-decided-what-is-next">Model is decided – what is next?</h2>
<p>Once a capable base model is selected, the focus shifts from general performance to optimizing for the specific <strong>use case</strong> — that is, tailoring the model’s behavior to the practical context in which it will be served. This means looking beyond standard benchmarks to enhance the qualities that are the most relevant for the application’s success.</p>
<p>For example, imagine a model that already performs well on standard text-to-image generation benchmarks. If the use case is creating photorealistic marketing images, the goal would be to improve specific attributes like photorealism and text-to-image alignment. Conversely, if the use case is an interactive avatar generator, the most critical factor might be real-time interaction, demanding the lowest possible latency.</p>
<p>In this section, we look at some approaches to identifying and fine-tuning for the specific demands of an application, i.e., <em>optimizing the use case</em>.</p>
<section id="post-training" class="level3">
<h3 class="anchored" data-anchor-id="post-training">Post-training</h3>
<p>Despite all the standard metrics available for image (or video) generation, in order for a use case to grow, it is quite important to have evaluation metrics centered around the use case. For the above example, we would want to particularly look for metrics that faithfully cover photorealism and text-to-image alignment. If preference data can be obtained, it could also be beneficial to do a round of preference learning <span class="citation" data-cites="black2023ddpo Wallace_2024_CVPR">[19, 20]</span> and to see if that helps in further improvements.</p>
<p>Preference datasets can also be used for supervised fine-tuning (SFT) since we have an understanding of which image is “preferred” given a prompt. We can take our base model and fine-tune on the pairs of prompts and the preferred images.</p>
<p>Preference learning leverages human feedback on model outputs to guide further training towards preferred styles or qualities, whereas supervised fine-tuning (SFT) uses curated datasets of prompt-output pairs to directly teach the model desired behaviors. However, when to use what, i.e., typical preference learning or SFT, is still very much an open question.</p>
<p>Note that post-training in these models can also come in other ways, such as ControlNets <span class="citation" data-cites="zhang2023addingconditionalcontroltexttoimage">[21]</span>, and they deserve a separate post on their own.</p>
</section>
<section id="latency-optimization" class="level3">
<h3 class="anchored" data-anchor-id="latency-optimization">Latency optimization</h3>
<p>Before the model meets with actual deployment, it typically goes through some kind of latency-based optimization. Additionally, these techniques also amortize the long training durations over the course of serving a model. Examples of these techniques include compilation, integration of any specialized kernels to target input shapes and the available hardware, use of exotic parallelism techniques, and many more. Some optimization would be inference-only (post-training quantization, for example), while some would apply to both training and inference (flash-attention <span class="citation" data-cites="dao2022flashattentionfastmemoryefficientexact">[22]</span>, for example).</p>
<p>Many optimization techniques in this regard would be quite hardware-dependent. For example, Flash Attention 3 <span class="citation" data-cites="shah2024flashattention3fastaccurateattention">[23]</span> is currently only supported for the Hopper GPU architecture, while the FP8 dynamic quantization scheme needs GPUs with a compute capability of at least 8.9. So, we may now appreciate why keeping hardware awareness in mind can be truly helpful.</p>
<p>It is also a good exercise to have an estimate of the theoretical throughput possible for the model with sample inputs and the available hardware. This can then be used to inform the optimization process in this stage if the realized throughput is significantly lower than the theoretical one.</p>
<p>Distillation is another popular way to optimize latency. We discuss distillation in a later section of the post.</p>
</section>
<section id="inference-time-scaling" class="level3">
<h3 class="anchored" data-anchor-id="inference-time-scaling">Inference-time scaling</h3>
<p>If training is out of scope, inference-time scaling <span class="citation" data-cites="Ma_2025_CVPR zhuo2025reflectionperfectionscalinginferencetime">[24, 25]</span> could be another promising avenue to explore. We scale the compute during inference by “searching” for better outputs, leading to potentially improved metrics (prompt following ability, for example) of choice. But what do we search for?</p>
<p>Recollect that during inference, flow models start with a random Gaussian noise, which is denoised over a few iterations. We can always search for a better noise at initialization and see what leads to better quality outputs, as different initial noise seeds can lead to significantly varied final outputs, and optimizing this starting point can guide the generation process towards higher quality results. If the search plays out well, it might even be possible to use a smaller model with inference-time scaling to offset the costs of serving a much larger model <span class="citation" data-cites="singhal2025generalframeworkinferencetimescaling">[26]</span>.</p>
</section>
<section id="prompting" class="level3">
<h3 class="anchored" data-anchor-id="prompting">Prompting</h3>
<p>Multiple works <span class="citation" data-cites="betker2023improvingimagegenerationwithbettercaptions segalis2023pictureworththousandwords liu2024playgroundv3improvingtexttoimage">[27–29]</span> have shown that better captions can also lead to improved outputs. What accounts for a “better” caption is highly use case dependent, but there are some general guidelines:</p>
<ul>
<li>What is the image medium? Is it a photo, a painting, a 3D illustration, or something else?<br>
</li>
<li>What is the image subject? Is it a person, animal, object, or scene?<br>
</li>
<li>What details would you like to see in the image?</li>
</ul>
<p>When we cannot expect highly detailed captions from the users, a specialized captioner model could be used to turn short user prompts into highly detailed ones. Figure&nbsp;4 provides a comparative example of the outputs obtained through a simple and a detailed prompt.</p>
<div id="fig-prompting" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-prompting-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/optimizing_flow/image4.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-prompting-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: An example of using detailed prompts as opposed to using shallow prompts.
</figcaption>
</figure>
</div>
<p>This idea of using detailed prompts to potentially improve the output quality is often referred to as “caption upsampling” <span class="citation" data-cites="betker2023improvingimagegenerationwithbettercaptions">[27]</span>. To benefit from using caption upsampling, the assumption is that the model was shown similar data during training <span class="citation" data-cites="segalis2023pictureworththousandwords 10.1007/978-3-031-72670-5_12">[28, 30]</span>. Caption upsampling could also be considered an inference-time scaling technique, wherein we start with a seed user prompt and gradually improve it until a threshold of a desired metric is met.</p>
<p>Among the two broad approaches (post-training and inference-time scaling) discussed in this section, it remains unclear when to use each. Can we even combine post-training and inference-time scaling for these kinds of generative models? Are these two things complementary to one another? This is still an open question.</p>
<p>So far, in the previous sections, we covered architectural choices, post-training, and inference-time tweaks. Distillation offers a path to create fundamentally faster models by learning from larger, more powerful ones.</p>
</section>
</section>
<section id="advanced-model-optimization-distillation" class="level2">
<h2 class="anchored" data-anchor-id="advanced-model-optimization-distillation">Advanced model optimization – Distillation</h2>
<p>Previously, we noted that distillation is a popular way to optimize latency. It is a powerful technique that deserves a closer look, as it allows us to create smaller and faster models by transferring knowledge from a larger ‘teacher’ model to a compact ‘student’ model. This process directly tackles the speed-memory-performance trade-off and comes in two primary forms for the class of models we have been discussing.</p>
<section id="architectural-compression" class="level3">
<h3 class="anchored" data-anchor-id="architectural-compression">Architectural compression</h3>
<p>Using distillation to compress a larger model into a smaller one dates back to 2015 through the paper <span class="citation" data-cites="hinton2015distillingknowledgeneuralnetwork">[31]</span>. We want a (smaller) “student” model to mimic the output of another (usually larger) “teacher” model.</p>
<p>When distilling the teacher model into a student model, ideally, we need to have access to the training dataset of the teacher model. However, in reality, it may not always be the case, especially when the consumers of the teacher model are not the ones who created it. This is where a significant effort might be needed to create a good dataset for distillation. If the samples draw too far away from the ones used to train the teacher, distillation could even be detrimental. If this becomes a dire problem, then fine-tuning the teacher model on the available dataset for distillation before the actual distillation process could be beneficial <span class="citation" data-cites="sreenivas2024llmpruningdistillationpractice">[32]</span>. This phase is often known as “teacher correction”.</p>
<p>A distilled model could be slightly worse than its teacher, but it could be significantly more memory-efficient and faster than the teacher. This could be particularly beneficial when model-serving resources are limited. Distilled models, in the premise of image and video generation, could be leveraged for real-time use cases. They could even be used as a proxy for the quality that users can expect. For example, during the first round of incoming requests, an application could show outputs from a distilled model. If the users are satisfied with the outputs, then we reduce costs by not invoking the larger model.</p>
</section>
<section id="timestep-distillation" class="level3">
<h3 class="anchored" data-anchor-id="timestep-distillation">Timestep distillation</h3>
<p>Flow models take a number of steps to denoise to provide a reasonable output. However, too many steps can get in the way of use cases that benefit from instantaneity. A number of techniques <span class="citation" data-cites="salimans2022progressivedistillationfastsampling yin2024improveddistributionmatchingdistillation song2023consistencymodels chen2025sanasprintonestepdiffusioncontinuoustime">[33–36]</span> have emerged to tackle this problem, and together, they’re known as “timestep distillation”. Timestep distilled models aim at reducing the number of steps it takes to obtain reasonable results.</p>
<p>The teacher model, being used to guide the distillation process, can still be superior to the distilled model in terms of quality. Hence, the same two-model philosophy discussed just above applies to timestep-distilled models, too. In the literature of flow models, one can also combine timestep distillation with architectural compression through classic distillation to take the best of both worlds <span class="citation" data-cites="song2024sdxsrealtimeonesteplatent">[37]</span>.</p>
<p>It is worth pointing out that distillation only becomes viable when we have a sufficiently well-performing teacher model. So, the techniques discussed above won’t be eliminated by distillation at all. Additionally, most of the techniques discussed above would be complementary to using distillation.</p>
<p>Guidance or more broadly, “classifier-free guidance” (CFG) <span class="citation" data-cites="ho2022classifierfreediffusionguidance">[38]</span>, is a vital component of flow-based generative models. It is used to steer the model output more towards the direction of the input conditions (such as text prompts), improving the overall output quality. The disadvantage is that we need two model forward passes to make CFG work. For a few iterations, this can add significant overhead to both memory consumption and inference latency. Therefore, guidance can also be distilled <span class="citation" data-cites="meng2023distillationguideddiffusionmodels">[39]</span> into a student model from a teacher trained with CFG. It can also be further combined with timestep distillation, providing both memory and latency benefits.</p>
<p>Timestep distillation or guidance distillation is usually done by fully fine-tuning a base model. Some works have explored the use of LoRA <span class="citation" data-cites="ren2024hypersdtrajectorysegmentedconsistency">[40]</span> in this regard. This path offers a cheaper alternative to full fine-tuning while still retaining the core benefits of such distillation mechanisms.</p>
</section>
</section>
<section id="generation-speed-the-endgame" class="level2">
<h2 class="anchored" data-anchor-id="generation-speed-the-endgame">Generation speed: The endgame(?)</h2>
<p>What is optimization in the context of image or video generation models? Is it just improving inference latency when user experiences are also considered?</p>
<p>Well, probably not. It is a no-brainer to aim for a model that is fast, performant, and memory-efficient. However, this speed-memory-performance trade-off is governed by the use case and the resources to support it. Below is a non-exhaustive list of the aspects that become apparent in this regard:</p>
<ul>
<li>What is the expected SLA around latency for the use case being served?<br>
</li>
<li>What is our current traffic? Do we have enough hardware accelerators to support that traffic while meeting the expected SLA?<br>
</li>
<li>Can we quantify model performance and tie it to an improvement in the use case? We could optimize specifically for those aspects. For example, if the use case primarily benefits from good text rendering abilities, the design decisions would be devised differently from those mainly optimizing fine-grained color control.<br>
</li>
<li>Do users always want the best-quality images/videos?<br>
</li>
<li>Does providing a little less with a better latency still satisfy users, especially when it could cost much less? OpenAI’s serving model is a great example here. They have different tiers of models, from small and fast to larger and slower ones. Each of them comes with different price points, with small models costing less and larger models costing more. If a small model can perform the user task well, then you also end up serving the user well, but at a lower cost.</li>
</ul>
<p>Whatever end model we end up with, we still want speed, though. That can never be out of place. Hopefully, this section has convinced you that while speed is paramount, there are other aspects worth considering.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>We took a deeper look into what it means to optimize image and video generation models and their use cases. We covered a couple of model-level approaches while also focusing on how to go beyond them, taking the use cases at the center. Throughout the post, the following theme became apparent: optimizing a model and optimizing its use cases are quite intertwined. Since we touched upon various connected components in the mix, below are some key points:</p>
<ul>
<li>Incorporate hardware awareness while designing the model architecture. For example, select matrix multiples of 64 and minimise tiling overhead<br>
</li>
<li>Chase architectural flexibility for greater future-proofing: consider the kinds of use cases you want to serve, their inputs, and expected outcomes; incorporate these aspects into the architecture design process.<br>
</li>
<li>Complement architectural benefits with latency optimization techniques, as these are often a free lunch.<br>
</li>
<li>Spend time optimizing for use cases, either through post-training or inference-time scaling or both.<br>
</li>
<li>If the use case demands it, operationalize distillation, either at the architectural level or at the timestep level, or both.</li>
</ul>
<p><em>Acknowledgements: Thanks to Sanchit Gandhi and Sander Dieleman for their reviews on the early post draft.</em></p>


</section>
</div>
</div>

</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body">
<div id="ref-lipman2023flow" class="csl-entry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Lipman Y, Chen RTQ, Ben-Hamu H, Nickel M, Le M (2023) <a href="https://openreview.net/forum?id=PqvMRDCJT9t">Flow matching for generative modeling</a>. In: The eleventh international conference on learning representations</div>
</div>
<div id="ref-NEURIPS2020_4c5bcfec" class="csl-entry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Ho J, Jain A, Abbeel P (2020) <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf">Denoising diffusion probabilistic models</a>. In: Larochelle H, Ranzato M, Hadsell R, Balcan MF, Lin H (eds) Advances in neural information processing systems. Curran Associates, Inc., pp 6840–6851</div>
</div>
<div id="ref-NIPS2014_f033ed80" class="csl-entry">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">Goodfellow IJ, Pouget-Abadie J, Mirza M, et al (2014) <a href="https://proceedings.neurips.cc/paper_files/paper/2014/file/f033ed80deb0234979a61f95710dbe25-Paper.pdf">Generative adversarial nets</a>. In: Ghahramani Z, Welling M, Cortes C, Lawrence N, Weinberger KQ (eds) Advances in neural information processing systems. Curran Associates, Inc.</div>
</div>
<div id="ref-blackforestlabs2024announcing" class="csl-entry">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">Black Forest Labs (2024) Announcing black forest labs. <a href="https://blackforestlabs.ai/announcing-black-forest-labs/">https://blackforestlabs.ai/announcing-black-forest-labs/</a></div>
</div>
<div id="ref-NIPS2017_3f5ee243" class="csl-entry">
<div class="csl-left-margin">5. </div><div class="csl-right-inline">Vaswani A, Shazeer N, Parmar N, et al (2017) <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention is all you need</a>. In: Guyon I, Luxburg UV, Bengio S, et al (eds) Advances in neural information processing systems. Curran Associates, Inc.</div>
</div>
<div id="ref-pmlr-v235-esser24a" class="csl-entry">
<div class="csl-left-margin">6. </div><div class="csl-right-inline">Esser P, Kulal S, Blattmann A, et al (2024) <a href="https://proceedings.mlr.press/v235/esser24a.html">Scaling rectified flow transformers for high-resolution image synthesis</a>. In: Salakhutdinov R, Kolter Z, Heller K, et al (eds) Proceedings of the 41st international conference on machine learning. PMLR, pp 12606–12633</div>
</div>
<div id="ref-Rombach_2022_CVPR" class="csl-entry">
<div class="csl-left-margin">7. </div><div class="csl-right-inline">Rombach R, Blattmann A, Lorenz D, Esser P, Ommer B (2022) High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR). pp 10684–10695</div>
</div>
<div id="ref-wan2.1" class="csl-entry">
<div class="csl-left-margin">8. </div><div class="csl-right-inline">Team W (2025) Wan: Open and advanced large-scale video generative models</div>
</div>
<div id="ref-warner2024smarterbetterfasterlonger" class="csl-entry">
<div class="csl-left-margin">9. </div><div class="csl-right-inline">Warner B, Chaffin A, Clavié B, et al (2024) <a href="https://arxiv.org/abs/2412.13663">Smarter, better, faster, longer: A modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference</a></div>
</div>
<div id="ref-Xiong_2021_CVPR" class="csl-entry">
<div class="csl-left-margin">10. </div><div class="csl-right-inline">Xiong Y, Liu H, Gupta S, et al (2021) MobileDets: Searching for object detection architectures for mobile accelerators. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR). pp 3825–3834</div>
</div>
<div id="ref-anthony2024casecodesigningmodelarchitectures" class="csl-entry">
<div class="csl-left-margin">11. </div><div class="csl-right-inline">Anthony Q, Hatef J, Narayanan D, et al (2024) <a href="https://arxiv.org/abs/2401.14489">The case for co-designing model architectures with hardware</a></div>
</div>
<div id="ref-bercovich2025puzzledistillationbasednasinferenceoptimized" class="csl-entry">
<div class="csl-left-margin">12. </div><div class="csl-right-inline">Bercovich A, Ronen T, Abramovich T, et al (2025) <a href="https://arxiv.org/abs/2411.19146">Puzzle: Distillation-based NAS for inference-optimized LLMs</a></div>
</div>
<div id="ref-10.5555/3600270.3602446" class="csl-entry">
<div class="csl-left-margin">13. </div><div class="csl-right-inline">Hoffmann J, Borgeaud S, Mensch A, et al (2022) Training compute-optimal large language models. In: Proceedings of the 36th international conference on neural information processing systems. Curran Associates Inc., Red Hook, NY, USA</div>
</div>
<div id="ref-dehghani2022efficiencymisnomer" class="csl-entry">
<div class="csl-left-margin">14. </div><div class="csl-right-inline">Dehghani M, Arnab A, Beyer L, Vaswani A, Tay Y (2022) <a href="https://arxiv.org/abs/2110.12894">The efficiency misnomer</a></div>
</div>
<div id="ref-hacohen2024ltxvideorealtimevideolatent" class="csl-entry">
<div class="csl-left-margin">15. </div><div class="csl-right-inline">HaCohen Y, Chiprut N, Brazowski B, et al (2024) <a href="https://arxiv.org/abs/2501.00103">LTX-video: Realtime video latent diffusion</a></div>
</div>
<div id="ref-xie2025sana" class="csl-entry">
<div class="csl-left-margin">16. </div><div class="csl-right-inline">Xie E, Chen J, Chen J, et al (2025) <a href="https://openreview.net/forum?id=N8Oj1XhtYZ"><span>SANA</span>: Efficient high-resolution text-to-image synthesis with linear diffusion transformers</a>. In: The thirteenth international conference on learning representations</div>
</div>
<div id="ref-blackforestlabs2024flux1tools" class="csl-entry">
<div class="csl-left-margin">17. </div><div class="csl-right-inline">Black Forest Labs (2024) Introducing FLUX.1 tools. <a href="https://bfl.ai/announcements/24-11-21-tools">https://bfl.ai/announcements/24-11-21-tools</a></div>
</div>
<div id="ref-blackforestlabs2025flux1kontextdev" class="csl-entry">
<div class="csl-left-margin">18. </div><div class="csl-right-inline">Black Forest Labs (2025) FLUX.1 kontext [dev] – open weights for image editing. <a href="https://bfl.ai/announcements/flux-1-kontext-dev">https://bfl.ai/announcements/flux-1-kontext-dev</a></div>
</div>
<div id="ref-black2023ddpo" class="csl-entry">
<div class="csl-left-margin">19. </div><div class="csl-right-inline">Black K, Janner M, Du Y, Kostrikov I, Levine S (2023) <a href="https://arxiv.org/abs/2305.13301">Training diffusion models with reinforcement learning</a></div>
</div>
<div id="ref-Wallace_2024_CVPR" class="csl-entry">
<div class="csl-left-margin">20. </div><div class="csl-right-inline">Wallace B, Dang M, Rafailov R, et al (2024) Diffusion model alignment using direct preference optimization. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR). pp 8228–8238</div>
</div>
<div id="ref-zhang2023addingconditionalcontroltexttoimage" class="csl-entry">
<div class="csl-left-margin">21. </div><div class="csl-right-inline">Zhang L, Rao A, Agrawala M (2023) <a href="https://arxiv.org/abs/2302.05543">Adding conditional control to text-to-image diffusion models</a></div>
</div>
<div id="ref-dao2022flashattentionfastmemoryefficientexact" class="csl-entry">
<div class="csl-left-margin">22. </div><div class="csl-right-inline">Dao T, Fu DY, Ermon S, Rudra A, Ré C (2022) <a href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and memory-efficient exact attention with IO-awareness</a></div>
</div>
<div id="ref-shah2024flashattention3fastaccurateattention" class="csl-entry">
<div class="csl-left-margin">23. </div><div class="csl-right-inline">Shah J, Bikshandi G, Zhang Y, Thakkar V, Ramani P, Dao T (2024) <a href="https://arxiv.org/abs/2407.08608">FlashAttention-3: Fast and accurate attention with asynchrony and low-precision</a></div>
</div>
<div id="ref-Ma_2025_CVPR" class="csl-entry">
<div class="csl-left-margin">24. </div><div class="csl-right-inline">Ma N, Tong S, Jia H, et al (2025) Scaling inference time compute for diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR). pp 2523–2534</div>
</div>
<div id="ref-zhuo2025reflectionperfectionscalinginferencetime" class="csl-entry">
<div class="csl-left-margin">25. </div><div class="csl-right-inline">Zhuo L, Zhao L, Paul S, et al (2025) <a href="https://arxiv.org/abs/2504.16080">From reflection to perfection: Scaling inference-time optimization for text-to-image diffusion models via reflection tuning</a></div>
</div>
<div id="ref-singhal2025generalframeworkinferencetimescaling" class="csl-entry">
<div class="csl-left-margin">26. </div><div class="csl-right-inline">Singhal R, Horvitz Z, Teehan R, et al (2025) <a href="https://arxiv.org/abs/2501.06848">A general framework for inference-time scaling and steering of diffusion models</a></div>
</div>
<div id="ref-betker2023improvingimagegenerationwithbettercaptions" class="csl-entry">
<div class="csl-left-margin">27. </div><div class="csl-right-inline">Betker J, Jun H, Ouyang L, Goh G, Ramesh A, et al (2023) <a href="https://cdn.openai.com/papers/dall-e-3.pdf">Improving image generation with better captions</a></div>
</div>
<div id="ref-segalis2023pictureworththousandwords" class="csl-entry">
<div class="csl-left-margin">28. </div><div class="csl-right-inline">Segalis E, Valevski D, Lumen D, Matias Y, Leviathan Y (2023) <a href="https://arxiv.org/abs/2310.16656">A picture is worth a thousand words: Principled recaptioning improves image generation</a></div>
</div>
<div id="ref-liu2024playgroundv3improvingtexttoimage" class="csl-entry">
<div class="csl-left-margin">29. </div><div class="csl-right-inline">Liu B, Akhgari E, Visheratin A, et al (2024) <a href="https://arxiv.org/abs/2409.10695">Playground v3: Improving text-to-image alignment with deep-fusion large language models</a></div>
</div>
<div id="ref-10.1007/978-3-031-72670-5_12" class="csl-entry">
<div class="csl-left-margin">30. </div><div class="csl-right-inline">Chatterjee A, Stan GBM, Aflalo E, et al (2024) <a href="https://doi.org/10.1007/978-3-031-72670-5_12">Getting it right: Improving spatial consistency in&amp;nbsp;text-to-image models</a>. In: Computer vision – ECCV 2024: 18th european conference, milan, italy, september 29 – october 4, 2024, proceedings, part XXII. Springer-Verlag, Berlin, Heidelberg, pp 204–222</div>
</div>
<div id="ref-hinton2015distillingknowledgeneuralnetwork" class="csl-entry">
<div class="csl-left-margin">31. </div><div class="csl-right-inline">Hinton G, Vinyals O, Dean J (2015) <a href="https://arxiv.org/abs/1503.02531">Distilling the knowledge in a neural network</a></div>
</div>
<div id="ref-sreenivas2024llmpruningdistillationpractice" class="csl-entry">
<div class="csl-left-margin">32. </div><div class="csl-right-inline">Sreenivas ST, Muralidharan S, Joshi R, et al (2024) <a href="https://arxiv.org/abs/2408.11796">LLM pruning and distillation in practice: The minitron approach</a></div>
</div>
<div id="ref-salimans2022progressivedistillationfastsampling" class="csl-entry">
<div class="csl-left-margin">33. </div><div class="csl-right-inline">Salimans T, Ho J (2022) <a href="https://arxiv.org/abs/2202.00512">Progressive distillation for fast sampling of diffusion models</a></div>
</div>
<div id="ref-yin2024improveddistributionmatchingdistillation" class="csl-entry">
<div class="csl-left-margin">34. </div><div class="csl-right-inline">Yin T, Gharbi M, Park T, et al (2024) <a href="https://arxiv.org/abs/2405.14867">Improved distribution matching distillation for fast image synthesis</a></div>
</div>
<div id="ref-song2023consistencymodels" class="csl-entry">
<div class="csl-left-margin">35. </div><div class="csl-right-inline">Song Y, Dhariwal P, Chen M, Sutskever I (2023) <a href="https://arxiv.org/abs/2303.01469">Consistency models</a></div>
</div>
<div id="ref-chen2025sanasprintonestepdiffusioncontinuoustime" class="csl-entry">
<div class="csl-left-margin">36. </div><div class="csl-right-inline">Chen J, Xue S, Zhao Y, et al (2025) <a href="https://arxiv.org/abs/2503.09641">SANA-sprint: One-step diffusion with continuous-time consistency distillation</a></div>
</div>
<div id="ref-song2024sdxsrealtimeonesteplatent" class="csl-entry">
<div class="csl-left-margin">37. </div><div class="csl-right-inline">Song Y, Sun Z, Yin X (2024) <a href="https://arxiv.org/abs/2403.16627">SDXS: Real-time one-step latent diffusion models with image conditions</a></div>
</div>
<div id="ref-ho2022classifierfreediffusionguidance" class="csl-entry">
<div class="csl-left-margin">38. </div><div class="csl-right-inline">Ho J, Salimans T (2022) <a href="https://arxiv.org/abs/2207.12598">Classifier-free diffusion guidance</a></div>
</div>
<div id="ref-meng2023distillationguideddiffusionmodels" class="csl-entry">
<div class="csl-left-margin">39. </div><div class="csl-right-inline">Meng C, Rombach R, Gao R, et al (2023) <a href="https://arxiv.org/abs/2210.03142">On distillation of guided diffusion models</a></div>
</div>
<div id="ref-ren2024hypersdtrajectorysegmentedconsistency" class="csl-entry">
<div class="csl-left-margin">40. </div><div class="csl-right-inline">Ren Y, Xia X, Lu Y, et al (2024) <a href="https://arxiv.org/abs/2404.13686">Hyper-SD: Trajectory segmented consistency model for efficient image synthesis</a></div>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Or maybe <a href="https://diffusionflow.github.io/">not</a>.↩︎</p></li>
<li id="fn2"><p>Parallel strategies <a href="https://huggingface.co/papers/2305.16317">exist</a> but they are not used in practice very much.↩︎</p></li>
<li id="fn3"><p>This is specific to shared-memory GPU architectures.↩︎</p></li>
<li id="fn4"><p>Sparser variants exist but they often lead to sub-optimal results.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>diffusion</category>
  <category>flow-matching</category>
  <category>optimization</category>
  <guid>https://sayak.dev/posts/flow-optim.html</guid>
  <pubDate>Mon, 11 Aug 2025 00:00:00 GMT</pubDate>
  <media:content url="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/optimizing_flow/header.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Flavors of attention in modern diffusion models</title>
  <link>https://sayak.dev/posts/attn-diffusion.html</link>
  <description><![CDATA[ 






<p>Attention is a crucial component in generative neural architectures for continuous modalities like images and videos from natural language. More specifically, cross-attention helps to contextualize the relationship between the natural language prompt inputs and the media, being generated.</p>
<p>With modern diffusion models (or shall we say “flow”) for condition-guided image and video generation, we saw the community going beyond cross-attention. For example, Stable Diffusion 3 (SD 3) <span class="citation" data-cites="esser2024scalingrectifiedflowtransformers">[1]</span> introduced “joint-attention” in its MMDiT architecture. SANA <span class="citation" data-cites="xie2024sanaefficienthighresolutionimage">[2]</span>, on the other hand, introduced a variant of “linear attention”, moving away from the standard attention mechanism.</p>
<p>While the changes between these variants may appear architecturally simple, it can be helpful to understand the factors that distinguish them. In this post, we will investigate the popular forms of attention blocks used in modern diffusion models. We will tear them apart with simple PyTorch code and comment on some additional findings.</p>
<p>Readers are expected to be familiar with diffusion-based image generation models and encoder-based transformer architectures.</p>
<div align="center">
<img src="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/attn_diffusion/llama_header.png" width="700/">
<p><small>Image generated with <a href="https://huggingface.co/black-forest-labs/FLUX.1-dev">Flux.1-Dev</a>.</small></p>
</div>
<section id="self-attention" class="level2">
<h2 class="anchored" data-anchor-id="self-attention">Self-attention</h2>
<p>For the sake of completeness, let’s take a quick look at how self-attention is implemented. This will help us understand how it can be evolved to cross-attention and others.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x, attn_mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb1-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># x shape: (batch_size, seq_length, embed_dim)</span></span>
<span id="cb1-3">    bsz, seq_length, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x.size()</span>
<span id="cb1-4">    </span>
<span id="cb1-5">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute queries, keys, and values using separate linear layers</span></span>
<span id="cb1-6">    q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_q(x)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># shape: (batch_size, seq_length, embed_dim)</span></span>
<span id="cb1-7">    k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_k(x)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># shape: (batch_size, seq_length, embed_dim)</span></span>
<span id="cb1-8">    v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_v(x)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># shape: (batch_size, seq_length, embed_dim)</span></span>
<span id="cb1-9">    </span>
<span id="cb1-10">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Reshape and transpose to get dimensions </span></span>
<span id="cb1-11">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (batch_size, num_heads, seq_length, head_dim)</span></span>
<span id="cb1-12">    q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> q.view(bsz, seq_length, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_heads, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb1-13">    k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> k.view(bsz, seq_length, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_heads, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb1-14">    v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> v.view(bsz, seq_length, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_heads, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb1-15">    </span>
<span id="cb1-16">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute scaled dot product attention using PyTorch's built-in function</span></span>
<span id="cb1-17">    attn_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> F.scaled_dot_product_attention(</span>
<span id="cb1-18">        q, k, v, attn_mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>attn_mask, dropout_p<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.dropout.p, is_causal<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb1-19">    )  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># shape: (batch_size, num_heads, seq_length, head_dim)</span></span>
<span id="cb1-20">    </span>
<span id="cb1-21">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Combine the attention output from multiple heads</span></span>
<span id="cb1-22">    attn_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> attn_output.transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>).reshape(bsz, seq_length, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.embed_dim)</span>
<span id="cb1-23">    </span>
<span id="cb1-24">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Final linear projection</span></span>
<span id="cb1-25">    output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.out_proj(attn_output)</span>
<span id="cb1-26">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> output</span></code></pre></div></div>
<p>With self-attention, we model the interactions between the different parts of the <em>same input</em>.</p>
<p>Regarding the implementation above, the initialization part of the underlying class was intentionally left out in the interest of brevity. We will follow this kind of snippets for the rest of this post.</p>
</section>
<section id="cross-attention" class="level2">
<h2 class="anchored" data-anchor-id="cross-attention">Cross-attention</h2>
<p>The premise of cross-attention is we want to model how two <em>different</em> <em>inputs</em> interact with each other. For example, image patches and text tokens.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode diff code-with-copy"><code class="sourceCode diff"><span id="cb2-1"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ def forward(self, x, context=None, attn_mask=None):</span></span>
<span id="cb2-2">    # x shape: (batch_size, seq_length, embed_dim)</span>
<span id="cb2-3"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    bsz, target_seq_len, _ = x.size()</span></span>
<span id="cb2-4">    </span>
<span id="cb2-5"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    if context is None:</span></span>
<span id="cb2-6"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+        context = x</span></span>
<span id="cb2-7"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    source_seq_len = context.size(1)</span></span>
<span id="cb2-8">    </span>
<span id="cb2-9"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    # Compute queries from x; keys and values from context</span></span>
<span id="cb2-10"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    q = self.to_q(x)      # (batch_size, target_seq_length, embed_dim)</span></span>
<span id="cb2-11"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    k = self.to_k(context)  # (batch_size, source_seq_length, embed_dim)</span></span>
<span id="cb2-12"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    v = self.to_v(context)  # (batch_size, source_seq_length, embed_dim)</span></span>
<span id="cb2-13">    </span>
<span id="cb2-14">    # Reshape and transpose to get dimensions</span>
<span id="cb2-15"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    q = q.view(bsz, target_seq_len, self.num_heads, self.head_dim).transpose(1, 2)</span></span>
<span id="cb2-16"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    k = k.view(bsz, source_seq_len, self.num_heads, self.head_dim).transpose(1, 2)</span></span>
<span id="cb2-17"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    v = v.view(bsz, source_seq_len, self.num_heads, self.head_dim).transpose(1, 2)</span></span>
<span id="cb2-18">    </span>
<span id="cb2-19">    # Compute scaled dot product attention using PyTorch's built-in function</span>
<span id="cb2-20">    attn_output = F.scaled_dot_product_attention(</span>
<span id="cb2-21">        q, k, v, attn_mask=attn_mask, dropout_p=self.dropout.p, is_causal=False</span>
<span id="cb2-22">    )  # shape: (batch_size, num_heads, seq_length, head_dim)</span>
<span id="cb2-23">    </span>
<span id="cb2-24">    # Combine the attention output from multiple heads</span>
<span id="cb2-25">    attn_output = attn_output.transpose(1, 2).reshape(</span>
<span id="cb2-26">        bsz, target_seq_len, self.embed_dim</span>
<span id="cb2-27">    )</span>
<span id="cb2-28">    </span>
<span id="cb2-29">    # Final linear projection</span>
<span id="cb2-30">    output = self.out_proj(attn_output)</span>
<span id="cb2-31">    return output</span></code></pre></div></div>
<p>For the context of this post, <code>x</code> would be the noisy latents we will denoise during inference and <code>context</code> would be the representations computed from input text prompts. In this case, the attention masks (<code>attn_mask</code>) are usually computed from the <code>context</code>. For example, for text prompts, the attention masks are constructed from the actual text tokens and the padding tokens.</p>
<p>Let’s consider the sentence - “a dog”. Without going into too many details, if we want to tokenize it with a maximum sequence length of 10, the attention masks would look like so:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">[1,</span> 1, 1, 1, 0, 0, 0, 0, 0, 0]</span></code></pre></div></div>
<p>The exact text tokens would change based on the tokenizer being used but we get the idea of how attention masks might look like.</p>
<p>With the presence of attention masks, attention computation accelerates while also reducing memory footprint.</p>
<p>This implementation often meets with other elements that help stabilize training, improve the end performance, extrapolate to larger resolutions, etc. Some of these popular elements include:</p>
<p><strong>QK normalization</strong></p>
<p>Introduced in ViT-22B <span class="citation" data-cites="dehghani2023scalingvisiontransformers22">[3]</span>, QK normalization is a commonly used technique to help stabilize the training of transformers at scale. In code, this is simple to implement:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode diff code-with-copy"><code class="sourceCode diff"><span id="cb4-1">...</span>
<span id="cb4-2">q = self.to_q(x)</span>
<span id="cb4-3">k = self.to_k(k)</span>
<span id="cb4-4">...</span>
<span id="cb4-5"></span>
<span id="cb4-6"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ q = self.q_norm_layer(q)</span></span>
<span id="cb4-7"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ k = self.k_norm_layer(k)</span></span>
<span id="cb4-8">...</span></code></pre></div></div>
<p>Some choices of norms include LayerNorm, RMSNorm, and L2Norm, with the first two being the most common.</p>
<p><strong>Grouped-query attention (GQA)</strong></p>
<p>In the standard attention, every query in the sequence independently computes its attention weights with every key. But there may be redundancy <span class="citation" data-cites="ainslie2023gqatraininggeneralizedmultiquery">[4]</span> in this setup. We can maintain a reduced space for the keys, and the values repeat them across groups of queries. In practice, this looks like so:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode diff code-with-copy"><code class="sourceCode diff"><span id="cb5-1">...</span>
<span id="cb5-2">q = self.to_q(x) # (batch_size, seq_length, embed_dim)</span>
<span id="cb5-3"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ k = self.to_k(context) # (batch_size, context_seq_length, embed_dim // reduced_kv_heads)</span></span>
<span id="cb5-4"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ v = self.to_v(context) # (batch_size, context_seq_length, embed_dim // reduced_kv_heads)</span></span>
<span id="cb5-5"></span>
<span id="cb5-6"># Note there's no transpose yet (.transpose(1, 2)).</span>
<span id="cb5-7">q = q.view(batch_size, target_seq_length, self.num_heads, self.head_dim)</span>
<span id="cb5-8"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ k = k.view(batch_size, source_seq_length, self.kv_heads, self.head_dim)</span></span>
<span id="cb5-9"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ v = v.view(batch_size, source_seq_length, self.kv_heads, self.head_dim)</span></span>
<span id="cb5-10"></span>
<span id="cb5-11"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ n_rep = self.num_heads // self.kv_heads</span></span>
<span id="cb5-12"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ if n_rep &gt;= 1:</span></span>
<span id="cb5-13"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+       # Perform repeats.</span></span>
<span id="cb5-14"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    k = k.unsqueeze(3).repeat(1, 1, 1, n_rep, 1).flatten(2, 3)</span></span>
<span id="cb5-15"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    v = v.unsqueeze(3).repeat(1, 1, 1, n_rep, 1).flatten(2, 3)</span></span>
<span id="cb5-16"></span>
<span id="cb5-17"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ # Complete the transpose to get (batch_size, num_heads, seq_length, head_dim).</span></span>
<span id="cb5-18"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ q = q.transpose(1, 2)</span></span>
<span id="cb5-19"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ k = k.transpose(1, 2)</span></span>
<span id="cb5-20"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ v = v.transpose(1, 2)</span></span>
<span id="cb5-21"></span>
<span id="cb5-22"># Apply `scaled_dot_product_attention()`</span>
<span id="cb5-23">...</span></code></pre></div></div>
<p>This helps reduce memory overhead without hurting performance too much. This is crucial when generating high-resolution images and videos.</p>
<p><strong>Rotary position embeddings (RoPE)</strong></p>
<p>Rotary position embeddings have become de facto as they help extrapolate to longer sequences. Image (and video) generation is no exception! The explanation of RoPE is out of this post’s scope. Interested readers should check out <a href="https://huggingface.co/blog/designing-positional-encoding">this post</a>, instead.</p>
<p>Below, we provide where RoPE is usually incorporated when computing attention:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode diff code-with-copy"><code class="sourceCode diff"><span id="cb6-1">...</span>
<span id="cb6-2"></span>
<span id="cb6-3">q = self.to_q(x) # (batch_size, seq_length, embed_dim)</span>
<span id="cb6-4">k = self.to_k(context) # (batch_size, context_seq_length, embed_dim)</span>
<span id="cb6-5">v = self.to_v(context) # (batch_size, context_seq_length, embed_dim)</span>
<span id="cb6-6"></span>
<span id="cb6-7"># Note there's no transpose yet (.transpose(1, 2)).</span>
<span id="cb6-8">q = q.view(batch_size, target_seq_length, self.num_heads, self.head_dim)</span>
<span id="cb6-9">k = k.view(batch_size, source_seq_length, self.num_heads, self.head_dim)</span>
<span id="cb6-10">v = v.view(batch_size, source_seq_length, self.num_heads, self.head_dim) </span>
<span id="cb6-11"></span>
<span id="cb6-12"># The `*_rotary_emb()` below are computed based on the inputs.</span>
<span id="cb6-13"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ query = apply_rotary_emb(query, query_rotary_emb, use_real=False)</span></span>
<span id="cb6-14"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+ key = apply_rotary_emb(key, key_rotary_emb, use_real=False)</span></span>
<span id="cb6-15"></span>
<span id="cb6-16">...</span></code></pre></div></div>
<p>Popular models that use cross-attention include Stable Diffusion XL <span class="citation" data-cites="podell2023sdxlimprovinglatentdiffusion">[5]</span>, PixArt-{Alpha. Sigma} <span class="citation" data-cites="chen2023pixartalphafasttrainingdiffusion chen2024pixartsigmaweaktostrongtrainingdiffusion">[6, 7]</span>, Lumina-Next <span class="citation" data-cites="zhuo2024luminanextmakingluminat2xstronger">[8]</span>, LTX-Video <span class="citation" data-cites="hacohen2024ltxvideorealtimevideolatent">[9]</span>, etc. Lumina-Next incorporates all the other elements as well.</p>
</section>
<section id="joint-attention" class="level2">
<h2 class="anchored" data-anchor-id="joint-attention">Joint-attention</h2>
<p>Through cross-attention, we also inherit any bias that might be present in the prompt embeddings computed with text encoders. For example, if a text encoder exhibits a unidirectional bias (through causal attention), that can creep unexpectedly into the diffusion model representations. Joint-attention alleviates this by allowing the representations coming from two different modalities to co-evolve with training.</p>
<p>The diagram below depicts the MMDiT architecture, which also introduces joint-attention.</p>
<div align="center">
<img src="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/attn_diffusion/mmdit.png" width="600/">
<p><small>MMDiT architecture. Figure taken from SD 3 paper [@esser2024scalingrectifiedflowtransformers].</small>
</p></div>
<p>If the diagram appears to be overwhelming, don’t worry, the implementation of it is simpler than one might think. In a nutshell, in joint-attention, the QKV projection is performed separately (with separate sets of params) on each of the two modalities shown above (<code>c</code> being the representation computed from the text prompts and <code>x</code> being noisy latents to be denoised). Before computing the attention, these projections are concatenated. Quoting from the SD3 paper <span class="citation" data-cites="esser2024scalingrectifiedflowtransformers">[1]</span>:</p>
<blockquote class="blockquote">
<p>Since text and image embeddings are conceptually quite different, we use two separate sets of weights for the two modalities. […], this is equivalent to having two independent transformers for each modality, but joining the sequences of the two modalities for the attention operation, such that both representations can work in their own space yet take the other one into account.</p>
</blockquote>
<p>Interested readers can check out <a href="https://x.com/RisingSayak/status/1888462811971400063">this thread</a> for more insights from the community.</p>
<p>Let’s now turn our attention to how it is implemented in practice.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x, context<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, attn_mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb7-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> context <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb7-3">        source_seq_len <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> context.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb7-4">    </span>
<span id="cb7-5">    bsz, target_seq_len, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x.size()</span>
<span id="cb7-6">    </span>
<span id="cb7-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute projections on the different modalities separately</span></span>
<span id="cb7-8">    q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_q(x)      </span>
<span id="cb7-9">    k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_k(x)  </span>
<span id="cb7-10">    v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_v(x)  </span>
<span id="cb7-11"></span>
<span id="cb7-12">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Reshape and transpose for multi-head attention</span></span>
<span id="cb7-13">    q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> q.view(bsz, target_seq_len, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_heads, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb7-14">    k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> k.view(bsz, target_seq_len, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_heads, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb7-15">    v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> v.view(bsz, target_seq_len, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_heads, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb7-16"></span>
<span id="cb7-17">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute projections on the condition separately</span></span>
<span id="cb7-18">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> context <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb7-19">        context_q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_add_q(context) </span>
<span id="cb7-20">        context_k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_add_k(context)</span>
<span id="cb7-21">        context_v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_add_v(context)</span>
<span id="cb7-22">        </span>
<span id="cb7-23">        context_q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> context_q.view(bsz, source_seq_len, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_heads, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb7-24">        context_k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> context_k.view(bsz, source_seq_len, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_heads, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb7-25">        context_v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> context_v.view(bsz, source_seq_len, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_heads, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head_dim).transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb7-26"></span>
<span id="cb7-27">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Concatenate across the sequence length dimension</span></span>
<span id="cb7-28">        q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.cat([context_q, q], dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb7-29">        k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.cat([context_k, k], dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb7-30">        v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.cat([context_v, v], dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb7-31">    </span>
<span id="cb7-32">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute scaled dot product attention</span></span>
<span id="cb7-33">    attn_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> F.scaled_dot_product_attention(</span>
<span id="cb7-34">        q, k, v, attn_mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>attn_mask, dropout_p<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.dropout.p, is_causal<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb7-35">    )  </span>
<span id="cb7-36">    </span>
<span id="cb7-37">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Combine attention heads</span></span>
<span id="cb7-38">    attn_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> attn_output.transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>).reshape(</span>
<span id="cb7-39">        bsz, target_seq_len, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.embed_dim</span>
<span id="cb7-40">    )</span>
<span id="cb7-41"></span>
<span id="cb7-42">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Seperate context from latents and final linear projection</span></span>
<span id="cb7-43">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> context <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb7-44">        context, x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb7-45">            attn_output[:, : source_seq_len],</span>
<span id="cb7-46">            attn_output[:, source_seq_len :],</span>
<span id="cb7-47">        )</span>
<span id="cb7-48">        context <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.add_out_proj(context)</span>
<span id="cb7-49">        x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.out_proj(x)</span>
<span id="cb7-50"></span>
<span id="cb7-51">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> x, context</span>
<span id="cb7-52">    </span>
<span id="cb7-53">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb7-54">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.out_proj(attn_output)</span></code></pre></div></div>
<p>With joint-attention, it becomes unclear how to incorporate attention masks while computing attention and how much of a performance penalty it incurs due to that.</p>
<p><strong>Part joint-attention, part self-attention</strong></p>
<p>Subsequent works like AuraFlow <span class="citation" data-cites="auraflow">[10]</span> and Flux <span class="citation" data-cites="flux">[11]</span> introduced a small change in the original MMDiT architecture. They use joint attention for the first few layers within the diffusion transformer. They then concatenate the two different outputs and operate on the concatenated output. As per the AuraFlow authors, it helps with better FLOPs optimization. In pseudo-code, it looks like so:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Regular MMDiT blocks.</span></span>
<span id="cb8-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> block <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.double_blocks:</span>
<span id="cb8-3">    context, x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> block(</span>
<span id="cb8-4">        x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>x,</span>
<span id="cb8-5">        context<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>context, </span>
<span id="cb8-6">        ...</span>
<span id="cb8-7">    )</span>
<span id="cb8-8"></span>
<span id="cb8-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Concatenate.</span></span>
<span id="cb8-10">context_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.cat([context, x], dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)  </span>
<span id="cb8-11"></span>
<span id="cb8-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Continue with the rest.</span></span>
<span id="cb8-13"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> block <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.single_blocks:</span>
<span id="cb8-14">    x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> block(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>context_x, ...) </span></code></pre></div></div>
<p>The joint-attention implementation provided above is already equipped to handle situations when <code>context</code> may not be provided while computing attention (the <code>context is not None</code> code path).</p>
<p><strong>Attention gymnastics</strong></p>
<p>Flux, additionally, improves the hardware efficiency by using parallel layers <span class="citation" data-cites="dehghani2023scalingvisiontransformers22">[3]</span>. To better understand how parallel layers can be helpful in improving efficiency, let’s look at the first set of equations that govern an encoder-style transformer block:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20y%5E%7B%5Cprime%7D=%5Coperatorname%7BLayerNorm%7D(x)%20%5C%5C%0A&amp;%20y=x+%5Coperatorname%7BMLP%7D%5Cleft(y%5E%7B%5Cprime%7D%5Cright)+%5Coperatorname%7BAttention%7D%5Cleft(y%5E%7B%5Cprime%7D%5Cright)%0A%5Cend%7Baligned%7D%0A"></p>
<p>We can combine the linear projection layers of attention (QKV) and the MLP. From the ViT-22B paper:</p>
<blockquote class="blockquote">
<p>In particular, the matrix multiplication for query/key/value-projections and the first linear layer of the MLP are fused into a single operation, and the same is done for the attention out-projection and second linear layer of the MLP.</p>
</blockquote>
<p>To understand how it is implemented in practice, we first need to understand QKV fusion. It lets us perform the three different projections involved in attention in one go. Instead of having three different projection layers, we only keep one:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">...</span>
<span id="cb9-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># This becomes</span></span>
<span id="cb9-3"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(embed_dim, embed_dim)</span>
<span id="cb9-4"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(embed_dim, embed_dim)</span>
<span id="cb9-5"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(embed_dim, embed_dim)</span>
<span id="cb9-6"></span>
<span id="cb9-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># this ⬇️</span></span>
<span id="cb9-8"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_qkv <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(embed_dim, embed_dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span></code></pre></div></div>
<p>Then, during the forward pass, we do:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">...</span>
<span id="cb10-2">qkv <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.to_qkv(x)</span>
<span id="cb10-3">split_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> qkv.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb10-4">q, k, v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.split(qkv, split_size, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb10-5"></span>
<span id="cb10-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Rest of the process is the same</span></span>
<span id="cb10-7">...</span></code></pre></div></div>
<p>Now, to incorporate the MLP into the mix, we need some changes in the initialization:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">...</span>
<span id="cb11-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># MLP ratio is typically 4.</span></span>
<span id="cb11-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The first part of fusion. QKV + first layer of an MLP from a transformer block.</span></span>
<span id="cb11-4"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.qkv_mlp_first <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(embed_dim, embed_dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> mlp_ratio))</span>
<span id="cb11-5"></span>
<span id="cb11-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Second part of fusion. Attention out projection + second MLP layer.</span></span>
<span id="cb11-7"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.attn_proj_mlp_second <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(</span>
<span id="cb11-8">    embed_dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> embed_dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> mlp_ratio, embed_dim</span>
<span id="cb11-9">)</span></code></pre></div></div>
<p>The forward pass would be:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">qkv, mlp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.split(</span>
<span id="cb12-2">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.qkv_mlp_first(x_mod), </span>
<span id="cb12-3">    [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.embed_dim, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.embed_dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mlp_ratio)], </span>
<span id="cb12-4">    dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb12-5">)</span>
<span id="cb12-6">q, k, v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.split(qkv, qkv.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb12-7"></span>
<span id="cb12-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute attention</span></span>
<span id="cb12-9">...</span>
<span id="cb12-10"></span>
<span id="cb12-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># MLP computation</span></span>
<span id="cb12-12">concat_attn_mlp_in <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.cat((attn_output, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mlp_act(mlp)), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb12-13">output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.attn_proj_mlp_second(concat_attn_mlp_in)</span></code></pre></div></div>
<p>The ViT-22B paper <span class="citation" data-cites="dehghani2023scalingvisiontransformers22">[3]</span> also provides a great visualization for this:</p>
<div align="center">
<img src="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/attn_diffusion/parallel_layers.png" width="600/">
<p><small>Transformer block with parallel layers. Figure taken from the ViT-22B paper [@dehghani2023scalingvisiontransformers22].</small>
</p></div>
<p>It is also a good idea to mention that some of the other elements we discussed earlier — QK normalization, GQA, and RoPE — can also be combined with joint attention. One of the most popular models, Flux, uses QK normalization and RoPE. Lumina2 <span class="citation" data-cites="lumina2">[12]</span> combines all three and uses joint-attention with a twist:</p>
<ul>
<li>It first uses very few layers of self-attention transformer blocks separately on the noisy latents and the conditional representations.</li>
<li>It then combines the two representations and runs it through a number of self-attention transformer blocks.</li>
</ul>
<p>Interested readers can check out the implementation details <a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/transformers/transformer_lumina2.py#L498C1-L525C1">here</a>. The figure below provides a side-by-side comparison of the differences in attention used in SD3 and Lumina2:</p>
<div align="center">
<img src="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/attn_diffusion/comparison_sd3_lumina2.jpg" width="600/">
<p><small>Comparison between the attention schemes used in SD3 and Lumina2, respectively. Figures were intentionally simplified to convey the main idea.</small>
</p></div>
</section>
<section id="linear-attention" class="level2">
<h2 class="anchored" data-anchor-id="linear-attention">Linear attention</h2>
<p>As the world already knows, attention has a quadratic time complexity. This can pose prohibitive challenges when operating with very long sequences despite improved techniques like Flash Attention.</p>
<p>SANA <span class="citation" data-cites="xie2024sanaefficienthighresolutionimage">[2]</span> replaced all vanilla attention with linear attention. More specifically, in each of its transformer blocks, SANA has two kinds of attention:</p>
<ol type="1">
<li>linear self-attention for the noisy latents,</li>
<li>regular cross-attention for the noisy latents (<code>x</code>) and the condition representations (<code>context</code>).</li>
</ol>
<p>To facilitate local interactions between the tokens, it used “Mix-FFN” blocks <span class="citation" data-cites="xie2021segformersimpleefficientdesign liu2023efficientvitmemoryefficientvision">[13, 14]</span>.</p>
<div align="center">
<img src="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/attn_diffusion/sana_linear_attn.png" width="600/">
<p><small>Linear attention block and the Mix-FFN block. Figure taken from the SANA paper [@xie2024sanaefficienthighresolutionimage].</small>
</p></div>
<p>Implementation of this linear-attention variant is by far the simplest. We show the main changes introduced when compared to the classic self-attention:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode diff code-with-copy"><code class="sourceCode diff"><span id="cb13-1">def forward(self, x):</span>
<span id="cb13-2">    # x shape: (batch_size, seq_length, embed_dim)</span>
<span id="cb13-3">    bsz, seq_length, _ = x.size()</span>
<span id="cb13-4">    </span>
<span id="cb13-5">    # Compute queries, keys, and values using separate linear layers</span>
<span id="cb13-6">    q = self.to_q(x)  # shape: (batch_size, seq_length, embed_dim)</span>
<span id="cb13-7">    k = self.to_k(x)  # shape: (batch_size, seq_length, embed_dim)</span>
<span id="cb13-8">    v = self.to_v(x)  # shape: (batch_size, seq_length, embed_dim)</span>
<span id="cb13-9">    </span>
<span id="cb13-10">    # Reshape and transpose to get dimensions </span>
<span id="cb13-11">    # (batch_size, num_heads, seq_length, head_dim)</span>
<span id="cb13-12">    q = q.view(bsz, seq_length, self.num_heads, self.head_dim).transpose(1, 2)</span>
<span id="cb13-13">    k = k.view(bsz, seq_length, self.num_heads, self.head_dim).transpose(1, 2)</span>
<span id="cb13-14">    v = v.view(bsz, seq_length, self.num_heads, self.head_dim).transpose(1, 2)</span>
<span id="cb13-15"></span>
<span id="cb13-16"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+   # Reshape to (batch_size, seq_length, num_heads, head_dim)</span></span>
<span id="cb13-17"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+   q = q.transpose(2, 3)</span></span>
<span id="cb13-18"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+   v = v.transpose(2, 3)</span></span>
<span id="cb13-19">    </span>
<span id="cb13-20"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    # Introduce non-linearity</span></span>
<span id="cb13-21"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    q = F.relu(q)</span></span>
<span id="cb13-22"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    k = F.relu(k)</span></span>
<span id="cb13-23"></span>
<span id="cb13-24"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    # Combine scores</span></span>
<span id="cb13-25"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    scores = torch.matmul(v, k)</span></span>
<span id="cb13-26"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    x = torch.matmul(scores, q)</span></span>
<span id="cb13-27">    </span>
<span id="cb13-28"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    # Scale</span></span>
<span id="cb13-29"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">+    x = x[:, :, :-1] / (x[:, :, -1:] + 1e-15)</span></span>
<span id="cb13-30">    </span>
<span id="cb13-31">    # Combine the output from multiple heads</span>
<span id="cb13-32">    x = x.transpose(1, 2).reshape(bsz, seq_length, self.embed_dim)</span>
<span id="cb13-33">    </span>
<span id="cb13-34">    # Final linear projection</span>
<span id="cb13-35">    output = self.out_proj(x)</span>
<span id="cb13-36">    return output</span></code></pre></div></div>
<p>It’s also worth pointing out that SANA uses no positional encodings (so-called <strong>NoPE</strong>). The SANA authors conjectured that the use of Mix-FFN blocks helped them get away with NoPE without incurring any loss in performance.</p>
</section>
<section id="thoughts" class="level2">
<h2 class="anchored" data-anchor-id="thoughts">Thoughts</h2>
<p>Throughout the course of this post, we saw many architectural configurations of the attention mechanism. Some questions that may still arise:</p>
<ul>
<li>Is there any benefit to using cross-attention for image-video generation at all?</li>
<li>How can we compensate for the compute intensity of joint-attention?</li>
<li>Is the Lumina2 way of doing joint-attention the way to go?</li>
<li>Is it necessary to do masking in joint-attention? If so, what are the benefits?</li>
</ul>
<p>In defense of the widely adopted and optimized vanilla attention, could we interleave quadratic attention and window attention (as done in Gemma2 <span class="citation" data-cites="gemmateam2024gemma2improvingopen">[15]</span>)?</p>
<p>All of these questions (and possibly more) warrant a careful ablation study.</p>
<p><em>Acknowledgements</em>: Thanks to <a href="https://twitter.com/ariG23498">Aritra Roy Gosthipaty</a> for useful feedback.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body">
<div id="ref-esser2024scalingrectifiedflowtransformers" class="csl-entry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Esser P, Kulal S, Blattmann A, et al (2024) <a href="https://arxiv.org/abs/2403.03206">Scaling rectified flow transformers for high-resolution image synthesis</a></div>
</div>
<div id="ref-xie2024sanaefficienthighresolutionimage" class="csl-entry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Xie E, Chen J, Chen J, et al (2024) <a href="https://arxiv.org/abs/2410.10629">SANA: Efficient high-resolution image synthesis with linear diffusion transformers</a></div>
</div>
<div id="ref-dehghani2023scalingvisiontransformers22" class="csl-entry">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">Dehghani M, Djolonga J, Mustafa B, et al (2023) <a href="https://arxiv.org/abs/2302.05442">Scaling vision transformers to 22 billion parameters</a></div>
</div>
<div id="ref-ainslie2023gqatraininggeneralizedmultiquery" class="csl-entry">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">Ainslie J, Lee-Thorp J, Jong M de, Zemlyanskiy Y, Lebrón F, Sanghai S (2023) <a href="https://arxiv.org/abs/2305.13245">GQA: Training generalized multi-query transformer models from multi-head checkpoints</a></div>
</div>
<div id="ref-podell2023sdxlimprovinglatentdiffusion" class="csl-entry">
<div class="csl-left-margin">5. </div><div class="csl-right-inline">Podell D, English Z, Lacey K, et al (2023) <a href="https://arxiv.org/abs/2307.01952">SDXL: Improving latent diffusion models for high-resolution image synthesis</a></div>
</div>
<div id="ref-chen2023pixartalphafasttrainingdiffusion" class="csl-entry">
<div class="csl-left-margin">6. </div><div class="csl-right-inline">Chen J, Yu J, Ge C, et al (2023) <a href="https://arxiv.org/abs/2310.00426">PixArt-<img src="https://latex.codecogs.com/png.latex?%5Calpha">: Fast training of diffusion transformer for photorealistic text-to-image synthesis</a></div>
</div>
<div id="ref-chen2024pixartsigmaweaktostrongtrainingdiffusion" class="csl-entry">
<div class="csl-left-margin">7. </div><div class="csl-right-inline">Chen J, Ge C, Xie E, et al (2024) <a href="https://arxiv.org/abs/2403.04692">PixArt-: Weak-to-strong training of diffusion transformer for 4K text-to-image generation</a></div>
</div>
<div id="ref-zhuo2024luminanextmakingluminat2xstronger" class="csl-entry">
<div class="csl-left-margin">8. </div><div class="csl-right-inline">Zhuo L, Du R, Xiao H, et al (2024) <a href="https://arxiv.org/abs/2406.18583">Lumina-next: Making lumina-T2X stronger and faster with next-DiT</a></div>
</div>
<div id="ref-hacohen2024ltxvideorealtimevideolatent" class="csl-entry">
<div class="csl-left-margin">9. </div><div class="csl-right-inline">HaCohen Y, Chiprut N, Brazowski B, et al (2024) <a href="https://arxiv.org/abs/2501.00103">LTX-video: Realtime video latent diffusion</a></div>
</div>
<div id="ref-auraflow" class="csl-entry">
<div class="csl-left-margin">10. </div><div class="csl-right-inline">fal.ai, Simo <a href="https://blog.fal.ai/auraflow">Introducing AuraFlow v0.1, an open exploration of large rectified flow models</a></div>
</div>
<div id="ref-flux" class="csl-entry">
<div class="csl-left-margin">11. </div><div class="csl-right-inline">Labs BF <a href="https://blackforestlabs.ai/announcing-black-forest-labs/">Announcing black forest labs</a></div>
</div>
<div id="ref-lumina2" class="csl-entry">
<div class="csl-left-margin">12. </div><div class="csl-right-inline">Team L <a href="https://github.com/Alpha-VLLM/Lumina-Image-2.0">Lumina-image 2.0 : A unified and efficient image generative model</a></div>
</div>
<div id="ref-xie2021segformersimpleefficientdesign" class="csl-entry">
<div class="csl-left-margin">13. </div><div class="csl-right-inline">Xie E, Wang W, Yu Z, Anandkumar A, Alvarez JM, Luo P (2021) <a href="https://arxiv.org/abs/2105.15203">SegFormer: Simple and efficient design for semantic segmentation with transformers</a></div>
</div>
<div id="ref-liu2023efficientvitmemoryefficientvision" class="csl-entry">
<div class="csl-left-margin">14. </div><div class="csl-right-inline">Liu X, Peng H, Zheng N, Yang Y, Hu H, Yuan Y (2023) <a href="https://arxiv.org/abs/2305.07027">EfficientViT: Memory efficient vision transformer with cascaded group attention</a></div>
</div>
<div id="ref-gemmateam2024gemma2improvingopen" class="csl-entry">
<div class="csl-left-margin">15. </div><div class="csl-right-inline">Team G (2024) <a href="https://arxiv.org/abs/2408.00118">Gemma 2: Improving open language models at a practical size</a></div>
</div>
</div></section></div> ]]></description>
  <category>diffusion</category>
  <category>diffusion-transformers</category>
  <category>diffusers</category>
  <guid>https://sayak.dev/posts/attn-diffusion.html</guid>
  <pubDate>Thu, 27 Feb 2025 00:00:00 GMT</pubDate>
  <media:content url="https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/attn_diffusion/llama_header.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Streamlining PyPI Releases: A Case Study with 🧨 Diffusers</title>
  <link>https://sayak.dev/posts/streamlined-releases.html</link>
  <description><![CDATA[ 






<p>Releasing a new version of an open-source library is an exhilarating experience. You ship new features, bug fixes, improved documentation, etc., to serve your users and also the mission of your library. Being one of the maintainers of the <a href="https://github.com/huggingface/diffusers/">🧨&nbsp;Diffusers library</a>, I am no exception to this.</p>
<p>Once a release is finalized, it’s usually published on a software repository for the distribution programming language you’re using. Diffusers is a Python library, so <a href="https://pypi.org/">PyPI</a> is our publishing platform.</p>
<p>In this post, I share some of what I learned from trying to streamline the entire process of releasing a new version of the library and then publishing it. If you have similar responsibilities at your workplace or for your personal projects, this post might be helpful for you.</p>
<section id="an-example-release-workflow-manual" class="level2">
<h2 class="anchored" data-anchor-id="an-example-release-workflow-manual">An example release workflow (manual)</h2>
<p>Before we proceed to the other sections of the post, it will be helpful to have a schematic of what constitutes a release. Note that this workflow will vary from library to library, but some principles will still apply. I will take the workflow we follow for Diffusers as an example.</p>
<p>The steps are well laid out in <code>setup.py</code> and can be found <a href="https://github.com/huggingface/diffusers/blob/main/setup.py#L20C1-L78C55">here</a>. Broadly, these are:</p>
<ol type="1">
<li>Prepare the release branch and cut it out from the <code>main</code>.</li>
<li>Run any test on the release branch and wait for them to pass. Fix any failures if needed.</li>
<li>Tag the release branch and push the tag.</li>
<li>Build the package source and wheel.<br>
</li>
<li>Upload the package distribution to the <a href="https://test.pypi.org/">Test PyPI server</a> and run any tests.</li>
<li>Finally, upload to the actual <a href="https://pypi.org/">PyPI server</a>.</li>
</ol>
<p>We identified that steps 1-3 will always require a bit of human intervention and cannot be automated much (props if that’s not the case for you). But steps 3-6 can indeed be automated. These steps require more attention, too:</p>
<ul>
<li>When building the package distribution, one must delete the previous one before starting the build. Otherwise, it can have unintended consequences.</li>
<li>Managing the credentials for the Test PyPI and PyPI servers.</li>
<li>Running any tests after publishing them on the Test PyPI server.</li>
</ul>
<p>These steps would be better automated in your library’s Continuous Integration suite, greatly reducing the mental burden.</p>
</section>
<section id="semi-automating-the-release-workflow" class="level2">
<h2 class="anchored" data-anchor-id="semi-automating-the-release-workflow">Semi-automating the release workflow</h2>
<p>Once we identified the above findings, we prepared a GitHub Actions workflow that gets triggered after a release is tagged and the tag is pushed. Additionally, we configured the workflow to be manually triggerable in case any intervention was needed.</p>
<p>This workflow takes the following steps:</p>
<ol type="1">
<li>Find out the release branch so that it can be checked out for the sequential steps.</li>
<li>Steps 3-6 as outlined in the above section.</li>
</ol>
<p>It’s worth noting that the trigger for this kind of workflow should be figured out to suit what’s best for the given project. In the case of Diffusers, we realized that release steps that come after pushing the release tags can be largely automated. Hence, we went with that trigger.</p>
<p>The workflow file is available <a href="https://github.com/huggingface/diffusers/blob/main/.github/workflows/pypi_publish.yaml">here</a>. When successfully executed, it appears like so:</p>
<p><img src="https://github.com/sayakpaul/portfolio/blob/master/posts/release_graph.png?raw=true" class="img-fluid"></p>
<p>You can find the complete details about the action run <a href="https://github.com/huggingface/diffusers/actions/runs/8283556088">here</a>.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Pay attention to the dependencies
</div>
</div>
<div class="callout-body-container callout-body">
<p>The initial workflow was missing a dependency that was needed to run the import tests after Test PyPI publishing. This was fixed in <a href="https://github.com/huggingface/diffusers/pull/7339">this PR</a>. So, please double-check any dependency that might be needed to run the tests after your package has been published on the Test PyPI server.</p>
</div>
</div>
<p>The <a href="https://github.com/huggingface/diffusers/blob/main/.github/workflows/pypi_publish.yaml">workflow</a> doesn’t make use of any pre-built actions (such as <a href="https://github.com/pypa/gh-action-pypi-publish"><code>pypa/gh-action-pypi-publish@v1.6.4</code></a>) for publishing on PyPI. Instead, we decided to just follow what we’d do manually, i.e., use <code>twine</code> to manage the process. If you’re looking to use such an action, <a href="https://github.com/philschmid/easyllm/blob/main/.github/workflows/publish.yaml">this</a> can be a handy reference.</p>
</section>
<section id="publishing-the-release-notes-and-communications" class="level2">
<h2 class="anchored" data-anchor-id="publishing-the-release-notes-and-communications">Publishing the release notes and communications</h2>
<p>The next step in the release process involves publishing the release notes on your repository and tagging it. Once a release is published, team members usually communicate about it internally within an organization and also more broadly with their communities through social media channels.</p>
<p>On the Diffusers team, we take release notes pretty seriously (<a href="https://github.com/huggingface/diffusers/releases/tag/v0.27.0">example notes</a>). This is why we intentionally keep the process of writing the notes and publishing them purely manual. Once a release is published on the repository, a workflow gets automatically triggered to communicate about it to an internal Slack channel. Successful execution of this workflow makes a bot automatically post the message below to a particular Slack channel:</p>
<p><img src="https://github.com/sayakpaul/portfolio/blob/master/posts/bot.png?raw=true" class="img-fluid"></p>
<p>This workflow can be found <a href="https://github.com/huggingface/diffusers/blob/main/.github/workflows/notify_slack_about_release.yml">here</a>.</p>
<p>Both the above steps were introduced in Diffusers through <a href="https://github.com/huggingface/diffusers/pull/7270">this PR</a>. I recommend readers to go through it if they want to incorporate similar changes in their projects.</p>
</section>
<section id="considerations" class="level2">
<h2 class="anchored" data-anchor-id="considerations">Considerations</h2>
<p>I played with the workflows rigorously on a <a href="https://github.com/sayakpaul/blossom">dummy repository</a> before introducing them in Diffusers. This is optional but highly recommended to confidently land similar changes in your actual projects.</p>
<p>We used incoming webhooks on Slack so that the bot could post messages. If you’re configuring something similar, this <a href="https://api.slack.com/messaging/webhooks">official tutorial</a> can be quite useful.</p>


</section>

 ]]></description>
  <category>pypi-releases</category>
  <category>github-actions</category>
  <category>diffusers</category>
  <guid>https://sayak.dev/posts/streamlined-releases.html</guid>
  <pubDate>Fri, 15 Mar 2024 00:00:00 GMT</pubDate>
  <media:content url="https://sayak.dev/posts/pypi-releases-diffusers.png" medium="image" type="image/png" height="72" width="144"/>
</item>
<item>
  <title>Practicing ML in a Non-ML Job</title>
  <link>https://sayak.dev/posts/ml-practice.html</link>
  <description><![CDATA[ 






<p>Many people who aspire to become Machine Learning (ML) practitioners find it particularly difficult to continue to hone relevant skills when they pursue a job that does not involve even a tiny bit of ML. So, if you’re serious about choosing ML as a potential career option, it’s important to ensure you continue to practise what you’re learning along the way. Otherwise, there’d likely be nothing for a recruiter to trust in your candidature which, in turn, minimizes your chances of landing the ML job you always wanted.</p>
<p>I myself am not an exception to this. Back in 2017, when I was working at Tata Consultancy Services Limited (TCS), I didn’t get any assignments involving ML expertise. But I tried to utilize my off-work hours in a way that helped me improve my ML-specific knowledge as well as strengthen my candidature.</p>
<p>So, in this post, I’ll share what I did during those days in the hope of providing some meaningful ways for navigation.</p>
<p><strong>Disclaimer</strong>: The opinions stated in this post are solely mine and they are not meant to demean anyone else’s opinions about the same topic.</p>
<section id="assumptions" class="level2">
<h2 class="anchored" data-anchor-id="assumptions">Assumptions</h2>
<p>The post is best-suited for professionals that have prior experience in coding (preferably in Python) and know their way around the fundamentals of ML. If you’re an absolute beginner then I recommend picking up a book (<a href="https://www.manning.com/books/grokking-machine-learning">an example</a>) or a course (<a href="https://developers.google.com/machine-learning/crash-course">an example</a>) to get started. Also, if you haven’t yet picked up an ML framework (Scikit-Learn, PyTorch, TensorFlow, JAX, etc.), then I highly recommend picking one up.</p>
</section>
<section id="solace-in-uncertainty" class="level2">
<h2 class="anchored" data-anchor-id="solace-in-uncertainty">Solace in Uncertainty</h2>
<p>Set your objectives straight. Ask yourself if you’re absolutely certain about wanting to pursue a career in ML. If so, then are you willing to make the adjustments necessary to attain that at any cost? Although these questions are not specific to the purposes of this post, they help set a mindset to push through uncertain times.</p>
<p>I was hell-bent on taking up a career in ML that helped me to work on myself in those extra hours after work. It didn’t feel like I’m being forced into doing this. I thoroughly enjoyed the process and I trusted it. There are things I still enjoy doing like, reading a new paper, learning about a new concept, implementing it, etc.</p>
</section>
<section id="overwhelm-and-courage-to-learn" class="level2">
<h2 class="anchored" data-anchor-id="overwhelm-and-courage-to-learn">Overwhelm and Courage to Learn</h2>
<p>Feeling overwhelmed especially in the ML domain is common given how vast the field is and how rapidly it is evolving regularly. I see this positively because I know that there are things I don’t yet know and I use it as a learning opportunity to improve my knowledge.</p>
<p>One might wonder, do I learn each and everything that comes out? That’s impossible and likely, isn’t very helpful. So, I like to pick up something from the vault of things that genuinely interest me in ML and start digging deeper. I find it incredibly helpful in boosting my confidence. I also figured that the more I did this, the better I was able to develop a general understanding of a broad number of relevant things.</p>
<p>In a nutshell, treating the feeling of “overwhelm” as a learning opportunity has been quite helpful for me.</p>
</section>
<section id="learn-apply-demo-repeat" class="level2">
<h2 class="anchored" data-anchor-id="learn-apply-demo-repeat">Learn, Apply, (Demo), Repeat</h2>
<p>Learning is not enough. You need to be able to develop evidence that shows you can apply what you’ve learned successfully. I highly recommend reading <a href="https://blog.floydhub.com/emils-story-as-a-self-taught-ai-researcher/">this interview with Emil Wallner</a> who’s an “internet-taught” ML Researcher working as a resident at Google.</p>
<p>Below, I discuss a few things you can do to exercise your ML learnings.</p>
<section id="kaggle" class="level3">
<h3 class="anchored" data-anchor-id="kaggle">Kaggle</h3>
<p>Kaggle is arguably one of the best platforms to develop skills for data preprocessing and applying ML in creative ways to solve unique problems. So, pick an interesting dataset or a competition <em>just for learning purposes</em>. Putting the competitive mindset aside, during my initial years it really helped me to develop a mindset of always learning to facilitate self-improvement. If you commit to it hard enough, you will have developed a bunch of useful skills. Over time, you’ll definitely get better.</p>
<p>Keeping an open mind for learning is important here because expectations of outcomes can quickly derail you. Also, remember that the rate of improvement is not the same for everyone. So, it’s better to just do things that are within your control (for example, learning something), and consistently get better at those.</p>
</section>
<section id="papers-concepts" class="level3">
<h3 class="anchored" data-anchor-id="papers-concepts">Papers / Concepts</h3>
<p>Reading research papers is a common undertaking in ML. It can be rewarding to summarize, implement, and blog about a paper that is impactful and tackles interesting problems. Extending on this theme, you have a number of options:</p>
<ul>
<li><p>You can summarize a paper in your own words and publish it on platforms like <a href="https://medium.com/">Medium</a> or even on <a href="https://github.com/fastai/fastpages">your own blog</a>. It’s also important to get feedback on your work. So, feel free to share your work on Social Media as well as let the authors of the actual paper know about your work. A paper summary is supposed to be a reflection of how you perceived the paper. So, if you have criticisms of a paper, do include those with solid reasoning. If you’re looking for an example, definitely check out <a href="https://medium.com/@nainaakash012">Aakash Kumar Nain’s paper summaries</a>.</p>
<p>Picking a paper could be a non-trivial work especially when there’s always a flood of papers on <a href="https://arxiv.org/">arXiv</a>. I usually follow the blogs of research labs at <a href="https://ai.googleblog.com/">Google</a>, <a href="https://ai.facebook.com/blog">Meta</a>, <a href="https://blog.allenai.org/">AI2</a>, <a href="https://bair.berkeley.edu/">BAIR</a>, etc., to keep myself up-to-date about the work I care about. There’s a good chance you’ll find your niche there. Following the works of the most accomplished researchers from my favorite domains is another practice I incorporate.</p>
<p>In this regard, I highly recommend the following two books that actively cite examples of relevant research papers and also implement them in ways that are practically beneficial: <a href="https://www.oreilly.com/library/view/deep-learning-for/9781492045519/">Deep Learning for Coders with fastai and PyTorch</a> by Jeremy Howard and Sylvain Gugger, <a href="https://www.oreilly.com/library/view/natural-language-processing/9781098103231/">Natural Language Processing with Transformers</a> by Lewis Tunstall, Leandro von Werra, and Thomas Wolf. For developing a general understanding of different areas in ML, I recommend the articles on <a href="https://distill.pub/">Distill Pub</a>. <br><br></p></li>
<li><p>Nowadays, a majority of ML papers come with official open-source implementations in the interest of reproducibility. But some don’t. Regardless of either, it’s a good exercise to try to implement the novel bits of a paper. The <a href="https://github.com/rwightman/pytorch-image-models">timm</a><a href="https://github.com/rwightman/pytorch-image-models">libary</a> is a great example of how paper reimplementations should be structured. <br><br></p></li>
<li><p>Blogging has easily become one of the most effective ways to communicate your understanding of something. This does not need to be just tied to papers, though. You can always pick up an interesting concept and blog about it. Many ML stalwarts keep pressing on why you should blog and here is one such example: <a href="https://medium.com/@racheltho/why-you-yes-you-should-blog-7d2544ac1045">Why you (yes, you) should blog</a> by Rachel Thomas.</p></li>
</ul>
<p>You can also consider making videos on papers, concepts, and so on. If you haven’t already, then definitely get to know <a href="https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfew">Yannic Kilcher</a> who has revolutionized the way forward in this theme.</p>
</section>
<section id="open-source-contributions" class="level3">
<h3 class="anchored" data-anchor-id="open-source-contributions">Open-source Contributions</h3>
<p>From my personal experience, I can confirm that making open-source contributions is one of the most useful ways to stay involved in the domain. All the popular ML Python libraries (Scikit-Learn, PyTorch, TensorFlow, Keras, JAX, Hugging Face Transformers, etc.) are open-source and that provides even more opportunities to learn and grow.</p>
<p>I have a <a href="https://www.youtube.com/watch?v=VsBEFUoESR4&amp;t=3123s">separate presentation</a> on this topic but here, I provide my perspectives for context:</p>
<ul>
<li><p>When you’re contributing to a well-maintained open-source library for the first time there’s a high chance that you’ll learn a few things other than just ML. These include writing unit tests, setting up the local development environment, library building tools, etc. This way, you get first-hand exposure to how software engineering is approached in the ML domain in general.</p>
<p>So, not only do you get to contribute to your favorite open-source library (which is an inexplicable feeling anyway), but you also get to learn skills that are practically quite demanding. Beyond these, you get a chance to interact with experts and get their feedback to improve your work. Additionally, you get to collect objective evidence of your skills - coding, thorough understanding of a critical component and the library, building a library, etc. - all of which are noteworthy.</p>
<p>Note that you’re not alone if you’re feeling lost when you’re just starting to contribute to an open-source library. It happens to most. But when you put your mind toward making your contribution anyway, you get to get better in the process. <br><br></p></li>
<li><p>If you feel you’re not ready yet to make contributions, working on your own open-source projects is another promising avenue to pursue. Take Andrej Karpathy’s <a href="https://github.com/karpathy/minGPT">miniGPT</a> project as an example. Besides being an amazing educational resource for learning about the <a href="https://en.wikipedia.org/wiki/GPT-3">GPT model</a>, it serves as a great reference for implementing many of the foundational blocks of <a href="https://arxiv.org/abs/1706.03762">Transformer</a>-based architectures.</p>
<p>If you’re looking for open-source project ideas then <a href="https://youtu.be/dllfKQKlzvg">my presentation</a> on this topic might be helpful.</p></li>
</ul>
<p>Now that we’ve looked into different ways of being engaged with our independent ML practice, let us take examples of two individuals from the ML community who have followed similar paths in this regard.</p>
</section>
</section>
<section id="references-from-the-community" class="level2">
<h2 class="anchored" data-anchor-id="references-from-the-community">References from the Community</h2>
<p><a href="https://twitter.com/carrigmat">Matt</a> (ML Engineer at Hugging Face) says -</p>
<blockquote class="blockquote">
<p><em>[…] I did a few small projects to get familiar with Keras and then tried reimplementing papers or building examples to contribute to places like <a href="https://github.com/keras-team/keras-contrib">keras-contrib</a> or <a href="https://keras.io/">keras.io</a>.</em></p>
</blockquote>
<p><br></p>
<p><a href="https://twitter.com/algo_diver">Chansung</a> (ML-GDE and MLOps Engineer) says -</p>
<blockquote class="blockquote">
<p><em>[…] Anyways, I actually didn’t plan what to do for the next few years. I just have followed my interests and the joy to participate as a community member. And whenever I make any moves, I found other exciting events are waiting for me. These days, I am really enjoying creating open-source projects and applied ML products, and collaborative projects with you as well.</em></p>
</blockquote>
<p><br></p>
<p>Both of them continue to push their boundaries for self-improvement and are exceptional at what they do.</p>
</section>
<section id="finishing-up" class="level2">
<h2 class="anchored" data-anchor-id="finishing-up">Finishing Up</h2>
<p>The pursuit of betterment doesn’t stop after you land the job you were aspiring for. I continue to benefit from my open-source engagements even after professionally working in the area for some time now. I hope you’re able to take forward the pointers discussed in the post and experiment with them. If you have any suggestions for other interesting ways for independent ML practice please <a href="mailto:spsayakpaul@gmail.com">let me know</a>.</p>
</section>
<section id="acknowledgments" class="level2">
<h2 class="anchored" data-anchor-id="acknowledgments">Acknowledgments</h2>
<p>I want to thank all my wonderful collaborators and mentors who continue to inspire me to be better. I am also thankful to <a href="https://www.instagram.com/neerajanmusic/">Neerajan Saha</a> for proofreading this post.</p>


</section>

 ]]></description>
  <category>ml-practice</category>
  <category>jobs</category>
  <guid>https://sayak.dev/posts/ml-practice.html</guid>
  <pubDate>Fri, 27 May 2022 00:00:00 GMT</pubDate>
</item>
<item>
  <title>First Steps in GSoC</title>
  <link>https://sayak.dev/posts/gsoc-faqs.html</link>
  <description><![CDATA[ 






<p>In this post, I discuss my perspective on two primary questions pertaining to the <a href="https://summerofcode.withgoogle.com/">Google Summer of Code (GSoC) program</a>. Even though my work is centered around Machine Learning (ML), I believe these pointers are domain-agnostic. This is based on <a href="https://blog.tensorflow.org/2021/09/tensorflow-hubs-experience-with-gsoc-2021.html">my experience of mentoring for TensorFlow at GSoC 2021</a>. Please note that these thoughts are of my own and may not reflect what anyone else (including the program organizers and my employer) thinks.</p>
<section id="how-should-i-get-started" class="level2">
<h2 class="anchored" data-anchor-id="how-should-i-get-started">How should I get started?</h2>
<p>First and foremost, it’s important to acknowledge that GSoC requires some amount of open-source experience beforehand. That not only makes your application stronger but also sets you up for the program itself. But beyond everything else, having a genuine passion for contributing to open-source is important and is a key enabling factor. Open-source should be a fun engagement driven by your passion for helping a community. So, ensure you’re chasing the right things.</p>
<ul>
<li><p>Understand what GSoC is, how it works, what are the rules, and study some projects from the previous years by going to GSoC’s official website: ​​<a href="https://summerofcode.withgoogle.com/" class="uri">https://summerofcode.withgoogle.com/</a>.</p>
<p>Here’s <a href="https://developers.google.com/open-source/gsoc/resources/downloads/GSoC2022Presentation.pdf">another resource</a> that might be equally helpful. It’s important that you set your expectations right from the very beginning. Sometimes having conversations with the past GSoC contributors is really helpful in this regard. <br></p></li>
<li><p>Take a look at the <a href="https://summerofcode.withgoogle.com/programs/2022/organizations">organizations</a> taking part in GSoC. You’ll notice that they have all their projects listed for which they are welcoming contributions. <br></p></li>
<li><p>Study all the official resources that are out there for the project you want to contribute to. You may be interested in multiple projects but it helps to laser focus on one so that you can precisely figure out what components you’d want to work on, your timeline, etc. <br></p></li>
<li><p>Get started contributing. Here are some good examples that make it clear what a GSoC contributor should do first:</p>
<ul>
<li><a href="https://ardupilot.org/dev/docs/gsoc.html">ArduPilot</a></li>
<li><a href="https://gsoc.gnome.org/">GNOME</a></li>
<li><a href="https://pcp.io/gsoc/contributors.html">Performance Co-Pilot</a></li>
<li><a href="https://robocomp.github.io/web/gsoc/2022/contributor_guidance">RoboComp</a></li>
<li><a href="https://www.mediawiki.org/wiki/Google_Summer_of_Code/Participants#Application_process_steps">Wikimedia Foundation</a> <br><br></li>
</ul></li>
<li><p>Sometimes a project may not require having prior contribution experience but having it is almost always better. <a href="https://www.tensorflow.org/hub/overview">TensorFlow Hub</a> (TF-Hub) is one such example where you’re generally asked to code a ML model, (optionally) train it, and host the model on TF-Hub thereby making it easier for the community to use the model. <br><br></p></li>
<li><p>Lastly, if you haven’t worked with a version control system before definitely spend time doing that. Git is a good example of such a system and <a href="https://www.udacity.com/course/version-control-with-git--ud123">here’s</a> a good course from Udacity that can be helpful.</p></li>
</ul>
</section>
<section id="what-makes-a-proposal-great" class="level2">
<h2 class="anchored" data-anchor-id="what-makes-a-proposal-great">What makes a proposal great?</h2>
<p>Firstly, I’d like to acknowledge that the answers to this question should be subjective. That said, I think there are certain common aspects that are shared by all great GSoC proposals.</p>
<ul>
<li><p>GSoC is about building things. So including your experience that reflects the same immediately catches the eye. You build that experience over a period of time, it’s not something built overnight. That way, your experience speaks about a few things: consistency, technical depth, punctuality, communication, etc. Let me provide some examples. <br><br> Say, you wanted to contribute to <a href="https://github.com/keras-team/keras-cv/">KerasCV</a> by adding a new layer(s) to it. If you can show that you’ve already worked on something that reflects the experience relevant to the contribution, it puts you in a better position than someone without that experience.</p>
<p>Similarly, if you wanted to contribute a model to TF-Hub, it helps to show that you’ve experience implementing models and relevant things such as layers, blocks, etc. <br></p></li>
<li><p>When you’re talking about an experience in your proposal be sure to back it with verifiable links. Without that, the mention becomes practically void. <br></p></li>
<li><p>Don’t just mention the components of the project you’d like to work on. Include all the nitty-gritty of that – why you’d like to work on them and why it’s useful, what your approaches will be, etc. If you anticipate some edge cases or blockers include them too. This speaks volumes about your maturity. <br></p></li>
<li><p>Keep your proposal grammatically correct and easily understandable. This helps you communicate your proposal better. Remember that it’s your responsibility to ensure that your proposal was communicated in an expected way. <br><br></p></li>
<li><p>Sometimes, applications come with incomplete sentences, inconsistency in sentence casing, without punctuations, etc. This is an anti-pattern. Try hard to ensure your proposal doesn’t have those things. This may readily reduce the seriousness of your proposal and the work you put into it. <br><br></p></li>
<li><p>Include a realistic timeline that covers the project deliverables and includes enough time for you and the mentors to communicate effectively. Unexpected things can happen all the time so, it helps to also include some extra time to dedicate to those situations.</p></li>
</ul>
<p>Sometimes, a project may welcome ideas from the contributors. If you’d like to propose something that’s already not enlisted in a project, be sure to reach out to the project mentor to discuss the feasibility of your idea before working on the proposal.</p>
</section>
<section id="additional-notes" class="level2">
<h2 class="anchored" data-anchor-id="additional-notes">Additional notes</h2>
<p>During <a href="https://summerofcode.withgoogle.com/archive/2021/organizations/6649841832165376">GSoC 2021</a>, I had the opportunity to work with Aditya Kane and Vausdev Gupta as their mentor. Here are their GSoC proposals:</p>
<ul>
<li><a href="https://docs.google.com/document/d/1h9kZCywWWveekUFH1SS5rBZINK-W9SDb/edit">Aditya</a></li>
<li><a href="https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/assets/milestone.pdf">Vasudev</a></li>
</ul>
<p>I’m fortunate to be mentoring for TensorFlow at GSoC 2022 as well. If you’re curious, I have a <a href="https://www.youtube.com/watch?v=VsBEFUoESR4&amp;t=3123s">presentation</a> that discusses how open-source can enable different possibilities in the ML world. If you’re looking for an example of a non-ML proposal, then <a href="https://drive.google.com/file/d/1c-nqgm54pIvm_YQKJ4SohLUV6iKGGYH6/view">Anubhav Singh’s proposal</a> is a great example.</p>
<p>Additionally, we penned down our mentorship experience in <a href="https://blog.tensorflow.org/2021/09/tensorflow-hubs-experience-with-gsoc-2021.html">this blog post</a> that may provide additional context.</p>


</section>

 ]]></description>
  <category>gsoc</category>
  <category>open-source</category>
  <guid>https://sayak.dev/posts/gsoc-faqs.html</guid>
  <pubDate>Sun, 13 Mar 2022 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Publishing ConvNeXt Models on TensorFlow Hub</title>
  <link>https://sayak.dev/posts/convnext-tfhub.html</link>
  <description><![CDATA[ 






<p>I recently added 15 different variants of the <a href="https://arxiv.org/abs/2201.03545">ConvNeXt architecture</a> to <a href="https://tfhub.dev/sayakpaul/collections/convnext/1">TensorFlow Hub</a> (TF-Hub). This post is a reflection of what had to be done to get to that point. First, we’ll discuss the implementation of ConvNeXt in Keras and how the original pre-trained parameters were ported into these models. We’ll then talk about TF-Hub’s ConvNeXt collection and what it offers.</p>
<p>I hope this post is useful for anyone willing to contribute models to TF-Hub as doing it the right way can be a good amount of work.</p>
<section id="about-convnext" class="level1">
<h1>About ConvNeXt</h1>
<p>ConvNeXt models were proposed by Liu et al.&nbsp;in <a href="https://arxiv.org/abs/2201.03545">A ConvNet for the 2020s</a>. ConvNeXt models are composed of standard layers such as depthwise convolutions, layer normalization, etc., and use standard network topologies. They don’t use self-attention or any hybrid approaches, unlike the recent architectures such as <a href="https://arxiv.org/abs/2010.11929">Vision Transformers</a>, <a href="https://arxiv.org/abs/2106.04803">CoAtNet</a>, etc. The authors start with a base architecture and gradually refine it to match some of the design choices of <a href="https://arxiv.org/abs/2103.14030">Swin Transformers</a>. In the process, they developed a family of models named ConvNeXt achieving performance on the <a href="https://www.image-net.org/">ImageNet-1k dataset</a> with efficiency. For details, check out the <a href="https://arxiv.org/abs/2201.03545">original paper</a>.</p>
<img src="https://github.com/sayakpaul/portfolio/raw/master/posts/convnext_tfhub/convnext.png" class="img-fluid">
<center>
<b>Figure 1</b>: ConvNeXt performance (source: original paper).
</center>
</section>
<section id="implementation-and-weight-porting" class="level1">
<h1>Implementation and weight porting</h1>
<p>The ConvNeXt models are fairly easy to implement especially with the <a href="https://github.com/facebookresearch/ConvNeXt">official PyTorch codebase available</a> for reference. As mentioned before, these models can be implemented using the standard components provided in most of the major deep learning frameworks such as JAX, PyTorch, and TensorFlow.</p>
<p>ConvNeXt models use the following block structure with layer scaling as introduced in <a href="https://arxiv.org/abs/2103.17239">Going deeper with image transformers</a> by Touvron et al.</p>
<center>
<figure class="figure">
<img src="https://github.com/sayakpaul/portfolio/raw/master/posts/convnext_tfhub/convnext_block.png" class="figure-img">
</figure>
</center>
<center>
<b>Figure 2</b>: ConvNeXt block (source: original paper).
</center>
<p>The skip connection is controlled with <a href="https://arxiv.org/abs/1603.09382">Stochastic Depth</a> to induce regularization <em>during</em> training. Different ConvNeXt variants correspond to different depths along with different channels used in each of the stages. For example, the “tiny” variant uses the following setup:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb1-1">depths <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>]</span>
<span id="cb1-2">dims <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">96</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">192</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">384</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">768</span>]</span></code></pre></div></div>
<p>If you plan to populate the implemented models with the original parameters then it helps to align the architecture implementation with the official one as much as possible. Since I went with this approach I tried closely following <a href="https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py">the official implementation</a>. My final implementation is available in <a href="https://github.com/sayakpaul/ConvNeXt-TF/blob/main/models/convnext_tf.py">this script</a>. Note that, it does not yet include the <a href="https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext_isotropic.py">isotropic ConvNeXt models</a>.</p>
<p>Coming to the weight porting part, this is usually the most interesting part because there’s no standard recipe that’d work for all the models. You’ll need to think about how to best align the original model parameters with your implementation.</p>
<p>A ConvNeXt model is divided into three main parts: (1) stem which directly operates on the input image, (2) downsample blocks that reduce the resolution of feature maps as the network progresses, and (3) stages that apply the ConvNeXt blocks shown above. This is why I organized my weight porting script such that it has a correspondence between these different parts with the original parameters. Here is an example:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> layer <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> stem_block.layers:</span>
<span id="cb2-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(layer, tf.keras.layers.Conv2D):</span>
<span id="cb2-3">        layer.kernel.assign(</span>
<span id="cb2-4">            tf.Variable(param_list[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].numpy().transpose(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>))</span>
<span id="cb2-5">        )</span>
<span id="cb2-6">        layer.bias.assign(tf.Variable(param_list[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].numpy()))</span>
<span id="cb2-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(layer, tf.keras.layers.LayerNormalization):</span>
<span id="cb2-8">        layer.gamma.assign(tf.Variable(param_list[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>].numpy()))</span>
<span id="cb2-9">        layer.beta.assign(tf.Variable(param_list[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>].numpy()))</span></code></pre></div></div>
<p>The most difficult bit was figuring out how to properly populate the weights of the convolutional layers in TensorFlow from PyTorch. In an earlier implementation, I was simply using <code>transpose()</code>. The resulting models were giving poorer performance than expected. <a href="https://in.linkedin.com/in/vasudevgupta7">Vasudev</a> helped me figure out the correct transposition of the weight axes and the models were then coming out as expected. More about the evaluation of these models in a moment.</p>
<p>Once the weights were ported successfully, the next task was to verify if the outputs of the intermediate layers matched with their original counterparts. One minor detail to note here is that the outputs of layers are <strong>not</strong> the same as their parameters. So, even if you check if the parameters of your implemented model and the original model are matching, their outputs could still mismatch. This mainly happens because of mismatches between the layer configurations of your model and the original one.</p>
<p>The final model conversion script is available <a href="https://github.com/sayakpaul/ConvNeXt-TF/blob/main/convert.py">here</a>.</p>
</section>
<section id="evaluation-of-the-models" class="level1">
<h1>Evaluation of the models</h1>
<p>To be more certain, it’s also important to check the evaluation metrics of the converted models on the datasets used during training. In this case, we need to use the top-1 accuracy of the models on the ImageNet-1k dataset (validation set).</p>
<p>To set up this evaluation, I developed <a href="https://github.com/sayakpaul/ConvNeXt-TF/blob/main/i1k_eval/eval.ipynb">this notebook</a> where I closely followed <a href="https://github.com/facebookresearch/ConvNeXt/blob/main/datasets.py">the preprocessing used in the official codebase</a> for inference. The following table reflects the top-1 accuracies of the converted models along with the original scores reported <a href="https://github.com/facebookresearch/ConvNeXt/#results-and-pre-trained-models">here</a>.</p>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
</style>
<table class="tg">
<thead>
<tr>
<th class="tg-c3ow">
<span style="font-weight:bold">Name</span>
</th>
<th class="tg-c3ow">
<span style="font-weight:bold">Original acc@1</span>
</th>
<th class="tg-c3ow">
<span style="font-weight:bold">Keras acc@1</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-c3ow">
convnext_tiny_1k_224
</td>
<td class="tg-c3ow">
82.1
</td>
<td class="tg-c3ow">
81.312
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_small_1k_224
</td>
<td class="tg-c3ow">
83.1
</td>
<td class="tg-c3ow">
82.392
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_base_1k_224
</td>
<td class="tg-c3ow">
83.8
</td>
<td class="tg-c3ow">
83.28
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_base_1k_384
</td>
<td class="tg-c3ow">
85.1
</td>
<td class="tg-c3ow">
84.876
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_large_1k_224
</td>
<td class="tg-c3ow">
84.3
</td>
<td class="tg-c3ow">
83.844
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_large_1k_384
</td>
<td class="tg-c3ow">
85.5
</td>
<td class="tg-c3ow">
85.376
</td>
</tr>
<tr>
<td class="tg-c3ow">
</td>
<td class="tg-c3ow">
</td>
<td class="tg-c3ow">
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_base_21k_1k_224
</td>
<td class="tg-c3ow">
85.8
</td>
<td class="tg-c3ow">
85.364
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_base_21k_1k_384
</td>
<td class="tg-c3ow">
86.8
</td>
<td class="tg-c3ow">
86.79
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_large_21k_1k_224
</td>
<td class="tg-c3ow">
86.6
</td>
<td class="tg-c3ow">
86.36
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_large_21k_1k_384
</td>
<td class="tg-c3ow">
87.5
</td>
<td class="tg-c3ow">
87.504
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_xlarge_21k_1k_224
</td>
<td class="tg-c3ow">
87.0
</td>
<td class="tg-c3ow">
86.732
</td>
</tr>
<tr>
<td class="tg-c3ow">
convnext_xlarge_21k_1k_384
</td>
<td class="tg-c3ow">
87.8
</td>
<td class="tg-c3ow">
87.68
</td>
</tr>
</tbody>
</table>
<p><code>Keras acc@1</code> refers to the scores of my implementation. Differences in the results are primarily because of the differences in the library implementations, especially how image resizing is implemented in PyTorch and TensorFlow. My evaluation logs are available at <a href="https://tensorboard.dev/experiment/odN7OPCqQvGYCRpJP1GhRQ/">this URL</a>. I’d like to thank <a href="https://twitter.com/gusthema">Gus</a> from the TF-Hub team for the productive discussions during this phase.</p>
</section>
<section id="publishing-on-tf-hub" class="level1">
<h1>Publishing on TF-Hub</h1>
<p>With the models converted as expected, I was now tasked with publishing them on TF-Hub. These models can be categorized into two different variants: (1) off-the-shelf classifiers and (2) feature extractors used for downstream tasks. This means that the 15 model variants that I had converted would actually amount to 30 models.</p>
<p>Whenever I publish models on TF-Hub, I try to accompany each model with the following:</p>
<ul>
<li>Documentation that includes references of the models, how it was exported, etc.</li>
<li>Colab Notebook showing the model usage.</li>
</ul>
<p>Doing these things (especially the documentation part) for 30 models can be quite cumbersome. <a href="https://www.linkedin.com/in/willi-gierke/">Willi</a> from the TF-Hub team supported me in automatically generating documentation for 30 models. The script is available <a href="https://github.com/sayakpaul/ConvNeXt-TF/blob/main/hub_utilities/generate_doc.py">here</a>. This script was basically generated from a documentation template and can be used for generating documentation when publishing more than one model. Additionally, I worked on a <a href="https://github.com/sayakpaul/ConvNeXt-TF/blob/main/hub_utilities/export_to_hub.py">script</a> that can archive the TensorFlow SavedModels in a way accepted by TF-Hub.</p>
<p>I hope these scripts will be beneficial for anyone planning to contribute models to TF-Hub.</p>
<p>As of today, all 30 models are <a href="https://tfhub.dev/sayakpaul/collections/convnext/1">available on TF-Hub</a>. They come with Colab Notebooks and documentation so that it’s easier to get started. Moreover, these TF-Hub models are not black-box SavedModels. You can load them as <code>tf.keras.Model</code> objects for further inspection. Here’s an example:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb3-1">model_gcs_path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gs://tfhub-modules/sayakpaul/convnext_tiny_1k_224/1/uncompressed"</span></span>
<span id="cb3-2">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.models.load_model(model_gcs_path)</span>
<span id="cb3-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(model.summary(expand_nested<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>))</span></code></pre></div></div>
</section>
<section id="wrapping-up" class="level1">
<h1>Wrapping up</h1>
<p>That’s it for the post. We discussed a standard workflow that I use to publish models on TF-Hub and the difficulties that can arise during the process. I hope you’ve found it to be worthy of your time and thank you for reading!</p>
</section>
<section id="acknowledgements" class="level1">
<h1>Acknowledgements</h1>
<ul>
<li><a href="https://github.com/vasudevgupta7">Vasudev</a> for helping with transposition bug</li>
<li><a href="https://twitter.com/gusthema">Gus</a> for fruitful discussions</li>
<li><a href="https://www.linkedin.com/in/willi-gierke/">Willi</a> for helping with publishing</li>
<li><a href="https://developers.google.com/programs/experts/">ML-GDE program</a> for providing Google Cloud Platform credits</li>
</ul>


</section>

 ]]></description>
  <category>tensorflow</category>
  <category>keras</category>
  <category>cnns</category>
  <category>imagenet-1k</category>
  <category>convnext</category>
  <guid>https://sayak.dev/posts/convnext-tfhub.html</guid>
  <pubDate>Thu, 03 Feb 2022 00:00:00 GMT</pubDate>
  <media:content url="https://sayak.dev/posts/convnext_tfhub/convnext_header.png" medium="image" type="image/png" height="93" width="144"/>
</item>
<item>
  <title>Building and Installing OpenCV 4.5.0 on an M1 Mac</title>
  <link>https://sayak.dev/posts/install-opencv-m1.html</link>
  <description><![CDATA[ 






<p>This post shows how to build and install OpenCV 4.5.0 on a MacBook Pro that comes with an <a href="https://www.apple.com/in/mac/m1/">M1 chip</a>. Yes, you guessed it right - as of <strong>January 01, 2021</strong>, there’s no pre-compiled OpenCV binary compatible with this MacBook Pro variant. So, open up a terminal and get started!</p>
<p>Here’s a brief summary of the configuration of my MacBook -</p>
<p><img src="https://i.ibb.co/XDZZQ4t/image.png" class="img-fluid"></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The following steps should run well on an M1 MacBook Air too.</p>
</div>
</div>
<section id="install-xcode-and-homebrew" class="level2">
<h2 class="anchored" data-anchor-id="install-xcode-and-homebrew">Install Xcode and Homebrew</h2>
<p>We start by executing <code>sudo xcodebuild -license</code> from a terminal.</p>
<p>When you execute the above command, you would need to accept the Xcode license. Then, in order to make use of Apple command line tools, we need to install it - <code>sudo xcode-select --install</code>.</p>
<p>Homebrew manages packages on a Mac. In order to install it execute the following - <code>/usr/bin/ruby -e "%(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"</code>.</p>
<p>You would want to add the command brew after the installation is complete. To do so, execute the following - <code>nano ~/.zshrc</code></p>
<p>Then insert <code>export PATH=$PATH:/opt/homebrew/bin</code> into it and press <code>Ctrl + X</code> from your keyboard. Then execute <code>source ~/.zshrc</code> from the terminal.</p>
<p>Note that the exact path to Homebrew might be different for your system, so please double check that.</p>
<p>Next up, we install a few system-level utilities required by OpenCV on a Mac.</p>
</section>
<section id="install-conda" class="level2">
<h2 class="anchored" data-anchor-id="install-conda">Install conda</h2>
<p>My favorite Python virtual environment manager is virtualenv. Unfortunately, it does not play out that well with the new M1 chip. This is mostly because the pip-installable packages often break during their installations on the chip. This is why conda, specifically its <strong>miniforge</strong> distribution is the recommended package manager for a Mac shipped with M1. You can install it from <a href="https://github.com/conda-forge/miniforge#miniforge3">here</a>. This installs <strong>Python 3.8</strong>.</p>
<p>After the installation is complete, please create a new Python virtual environment by executing <code>conda create --name &lt;environment_name&gt;</code>. Then activate it by running <code>conda activate  &lt;environment_name&gt;</code>.</p>
<p>Running <code>conda install -y python==3.8.6</code> will install a few common Python packages for you. I highly recommend running this.</p>
</section>
<section id="install-numpy" class="level2">
<h2 class="anchored" data-anchor-id="install-numpy">Install NumPy</h2>
<p>NumPy is needed by OpenCV. So, we need to install it before we build and install OpenCV. Apple provides a <code>numpy</code> wheel that is compatible with the M1 chip. Follow the steps below to install it -</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> wget https://github.com/apple/tensorflow_macos/releases/download/v0.1alpha0/tensorflow_macos-0.1alpha0.tar.gz</span>
<span id="cb1-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> tar xvf tensorflow_macos-0.1alpha0.tar.gz</span>
<span id="cb1-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> cd tensorflow_macos/arm64</span>
<span id="cb1-4"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> pip install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--upgrade</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--no-dependencies</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--force</span> numpy-1.18.5-cp38-cp38-macosx_11_0_arm64.whl </span>
<span id="cb1-5"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> cd ~</span></code></pre></div></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Be sure to activate your conda environment before doing the pip-install.</p>
</div>
</div>
</section>
<section id="compile-opencv" class="level2">
<h2 class="anchored" data-anchor-id="compile-opencv">Compile OpenCV</h2>
<p>First, let’s download the OpenCV and OpenCV extended module files and prepare them for compilation.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> wget <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-O</span> opencv.zip https://github.com/opencv/opencv/archive/4.5.0.zip</span>
<span id="cb2-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> wget <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-O</span> opencv_contrib.zip https://github.com/opencv/opencv_contrib/archive/4.5.0.zip</span>
<span id="cb2-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> unzip opencv.zip</span>
<span id="cb2-4"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> unzip opencv_contrib.zip</span>
<span id="cb2-5"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> cd opencv-4.5.0</span>
<span id="cb2-6"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> mkdir build <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">cd</span> build</span></code></pre></div></div>
<p>Now, we are all set to fire the <code>cmake</code> command that would build OpenCV for us. Let’s review it briefly -</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> cmake <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-2">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-DCMAKE_SYSTEM_PROCESSOR</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>arm64 <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-3">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-DCMAKE_OSX_ARCHITECTURES</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>arm64 <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-4">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-DWITH_OPENJPEG</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OFF <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-5">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-DWITH_IPP</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OFF <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-6">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> CMAKE_BUILD_TYPE=RELEASE <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-7">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> CMAKE_INSTALL_PREFIX=/usr/local <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-8">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> OPENCV_EXTRA_MODULES_PATH=/Users/sayakpaul/Downloads/opencv_contrib-4.5.0/modules <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-9">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> PYTHON3_EXECUTABLE=/Users/sayakpaul/miniforge3/envs/dev/bin/python3 <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-10">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> BUILD_opencv_python2=OFF <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-11">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> BUILD_opencv_python3=ON <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-12">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> INSTALL_PYTHON_EXAMPLES=ON <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-13">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> INSTALL_C_EXAMPLES=OFF <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-14">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> OPENCV_ENABLE_NONFREE=ON <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb3-15">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-D</span> BUILD_EXAMPLES=ON ..</span></code></pre></div></div>
<p>As per <a href="https://github.com/opencv/opencv/issues/18049#issuecomment-671878454">this issue comment</a>, <code>DCMAKE_SYSTEM_PROCESSOR</code>, <code>DCMAKE_OSX_ARCHITECTURES</code>, <code>DWITH_OPENJPEG</code>, and <code>DWITH_IPP</code> are needed to be set during the compilation step. Also, please pay attention to the following arguments - <code>OPENCV_EXTRA_MODULES_PATH</code> and <code>PYTHON3_EXECUTABLE</code>. For these two arguments, you would want to first determine the paths and then supply them accordingly.</p>
<p>Now, before you run the above <code>cmake</code> command, activate the conda environment you created in an earlier step (<code>conda activate &lt;environment_name&gt;</code>) if you haven’t already. The compilation took <em>~3 minutes</em> for me and it should produce outputs like so -</p>
<p><img src="https://i.ibb.co/YdpBSh0/image.png" class="img-fluid"></p>
<p>Next, we launch the make command - <code>make -j8</code>. With all the eight cores (<code>j8</code> stands for eight cores here) chugging along, this step took <em>~8 minutes</em> for me. You can adjust the <code>j</code> option with respect to the hardware available. After it’s done you should get an output like so -</p>
<p><img src="https://i.ibb.co/yFJq4jJ/image.png" class="img-fluid"></p>
<p>The final step here is to execute - <code>sudo make install</code>. It should take just a few seconds to complete execution. Upon successful completion, you should get an output like so -</p>
<p><img src="https://i.ibb.co/Pzzmxy4/image.png" class="img-fluid"></p>
</section>
<section id="sym-link-opencv-4-on-macos-to-virtual-environment-site-packages" class="level2">
<h2 class="anchored" data-anchor-id="sym-link-opencv-4-on-macos-to-virtual-environment-site-packages">Sym-link OpenCV 4 on macOS to virtual environment <code>site-packages</code></h2>
<p>To do this, we first need to locate the <code>.so</code> file generated during the compilation step. We can do this with the <code>mdfind</code> command -</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> mdfind cv2.cpython</span>
<span id="cb4-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">/.../opencv-4.5.0/build/lib/python3/cv2.cpython-38-darwin.so</span></span>
<span id="cb4-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">/usr/local/lib/python3.8/site-packages/cv2/python-3.8/cv2.cpython-38-darwin.so</span></span>
<span id="cb4-4"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">...</span></span></code></pre></div></div>
<p>Please note that I obfuscated some parts of the outputs for privacy reasons. In the above output, we can see the absolute locations for the <code>.so</code> files that were generated. Now, we need to execute the following to sym-link one of the <code>.so</code> files in our current Python virtual environment -</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> cd /.../miniforge3/envs/dev/lib/python3.8/site-packages</span>
<span id="cb5-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> ln <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-s</span> /usr/local/lib/python3.8/site-packages/cv2/python-3.8/cv2.cpython-38-darwin.so cv2.so</span></code></pre></div></div>
<p>Please double-check the paths before executing the commands.</p>
<p>And that’s it!</p>
<p>You can test the installation by executing the following -</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">%</span> conda activate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span>environment_name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">(</span><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">you</span> haven<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'t already)</span></span>
<span id="cb6-2"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">% python</span></span>
<span id="cb6-3"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">&gt;&gt;&gt; import cv2</span></span>
<span id="cb6-4"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">&gt;&gt;&gt; cv2.__version__</span></span></code></pre></div></div>
<p>It should print <code>'4.5.0'</code>.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://www.pyimagesearch.com/2018/08/17/install-opencv-4-on-macos/">Install OpenCV 4 on macOS</a></li>
<li><a href="https://towardsdatascience.com/tensorflow-2-4-on-apple-silicon-m1-installation-under-conda-environment-ba6de962b3b8">TensorFlow 2.4 on Apple Silicon M1 : installation under Conda environment</a></li>
</ul>


</section>

 ]]></description>
  <guid>https://sayak.dev/posts/install-opencv-m1.html</guid>
  <pubDate>Fri, 01 Jan 2021 00:00:00 GMT</pubDate>
  <media:content url="https://sayak.dev/posts/opencv_m1.png" medium="image" type="image/png" height="87" width="144"/>
</item>
<item>
  <title>A Battle of Text Detectors for Mobile Deployments: CRAFT vs. EAST</title>
  <dc:creator>Sayak Paul</dc:creator>
  <dc:creator>Tulasi Ram Laghumavarapu</dc:creator>
  <link>https://sayak.dev/posts/optimizing-text-detectors.html</link>
  <description><![CDATA[ 






<p>In the <a href="https://tulasi.dev/craft-in-tflite">previous post</a>, we saw how to convert the pre-trained <a href="https://arxiv.org/pdf/1904.01941">CRAFT</a> model from PyTorch to TensorFlow Lite (TFLite) and run inference with the converted TFLite model. In this post, we will be comparing the TFLite variants of the CRAFT model to another text detection model - <a href="https://arxiv.org/abs/1704.03155">EAST</a>. The objective of this post is to provide a comparative study between these two models with respect to various deployment-specific pointers such as inference latency, model size, performance on dense text regions, and so on. Text detection continues to be a very important use-case across many verticals. So we hope this post will serve as a systematic guide for developers that are interested to explore on-device text detection models.</p>
<p>Precisely, we will be comparing the two models on the basis of the following pointers which we think are very crucial when it comes to deploying them out in the wild -</p>
<ul>
<li>Visual Inspection of Performance</li>
<li>Model Size</li>
<li>Inference Latency</li>
<li>Memory Usage</li>
</ul>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you are interested to know about the conversion process and inference pipelines of the models, please refer to these notebooks - <a href="https://github.com/tulasiram58827/craft_tflite/tree/main/colabs">CRAFT</a> and <a href="https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/EAST_TFLite.ipynb">EAST</a>. The pre-converted models are available on TensorFlow Hub - <a href="https://tfhub.dev/tulasiram58827/lite-model/craft-text-detector/dr/1">CRAFT</a> and <a href="https://tfhub.dev/sayakpaul/lite-model/east-text-detector/dr/1">EAST</a>.</p>
</div>
</div>
<section id="benchmark-setup" class="level2">
<h2 class="anchored" data-anchor-id="benchmark-setup">Benchmark Setup</h2>
<p>We used the <a href="https://www.tensorflow.org/lite/performance/measurement">TensorFlow Lite Benchmark tool</a> in order to gather results on inference latency and memory usage of the models with <strong>Redmi K20 Pro</strong> as the target device. We chose a mobile device for this purpose because text detection is a pretty prevalent recipe of many mobile applications such as <a href="https://play.google.com/store/apps/details?id=com.google.ar.lens&amp;hl=en_IN&amp;gl=US">Google Lens</a>.</p>
<p>In order to make the comparisons fair, we consider the two models with three different image resolutions - 320x320, 640x416, and 1200x800. For each of these resolutions, we consider two different <a href="https://www.tensorflow.org/lite/performance/post_training_quantization">post-training quantization schemes</a> - dynamic-range and float16. <em>The CRAFT model conversion is not yet supported in the integer variant, hence we do not consider integer quantization (but the EAST model does support it)</em>.</p>
</section>
<section id="visual-inspection-of-performance" class="level2">
<h2 class="anchored" data-anchor-id="visual-inspection-of-performance">Visual Inspection of Performance</h2>
<p>In this setting, we run both of the models and their different variants (dynamic-range and float16 quantized) on a sample image that has dense text regions, and then we visualize the results. We observed that both of these models perform fairly well on images having lighter text regions. Here’s the sample image we used for the purpose -</p>
<p><img src="https://i.ibb.co/KVKnnct/image.png" class="img-fluid"></p>
<center>
<small>Image is taken from the <a href="https://rrc.cvc.uab.es/?ch=13">SROIE dataset</a>.</small><br>
</center>
<p>Time to detect some texts!</p>
<section id="craft---320x320-dynamic-range-float16" class="level3">
<h3 class="anchored" data-anchor-id="craft---320x320-dynamic-range-float16">CRAFT - 320x320 Dynamic-Range &amp; float16</h3>
<p>In the dynamic-range quantization setting, we can see the model misses out on some text blocks.</p>
<p><img src="https://i.ibb.co/RBX8XDn/image-w-593-h-442-rev-1-ac-1-parent-19-Qb-WABWc-E3n-SLPE6zqm-Tw6b-Vxkxee-YUw-Om-KTDn-Dz8k-Q.png" class="img-fluid"></p>
<center>
<small>Inference results from the 320x320 dynamic-range and float16 quantized CRAFT models.</small><br>
</center>
<p>With increased numerical precision i.e.&nbsp;float16, we can clearly see quite a bit of improvement in the results. It’s important to note that this improvement comes at the cost of increased model size.</p>
<p>Next up, we apply the same steps to the EAST model.</p>
</section>
<section id="east---320x320-dynamic-range-float16" class="level3">
<h3 class="anchored" data-anchor-id="east---320x320-dynamic-range-float16">EAST - 320x320 Dynamic-Range &amp; float16</h3>
<p>EAST apparently performs better than CRAFT under dynamic-range quantization. If we look closely, it appears that the CRAFT model produces far fewer overlaps in the detections compared to EAST. On the other hand, the EAST model is able to detect more text blocks. When developing practical applications with text detectors, it often becomes a classic case of <em>precision-recall</em> trade-offs like the one we are currently seeing. So, you would want to consider the application-specific needs in order to decide the level of trade-off to be achieved there.</p>
<p><img src="https://i.ibb.co/qsCMC5N/image-w-624-h-520-rev-37-ac-1-parent-19-Qb-WABWc-E3n-SLPE6zqm-Tw6b-Vxkxee-YUw-Om-KTDn-Dz8k-Q.png" class="img-fluid"></p>
<center>
<small>Inference results from the 320x320 dynamic-range and float16 quantized EAST models.</small><br>
</center>
<p>With increased precision, the above-mentioned points still hold, i.e.&nbsp;the number of overlaps being way higher for the EAST model than they are in the CRAFT equivalent. In this setting (float16 quantization), superiority in the performance of the CRAFT model is quite evident in regards to the EAST model.</p>
<p>As different applications may use different image resolutions we decided to test the performance of the models on larger dimensions as well. This is what we are going to see next.</p>
</section>
<section id="craft---640x416-dynamic-range-float16" class="level3">
<h3 class="anchored" data-anchor-id="craft---640x416-dynamic-range-float16">CRAFT - 640x416 Dynamic-Range &amp; float16</h3>
<p>On an increased resolution, the CRAFT model performs pretty well -</p>
<p><img src="https://i.ibb.co/VxbyWch/image-w-624-h-568-rev-38-ac-1-parent-19-Qb-WABWc-E3n-SLPE6zqm-Tw6b-Vxkxee-YUw-Om-KTDn-Dz8k-Q.png" class="img-fluid"></p>
<center>
<small>Inference results from the 640x416 dynamic-range and float16 quantized CRAFT models.</small><br>
</center>
<p>The float16 version of this resolution is a slam dunk (rightfully leaving behind the barcode which is not a piece of text).</p>
</section>
<section id="east---640x416-dynamic-range-float16" class="level3">
<h3 class="anchored" data-anchor-id="east---640x416-dynamic-range-float16">EAST - 640x416 Dynamic-Range &amp; float16</h3>
<p>The performance of the EAST model under these settings are very equivalent to CRAFT -</p>
<p><img src="https://i.ibb.co/ynBbrFZ/image-w-597-h-612-rev-36-ac-1-parent-19-Qb-WABWc-E3n-SLPE6zqm-Tw6b-Vxkxee-YUw-Om-KTDn-Dz8k-Q.png" class="img-fluid"></p>
<center>
<small>Inference results from the 640x416 dynamic-range and float16 quantized EAST models.</small><br>
</center>
<p>With float16 quantization and 640x416 as the resolution, the CRAFT model is a clear winner. Notice that the EAST model is still unable to discard the barcode part which might be an important point to note for some applications.</p>
<p>Time to inspect the results for our final and highest resolution - 1280x800.</p>
</section>
<section id="craft---1280x800-dynamic-range-float16" class="level3">
<h3 class="anchored" data-anchor-id="craft---1280x800-dynamic-range-float16">CRAFT - 1280x800 Dynamic-Range &amp; float16</h3>
<p>Under dynamic-range quantization, the results look okayish. The model misses out on a number of text blocks but the only ones that it detects appear to be neat.</p>
<p><img src="https://i.ibb.co/QMDpH9M/image-w-624-h-453-rev-34-ac-1-parent-19-Qb-WABWc-E3n-SLPE6zqm-Tw6b-Vxkxee-YUw-Om-KTDn-Dz8k-Q.png" class="img-fluid"></p>
<center>
<small>Inference results from the 1280x800 dynamic-range and float16 quantized CRAFT models.</small><br>
</center>
<p>The results from the float16 variant are tremendous (as you probably have guessed by now).</p>
</section>
<section id="east---1280x800-dynamic-range-float16" class="level3">
<h3 class="anchored" data-anchor-id="east---1280x800-dynamic-range-float16">EAST - 1280x800 Dynamic-Range &amp; float16</h3>
<p>At this resolution, the EAST model seems to be performing well too -</p>
<p><img src="https://i.ibb.co/xYHfXXn/image-w-624-h-483-rev-29-ac-1-parent-19-Qb-WABWc-E3n-SLPE6zqm-Tw6b-Vxkxee-YUw-Om-KTDn-Dz8k-Q.png" class="img-fluid"></p>
<center>
<small>Inference results from the 1280x800 dynamic-range and float16 quantized EAST models.</small><br>
</center>
<p>With float16 quantization as well, the CRAFT model beats EAST in terms of the detection quality.</p>
</section>
</section>
<section id="model-size" class="level2">
<h2 class="anchored" data-anchor-id="model-size">Model Size</h2>
<p>When it comes to deploying models to mobile devices model size becomes a really important factor. You may not want to have a heavy model that would, in turn, make your mobile application bulky. Moreover, <a href="https://support.google.com/googleplay/android-developer/answer/113469#apk">Playstore</a> and <a href="https://developer.apple.com/forums/thread/12455">AppStore</a> also have size restrictions on the applications one can host there.</p>
<p>On the other hand, heavier models tend to be slower. If your application cannot have increased inference latency then you would want to have the model size as low as possible.</p>
<p>The following figure shows the size of the CRAFT and EAST models -</p>
<p><img src="https://i.ibb.co/tX7bknk/nyrm-wh-z-itikr9-cnyl6-z1-fq3.png" class="img-fluid"></p>
<center>
<small>Model (TFLite variants) sizes of CRAFT and EAST.</small><br>
</center>
<p>The dynamic-range quantized versions of both the models are in a well-acceptable range with respect to size. However, the float16 variants may still be a bit heavier for some applications.</p>
</section>
<section id="inference-latency" class="level2">
<h2 class="anchored" data-anchor-id="inference-latency">Inference Latency</h2>
<p>Inference latency is also one of the major factors for mobile-based deployments especially when your applications might require instantaneous predictions. We are going to show a comparison between all the settings we considered in the visual inspection section.</p>
<p>To reiterate we performed the benchmarks for this section on a Redmi K20 Pro using 4 threads. In the following figures, we present inference latency of different variants of the CRAFT and EAST models.</p>
<p><img src="https://i.ibb.co/1GyPgR6/ylz3-vh2l-ownf4av-amai-w0j-oz.png" class="img-fluid"></p>
<center>
<small>Inference latency of different variants of the CRAFT model.</small><br>
</center>
<p><img src="https://i.ibb.co/ySBsQvs/z-q-o-zf7cl-hu-tfh-ou-a7-yscgm.png" class="img-fluid"></p>
<center>
<small>Inference latency of different variants of the EAST model.</small><br>
</center>
<p>As expected, with increased resolution the inference latency also increases. Inference latency is also quite lower for all the variants of the EAST model compared to CRAFT. Earlier we saw how a quantization affects model performance under a particular resolution. As stated earlier, when using these models inside a mobile application, the “<em>Size vs.&nbsp;Performance</em>” trade-off becomes extremely vital.</p>
<blockquote class="blockquote">
<p>important: The results for the float16 1280x800 CRAFT model could not be obtained on our target device.</p>
</blockquote>
</section>
<section id="memory-usage" class="level2">
<h2 class="anchored" data-anchor-id="memory-usage">Memory Usage</h2>
<p>In section, we shed light on the total memory allocated for the models while running the TensorFlow Lite Benchmark tool. Knowing about the memory usage of these models helps us plan application releases accordingly as not all the mobile phones may support extensive memory requirements. So based on this information, you may want to set some device requirements for your application using these models. On the other hand, if you would want your application to be as device-agnostic as possible then you may want to maintain separate models according to their size and memory usage.</p>
<p>In this case, also, we are going to consider all the settings we had considered in the previous sections. The following figures give us a sense of the memory footprint left behind by the models -</p>
<p><img src="https://i.ibb.co/TrnZ9vX/webp-net-resizeimage.png" class="img-fluid"></p>
<center>
<small>Memory footprint of different variants of the CRAFT model.</small><br>
</center>
<p><img src="https://i.ibb.co/3szkpK0/hfp-jmc4-nej-lloj-bc2-q-nz515y.png" class="img-fluid"></p>
<center>
<small>Memory footprint of different variants of the EAST model.</small><br>
</center>
<p>Detection performance-wise, CRAFT was a winner in many cases but if we factor in for inference latency and memory footprint the situation might need reconsideration. In other words, the best performing (with respect to a certain task, detection in this case) model may not always be the best candidate for deployments.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>The results for the float16 1280x800 CRAFT model could not be obtained on our target device.</p>
</div>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In this post, we presented a comparative study between two text detection models - CRAFT and EAST. We went beyond their task-specific performance and considered various essential factors that one needs to consider when deploying these models. At this point, you might have felt the need to consider another important factor of these models - <em>FPS information of the models on real-time videos</em>. Please check out <a href="https://github.com/farmaker47/OCR_with_Keras">this repository</a> to get a handle on how to approach that development.</p>
</section>
<section id="contribution" class="level2">
<h2 class="anchored" data-anchor-id="contribution">Contribution</h2>
<p><a href="https://www.linkedin.com/in/tulasi-ram-laghumavarapu-aba672103/">Tulasi</a> worked on the CRAFT model while Sayak worked on the EAST model. For the purpose of this post, Tulasi focused on gathering all the relevant information for doing the comparisons while Sayak focused on the writing part.</p>
<p>Thanks to <a href="https://twitter.com/khanhlvg">Khanh LeViet</a> from the TFLite team for reviewing the post.</p>


</section>

 ]]></description>
  <guid>https://sayak.dev/posts/optimizing-text-detectors.html</guid>
  <pubDate>Fri, 27 Nov 2020 00:00:00 GMT</pubDate>
  <media:content url="https://sayak.dev/posts/text_detector_benchmark.png" medium="image" type="image/png" height="131" width="144"/>
</item>
<item>
  <title>Optimizing MobileDet for Mobile Deployments</title>
  <link>https://sayak.dev/posts/mobiledet-optimization.html</link>
  <description><![CDATA[ 






<p>This year researchers from the University of Wisconsin-Madison and Google published their work on <a href="https://arxiv.org/abs/2004.14525">MobileDet</a>. MobileDet presents an architectural philosophy for designing object detectors specifically targeted toward running on mobile accelerators like DSP, EdgeTPU, and so on. MobileDet yields significant improvement over architectures MobileNetV2+SSDLite and MobileNetV3+SSDLite on the <a href="https://cocodataset.org/">COCO object detection task</a> with the same accelerated inference time. Long story cut short, if you are planning to use object detection models in mobile applications MobileDets may be an extremely good choice.</p>
<p>One fantastic thing about modern-day research is most of the time, the code and essential artifacts (like the trained models) are available publicly. MobileDet is no exception; the authors released their code and pre-trained models in <a href="https://github.com/tensorflow/models/tree/master/research/object_detection">TensorFlow Object Detection (TFOD) API</a>. The model files come in three different variants -</p>
<ul>
<li>Optimized for mobile CPU</li>
<li>Optimized for EdgeTPU</li>
<li>Optimized for DSP</li>
</ul>
<p>Each of these variants includes the pre-trained checkpoints, a TensorFlow Lite (TFLite) compatible model graph, a TFLite model file, a configuration file, and a graph proto. The models were pre-trained on the COCO dataset.</p>
<p>In this post, I am going to be revisiting the TFLite conversion from the pre-trained model checkpoints along with some of the non-trivial things that come up during the process. It is basically an extension of <a href="https://twitter.com/khanhlvg?lang=en">Khanh LeViet</a> and my findings we shared over <a href="https://github.com/ml-gde/e2e-tflite-tutorials/issues/21">this GitHub thread</a>.</p>
<p>The code discussed throughout this post is available <a href="https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/MobileDet_Conversion_TFLite.ipynb">here as a Colab Notebook</a>.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you want to train MobileDet models on your own dataset you may find <a href="https://github.com/sayakpaul/E2E-Object-Detection-in-TFLite/tree/master/colab_training">these notebooks</a> useful. They show you how to prepare the dataset, fine-tune a MobileDet model with the dataset, and optimize the fine-tuned model with TFLite.</p>
</div>
</div>
<section id="why-yet-another-post-on-model-conversion" class="level2">
<h2 class="anchored" data-anchor-id="why-yet-another-post-on-model-conversion">Why yet another post on model conversion?</h2>
<p>Fair question. After all, there are so many great examples and tutorials that show how to use the <a href="https://www.tensorflow.org/lite/performance/post_training_quantization"><u>post-training quantization APIs</u></a> in TFLite to perform the model conversion. MobileDet models in the TFOD API repository were trained in TensorFlow (TF) 1. If you ever wanted to use the latest TFLite converter to do the conversion, that may not be immediately approachable.</p>
<p>Besides, there are certain caveats to the EdgeTPU and DSP variants. They come in two precision formats - <code>uint8</code> and <code>float32</code>. The models in <code>uint8</code> precision were trained using <a href="https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html"><u>quantization aware training</u></a> (QAT) while the <code>float32</code> models were not. During QAT fake quantization nodes get inserted into a model’s computation graph. So, the models trained using QAT usually require some extra care during the TFLite conversion process as we’ll see in a moment.</p>
<p>If we wanted to convert a single shot detector (SSD) based model to TFLite then we first need to generate a frozen graph first that is compatible with the TFLite operator set (as per these guides - <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md"><u>TF1</u></a> and <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md"><u>TF2</u></a>). The TFOD API team provides stock scripts (<a href="https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_ssd_graph.py"><u>TF1</u></a>, <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_graph_tf2.py"><u>TF2</u></a>) for this. Both of these scripts add optimized postprocessing operations to the model graph. Now, these operations are not yet supported in int8 precision. So, if you ever wanted to convert these pre-trained checkpoints using <a href="https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization"><u>full integer quantization</u></a>, what would have been your approach?</p>
<p>By now, hopefully, I have been able to convince you that this post is not just about regular model conversion in TFLite. The situations we’ll be going through over the next sections may be helpful for your production TFLite models as well.</p>
</section>
<section id="the-hassle-free-conversions" class="level2">
<h2 class="anchored" data-anchor-id="the-hassle-free-conversions">The hassle-free conversions</h2>
<p>Before we build our way toward the fun stuff, let’s start with the conversions that won’t cost us a night’s sleep. Conversions based on <a href="https://www.tensorflow.org/lite/performance/post_training_quant"><u>dynamic-range</u></a> and <a href="https://www.tensorflow.org/lite/performance/post_training_float16_quant">float16</a> quantization would come under this category.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>The EdgeTPU and DSP variants of MobileDet are meant to run on the respective hardware accelerators. These accelerators need a model to be in full integer precision. So converting the EdgeTPU and DSP variants with dynamic-range and <code>float16</code> quantization does not have any practical usage.</p>
</div>
</div>
<p>So, for dynamic-range and <code>float16</code> quantization based conversions, we will be using the CPU variant only. This variant is available <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md#mobile-models">here</a> as <code>ssd_mobiledet_cpu_coco</code>. Once the model bundle is untar’d we get the following files -</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">├──</span> model.ckpt-400000.data-00000-of-00001</span>
<span id="cb1-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">├──</span> model.ckpt-400000.index</span>
<span id="cb1-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">├──</span> model.ckpt-400000.meta</span>
<span id="cb1-4"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">├──</span> model.tflite</span>
<span id="cb1-5"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">├──</span> pipeline.config</span>
<span id="cb1-6"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">├──</span> tflite_graph.pb</span>
<span id="cb1-7"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">└──</span> tflite_graph.pbtxt</span></code></pre></div></div>
<p><code>model.ckpt-*</code> files are the pre-trained checkpoints on the COCO dataset. If you train a MobileDet object detection model on your own dataset, you will have your own model checkpoint files. The <code>tflite_graph.pb</code> file is a frozen inference graph that is compatible with the TFLite operator set, which was exported from the pre-trained model checkpoints. <code>model.tflite</code> file is a TFLite model that was converted from the <code>tflite_graph.pb</code> frozen graph.</p>
<p>In case if you ever train a MobileDet model on your dataset, here’s how you’d get the TFLite frozen graph file (based on <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md"><u>this guide</u></a> mentioned above) -</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> PIPELINE_CONFIG=<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"checkpoint_name/pipeline.config"</span></span>
<span id="cb2-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> CKPT_PREFIX=<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"checkpoint_name/model.ckpt-400000"</span></span>
<span id="cb2-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> OUTPUT_DIR=<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"tflite_graph"</span></span>
<span id="cb2-4"> </span>
<span id="cb2-5"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> python models/research/object_detection/export_tflite_ssd_graph.py <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb2-6">   <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--pipeline_config_path</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$PIPELINE_CONFIG</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb2-7">   <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--trained_checkpoint_prefix</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$CKPT_PREFIX</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb2-8">   <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--output_directory</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$OUTPUT_DIR</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb2-9">   <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--add_postprocessing_op</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>true</span></code></pre></div></div>
<p>You can see a fully worked out example in the <a href="https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/MobileDet_Conversion_TFLite.ipynb"><u>Colab Notebook</u></a> mentioned above. If everything goes well, then you should have the frozen graph file exported in <code>OUTPUT_DIR</code>. Let’s now proceed to the TFLite model conversion part.</p>
<p>Here’s how the dynamic-range quantization would look like in TensorFlow 2 -</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">converter <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(</span>
<span id="cb3-2">    graph_def_file<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model_to_be_quantized,</span>
<span id="cb3-3">    input_arrays<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'normalized_input_image_tensor'</span>],              </span>
<span id="cb3-4">    output_arrays<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess'</span>,</span>
<span id="cb3-5">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess:1'</span>,</span>
<span id="cb3-6">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess:2'</span>,</span>
<span id="cb3-7">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess:3'</span>],</span>
<span id="cb3-8">   input_shapes<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'normalized_input_image_tensor'</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">320</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">320</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>]}</span>
<span id="cb3-9">)</span>
<span id="cb3-10">converter.optimizations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [tf.lite.Optimize.DEFAULT]</span>
<span id="cb3-11">tflite_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> converter.convert()</span></code></pre></div></div>
<p>A note about some of the parameters and their values from the above code listing -</p>
<ul>
<li><p><code>model_to_be_quantized</code> corresponds to the frozen graph file.</p></li>
<li><p><code>input_arrays</code> and <code>input_shapes</code> are set accordingly with respect to the frozen graph file. As we can see in the figure below that these values have been set correctly.</p>
<p><img src="https://i.ibb.co/F4xGRJB/image2.png" class="img-fluid"></p></li>
<li><p><code>output_arrays</code> is set according to the instructions provided in <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_ssd_graph.py"><u>this guide</u></a>. Those operations represent four arrays: <code>detection_boxes</code>, <code>detection_classes</code>, <code>detection_scores</code>, and <code>num_detections</code>, usually a mandate for any object detector out there.</p></li>
</ul>
<p>The rest of the parts in the code listing should be familiar to you if you already know about the typical post-training quantization process in TFLite. For <code>float16</code> quantization, all the things would remain the same; we just need to add this line before calling <code>convert()</code> - <code>converter.target_spec.supported_types = [tf.float16]</code>.</p>
<p>The dynamic-range quantized model is <strong>4.3 MB</strong> in size and <code>float16</code> one is <strong>8.2 MB</strong>. Later, we will see how fast this model would run on actual mobile devices with and without different accelerators.</p>
</section>
<section id="the-trickier-tflite-conversions-for-mobiledet" class="level2">
<h2 class="anchored" data-anchor-id="the-trickier-tflite-conversions-for-mobiledet">The trickier TFLite conversions for MobileDet</h2>
<p>In this section, we will be dealing with the full integer quantization for the three different variants of MobileDet. Full integer quantization is usually more involved than the other quantization formats supported by TFLite.</p>
<section id="representative-dataset" class="level3">
<h3 class="anchored" data-anchor-id="representative-dataset">Representative dataset</h3>
<p>Our first step toward doing full integer quantization is preparing a representative dataset. It is required to calibrate the activation ranges so that the quantized model is able to retain the original model performance as much as possible. For the purpose of this post, I sampled 100 images from the <a href="https://cocodataset.org/#download"><u>COCO training dataset</u></a> (<code>train2014</code> split). In my experience, 100 samples as the representative dataset have always been sufficient. I have hosted these images <a href="https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite/releases/tag/v0.9.0"><u>here</u></a> in case you are interested to use them.</p>
<p>The following code listing denotes a generator function that produces a preprocessed image to the TFLite converter -</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">rep_ds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.data.Dataset.list_files(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train_samples/*.jpg"</span>)</span>
<span id="cb4-2">HEIGHT, WIDTH <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">320</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">320</span></span>
<span id="cb4-3"> </span>
<span id="cb4-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> representative_dataset_gen():</span>
<span id="cb4-5">   <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> image_path <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> rep_ds:</span>
<span id="cb4-6">       img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.io.read_file(image_path)</span>
<span id="cb4-7">       img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.io.decode_image(img, channels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb4-8">       img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.convert_image_dtype(img, tf.float32)</span>
<span id="cb4-9">       resized_img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.resize(img, (HEIGHT, WIDTH))</span>
<span id="cb4-10">       resized_img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> resized_img[tf.newaxis, :]</span>
<span id="cb4-11">       <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">yield</span> [resized_img]</span></code></pre></div></div>
<p><strong>Note</strong> that these preprocessing steps should be in sync with the actual preprocessing steps that would apply before running inference with your TFLite model. In case if you are interested to know about more complex representative dataset generators you may find <a href="https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/Magenta_arbitrary_style_transfer_model_conversion.ipynb"><u>this notebook</u></a> useful.</p>
<p>Also, note that dynamic-range and <code>float16</code> quantization of the EdgeTPU and DSP variants don’t have much of practical usage. The next section is going to be solely about full integer quantization of these different variants and the nitty-gritty to take into consideration for the conversion process.</p>
</section>
<section id="dealing-with-fake-quantization-nodes-during-conversion" class="level3">
<h3 class="anchored" data-anchor-id="dealing-with-fake-quantization-nodes-during-conversion">Dealing with fake quantization nodes during conversion</h3>
<p>The figure below represents a portion of the <code>uint8</code> EdgeTPU model computation graph. The nodes highlighted in red are inserted by the QAT mechanism. You would notice the same kind of nodes in the <code>uint8</code> DSP model computation graph as well.</p>
<p><img src="https://i.ibb.co/B2qXzsf/image1.png" class="img-fluid"></p>
<p>Now, these nodes have some important implications that we need to consider during the conversion process -</p>
<ul>
<li>During QAT the activation ranges are already approximated i.e.&nbsp;QAT resembles post-training quantization during training and adjusts the activation ranges accordingly. So, we don’t need to provide a representative dataset for a full integer quantization based conversion.</li>
<li>These fake nodes are generally in integer precision. So, setting an optimization option (<code>converter.optimizations</code>) might lead to inconsistencies.</li>
<li>In order to convert the <code>uint8</code> models with full integer quantization, we need to set the input and output data type of the TFLite models to integer precision (typically <code>uint8</code> or <code>int8</code>). As per <a href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/lite/TFLiteConverter#attributes"><u>this documentation</u></a>, we also need to specify the <code>quantized_input_stats</code> parameter during the conversion process. This is needed in order for the converted TFLite model to map the quantized input values to real values. More details are available <a href="https://www.tensorflow.org/lite/performance/quantization_spec"><u>here</u></a>.</li>
</ul>
<p>So, how do we realize all of these in code?</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">converter <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(</span>
<span id="cb5-2">   graph_def_file<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model_to_be_quantized,</span>
<span id="cb5-3">   input_arrays<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'normalized_input_image_tensor'</span>],</span>
<span id="cb5-4">   output_arrays<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess'</span>,</span>
<span id="cb5-5">       <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess:1'</span>,</span>
<span id="cb5-6">       <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess:2'</span>,</span>
<span id="cb5-7">       <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TFLite_Detection_PostProcess:3'</span>],</span>
<span id="cb5-8">   input_shapes<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'normalized_input_image_tensor'</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">320</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">320</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>]}</span>
<span id="cb5-9">)</span>
<span id="cb5-10">converter.inference_input_type <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.uint8</span>
<span id="cb5-11">converter.quantized_input_stats <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"normalized_input_image_tensor"</span>: (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>)}</span>
<span id="cb5-12">tflite_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> converter.convert()</span></code></pre></div></div>
<p>If you’re thinking this does not look all that gory compared to the above code listing - it does not have to be! The tooling should help you do these things seamlessly. But catching these details during your project development may not be trivial. Note that we don’t specify <code>converter.inference_output_type</code>. Hold your breath, we will come to this in a moment.</p>
<p>After successful execution, we get two full integer quantized models - EdgeTPU one is <strong>4.2 MB</strong> and the DSP one is <strong>7.0 MB</strong>.</p>
</section>
<section id="integer-quantization-for-cpu-variants-and-float32-precision-models" class="level3">
<h3 class="anchored" data-anchor-id="integer-quantization-for-cpu-variants-and-float32-precision-models">Integer quantization for CPU variants and float32 precision models</h3>
<p>The variants that don’t contain fake quantization nodes (CPU and all the models in <code>float32</code> precision) have a <em>relatively</em> simpler conversion process. Recollect that the EdgeTPU and DSP variants come in two different precisions - <code>uint8</code> and <code>float32</code>. For example, here’s how it would be for the <code>float32</code> precision models -</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">converter.representative_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> representative_dataset_gen</span>
<span id="cb6-2">converter.inference_input_type <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.uint8</span>
<span id="cb6-3">converter.optimizations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [tf.lite.Optimize.DEFAULT]</span></code></pre></div></div>
<p>Note that we are specifying a representative dataset here because the <code>float32</code> precision models weren’t trained using QAT. For the CPU variant model, the lines of code would slightly change -</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">converter.inference_input_type <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.uint8</span>
<span id="cb7-2">converter.quantized_input_stats <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"normalized_input_image_tensor"</span>: (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>)}</span>
<span id="cb7-3">converter.optimizations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [tf.lite.Optimize.DEFAULT]</span></code></pre></div></div>
<p>Honestly, I found this configuration by trial and error. I observed that if I specify a representative dataset then it hurts the predictions of the converted model. Also, I found out that specifying <code>converter.quantized_input_stats</code> helped improve the predictions of the converted model.</p>
<p>We don’t specify <code>converter.inference_output_type</code> in this case as well. Let’s get to it now.</p>
</section>
<section id="dealing-with-non-integer-postprocessing-ops-during-conversion" class="level3">
<h3 class="anchored" data-anchor-id="dealing-with-non-integer-postprocessing-ops-during-conversion">Dealing with non-integer postprocessing ops during conversion</h3>
<p>Remember that frozen graph exporter scripts provided by the TFOD API team add optimized postprocessing operations to the graph. These operations are not supported in integer precision yet. So, even if you wanted to specify <code>converter.inference_output_type</code> as <code>tf.uint8</code> you’ll likely get the following error -</p>
<pre><code>RuntimeError: Unsupported output type UINT8 for output tensor 'TFLite_Detection_PostProcess' of type FLOAT32.</code></pre>
<p>This is why we did not set the <code>converter.inference_output_type</code> parameter.</p>
<p>This should resolve all the problems you may run into if you ever wanted to convert the MobileDet models offered by the TFOD API team. In the last two sections, we’ll see these converted models in action and how fast they can perform on respective hardware accelerators.</p>
</section>
</section>
<section id="show-me-some-results" class="level2">
<h2 class="anchored" data-anchor-id="show-me-some-results">Show me some results</h2>
<p>For the CPU variant model, its <code>float16</code> quantized TFLite provided decent results -</p>
<p><img src="https://i.ibb.co/k6c93CC/image3.png" class="img-fluid"></p>
<p>On Colab, the inference time is about <strong>92.36 ms</strong> for this particular model. I experimented with different threshold values for filtering out the weak predictions and a threshold of <strong>0.3</strong> yielded the best results. These results are pretty consistent across the several different models we talked about.</p>
<p>A major point to note here for the EdgeTPU and DSP variants, their converted counterparts would be much slower on Colab since they were specifically optimized for different hardware accelerators.</p>
<p>You are encouraged to play with the different converted models using the Colab Notebook mentioned above and see these results for yourself.</p>
</section>
<section id="model-benchmarks" class="level2">
<h2 class="anchored" data-anchor-id="model-benchmarks">Model benchmarks</h2>
<p>In this section, we’ll address the question - “So, how do I choose one among these many models?” Well, you could manually try them all out and see which performs the best on the runtime of your choice. But a more practical approach to this would be to first benchmark these models on a set of devices using the <a href="https://www.tensorflow.org/lite/performance/measurement"><u>TFLite Benchmark Tool</u></a> and then decide accordingly.</p>
<p>The following table provides a comprehensive summary of the important statistics about the runtime of different TFLite MobileDet models. These results were generated using the TFLite Benchmark Tool mentioned above.</p>
<img src="https://i.ibb.co/jrKshwB/image.png" class="img-fluid">
<center>
<small>* Device used - Pixel 4 (Inference timings are reported in milliseconds)</small><br> <small>** As reported <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md">here</a></small>
</center>
<p>We can see that with the proper hardware accelerators, the DSP EdgeTPU variants can really shine. For the CPU variant, on a GPU accelerated runtime the <code>float16</code> quantized TFLite model can bring in additional speed boosts.</p>
<p>A catch here is Pixel devices don’t allow third-party applications to use the Hexagon DSP therefore even if we instruct the Benchmark Tool to make use of that the model would fall back to the CPU for execution. This is why for fair benchmarking results for the DSP variants we should consider running the Benchmark Tool on a device (such as Samsung Galaxy S9+) that has Hexagon DSP and also allows third-party applications to use it.</p>
<img src="https://i.ibb.co/mHkyfpd/image.png" class="img-fluid">
<center>
<small>* Device used - Samsung Galaxy S9+ (Inference timings are reported in milliseconds)</small>
</center>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>To train a custom MobileDet-based object detector you can refer to <a href="https://github.com/sayakpaul/E2E-Object-Detection-in-TFLite/tree/master/colab_training">these notebooks</a>.</p>
</div>
</div>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>In this post, we discussed some of the intricate problems one may run into while converting different variants of the MobileDet model in TFLite. One aspect about TFLite that I really like is how it provides the tooling needed to deal with practical problems like this.</p>
<p>I am thankful to Khanh for thoroughly guiding me while writing this post. Thanks to <a href="https://sg.linkedin.com/in/martinandrews">Martin Andrews</a> for suggesting textual edits.</p>


</section>

 ]]></description>
  <category>tflite</category>
  <category>model-optimization</category>
  <category>mobiledet</category>
  <guid>https://sayak.dev/posts/mobiledet-optimization.html</guid>
  <pubDate>Tue, 29 Sep 2020 00:00:00 GMT</pubDate>
  <media:content url="https://sayak.dev/posts/benchmark.png" medium="image" type="image/png" height="62" width="144"/>
</item>
<item>
  <title>Different data augmentation recipes in tf.keras for image classification</title>
  <link>https://sayak.dev/posts/augmentation-recipes.html</link>
  <description><![CDATA[ 






<p>Data augmentation is a favorite recipe among deep learning practitioners especially for the ones working in the field of computer vision. Data augmentation is a technique used for introducing variety in training data thereby helping to mitigate overfitting.</p>
<p>When using Keras for training image classification models, using the <code>ImageDataGenerator</code> class for handling data augmentation is pretty much a standard choice. However, with TensorFlow, we get a number of different ways we can apply data augmentation to image datasets. In this tutorial, we are going to discuss three such ways. Knowing about these different ways of plugging in data augmentation in your image classification training pipelines will help you decide the best way for a given scenario.</p>
<p>Here’s a brief overview of the different ways we are going to cover:</p>
<ul>
<li>Using the standard ImageDataGenerator class</li>
<li>Using TensorFlow image ops with a TensorFlow dataset</li>
<li>Using Keras’s (experimental) image processing layers</li>
<li>Mix-matching different image ops &amp; image processing layers</li>
</ul>
<p>Let’s get started!</p>
<section id="experimental-setup" class="level1">
<h1>Experimental setup</h1>
<p>We are going to use the flowers dataset to demonstrate the experiments. Downloading the dataset is just as easy as executing the following line of code:</p>
<p><code>flowers</code> contains the path (which in my case is - <code>/root/.keras/datasets/flower_photos</code>) where the dataset got downloaded. The structure of the dataset looks like so -</p>
<pre><code>├── daisy [633 entries]
├── dandelion [898]
├── roses [641]
├── sunflowers [699 entries]
├── tulips [799 entries]
└── LICENSE.txt</code></pre>
<div id="cell-6" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Get the flowers dataset</span></span>
<span id="cb2-2">flowers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.utils.get_file(</span>
<span id="cb2-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'flower_photos'</span>,</span>
<span id="cb2-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz'</span>,</span>
<span id="cb2-5">    untar<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div></div>
</div>
<p>Using the standard ImageDataGenerator class For most of the scenarios, the ImageDataGenerator should be good enough. Its flexible API design is really to follow and it makes it easier to work with custom image datasets by providing meaningful high-level abstractions.</p>
<p>We instantiate the ImageDataGenerator class like so -</p>
<div id="cell-10" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">img_gen <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.preprocessing.image.ImageDataGenerator(</span>
<span id="cb3-2">    rescale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">255</span>,</span>
<span id="cb3-3">    rotation_range<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>,</span>
<span id="cb3-4">    horizontal_flip<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div></div>
</div>
<p>We specify two augmentation operations and a pixel rescaling operation in there. <code>ImageDataGenerator</code> comes with a handy <code>flow_from_directory</code> method that allows us to read images from a directory and apply the specified operations on the fly during the time of training. Here’s how to instruct the <code>img_gen</code> object to read images from a directory -</p>
<div id="cell-12" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="821f7d34-ab72-4cfc-9a52-3833df630437">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">IMG_SHAPE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">224</span></span>
<span id="cb4-2">BATCH_SIZE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span></span>
<span id="cb4-3"></span>
<span id="cb4-4">img_flow <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> img_gen.flow_from_directory(flowers, </span>
<span id="cb4-5">    shuffle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, </span>
<span id="cb4-6">    batch_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>BATCH_SIZE,</span>
<span id="cb4-7">    target_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(IMG_SHAPE, IMG_SHAPE))</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>Found 3670 images belonging to 5 classes.</code></pre>
</div>
</div>
<p>We then verify the images and the labels and they are indeed parsed right -</p>
<div id="cell-14" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:608}}" data-outputid="4a3e083a-6a12-49e4-8e29-88e79b0f86d1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">images, labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>(img_flow)</span>
<span id="cb6-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(images.shape, labels.shape)</span>
<span id="cb6-3">show_batch(images, labels)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>(32, 224, 224, 3) (32, 5)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://sayak.dev/posts/2020-05-10-augmemtation-recipes_files/figure-html/cell-9-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Training with an ImageDataGenerator instance is extremely straight-forward -</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_training_model()</span>
<span id="cb8-2">model.fit(img_flow, ...)</span></code></pre></div></div>
<p>For a fully worked out example, refer to <a href="https://www.pyimagesearch.com/2019/06/03/fine-tuning-with-keras-and-deep-learning/">this tutorial</a>.</p>
<p>As can be seen in <a href="https://www.tensorflow.org/tutorials/load_data/images#performance">this blog post</a>, <code>ImageDataGenerator</code>’s overall data loading performance can have a significant effect on how fast your model trains. To tackle situations, where you need to maximize the hardware utilization without burning unnecessary bucks, <a href="https://www.tensorflow.org/guide/data">TensorFlow’s data module</a> can be really helpful (comes at some costs).</p>
</section>
<section id="tensorflow-image-ops-with-tf.data-apis" class="level1">
<h1>TensorFlow image ops with tf.data APIs</h1>
<p>The blog post I mentioned in the previous section shows the kind of performance boost achievable with <code>tf.data</code> APIs. But it’s important to note that boost comes at the cost of writing boilerplate code which makes the overall process more involved. For example, here’s how you would load and preprocess your images and labels -</p>
<div id="cell-17" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> parse_images(image_path):</span>
<span id="cb9-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Load and preprocess the image</span></span>
<span id="cb9-3">    img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.io.read_file(image_path) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># read the raw image</span></span>
<span id="cb9-4">    img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.decode_jpeg(img, channels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># decode the image back to proper format</span></span>
<span id="cb9-5">    img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.convert_image_dtype(img, tf.float32) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># scale the pixel values to [0, 1] </span></span>
<span id="cb9-6">    img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.resize(img, [IMG_SHAPE, IMG_SHAPE]) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># resize the image</span></span>
<span id="cb9-7"></span>
<span id="cb9-8">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Parse the labels</span></span>
<span id="cb9-9">    label <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.strings.split(image_path, os.path.sep)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>]</span>
<span id="cb9-10"></span>
<span id="cb9-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (img, label)</span></code></pre></div></div>
</div>
<p>You would then write a separate augmentation policy with the <a href="https://www.tensorflow.org/api_docs/python/tf/image">TensorFlow Image ops</a> -</p>
<div id="cell-19" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> augment(image, label):</span>
<span id="cb10-2">    img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.rot90(image)</span>
<span id="cb10-3">    img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.flip_left_right(img)</span>
<span id="cb10-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (img, label)</span></code></pre></div></div>
</div>
<p>To chain the above two together you would first create an initial dataset consisting of only the image paths -</p>
<div id="cell-21" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">image_paths <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(paths.list_images(flowers))</span>
<span id="cb11-2">list_ds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.data.Dataset.from_tensor_slices((image_paths))</span></code></pre></div></div>
</div>
<p>Now, you would read, preprocess, shuffle, augment, and batch your dataset -</p>
<div id="cell-23" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">AUTO <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.data.experimental.AUTOTUNE</span>
<span id="cb12-2"></span>
<span id="cb12-3">train_ds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb12-4">    list_ds</span>
<span id="cb12-5">    .<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(parse_images, num_parallel_calls<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>AUTO)</span>
<span id="cb12-6">    .shuffle(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1024</span>)</span>
<span id="cb12-7">    .<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(augment, num_parallel_calls<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>AUTO) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># augmentation call</span></span>
<span id="cb12-8">    .batch(BATCH_SIZE)</span>
<span id="cb12-9">    .prefetch(AUTO)</span>
<span id="cb12-10">)</span></code></pre></div></div>
</div>
<p><code>num_parallel_calls</code> allows you to parallelize the mapping function and <code>tf.data.experimental.AUTOTUNE</code> lets TensorFlow decide the level of parallelism to use dynamically (how cool is that?). prefetch allows loading in the next batch of data well before your model finishes the current epoch of training. It is evident that this process is more involved than the previous one.</p>
<p>Verifying if we constructed the data input pipeline correctly is a vital step before you feed your data to the model -</p>
<div id="cell-25" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:608}}" data-outputid="de640bb6-85e6-4905-cce9-4ffa2cf1fa29">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">image_batch, label_batch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">iter</span>(train_ds))</span>
<span id="cb13-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(image_batch.shape, label_batch.shape)</span>
<span id="cb13-3">show_batch(image_batch.numpy(), label_batch.numpy(), image_data_gen<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>(32, 224, 224, 3) (32,)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://sayak.dev/posts/2020-05-10-augmemtation-recipes_files/figure-html/cell-14-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The “b”s appear before the class labels because TensorFlow parses the strings as byte-strings. Using train_ds with your model is also just about executing -</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_training_model()</span>
<span id="cb15-2">model.fit(train_ds, ...)</span></code></pre></div></div>
<p><a href="https://www.tensorflow.org/tutorials/images/data_augmentation">Here</a> you can find a fully worked out example. <a href="https://www.tensorflow.org/guide/data_performance">Here</a> you can know more about the different performance considerations when using tf.data. There are more image ops available with <strong>TensorFlow Addons</strong> which can found <a href="https://www.tensorflow.org/addons/tutorials/image_ops">here</a>.</p>
<p>Recently, Keras introduced <a href="https://keras.io/api/preprocessing/image/#image_dataset_from_directory"><code>image_dataset_from_directory</code></a> function (only available in <code>tf-nightly</code> at the time of writing this) which takes care of many of the boilerplate code we saw above and still yields pretty good performance. Here’s <a href="https://colab.research.google.com/drive/1umJnCp8tZ7UDTYSQsuWdKRhqbHts38AC">a tutorial</a> that shows how to use it.</p>
<p>Keras has also introduced <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing">a number of image processing layers</a> which can be very useful to build flexible augmentation pipelines using the Sequential API. In the next section, let’s see how.</p>
</section>
<section id="using-kerass-experimental-image-processing-layers" class="level1">
<h1>Using Keras’s (experimental) image processing layers</h1>
<p>Just like you would construct an entire model using the <code>Sequential</code> API, you can now construct very flexible data augmentation pipelines using the newly introduced (although experimental at the time of writing this) image processing layers. If we were to convert the data augmentation operations we have been following in the tutorial so far, building a data augmentation pipeline using this approach would be like so -</p>
<div id="cell-28" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">data_augmentation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.Sequential([</span>
<span id="cb16-2">    tf.keras.layers.experimental.preprocessing.RandomFlip(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'horizontal'</span>),</span>
<span id="cb16-3">    tf.keras.layers.experimental.preprocessing.RandomRotation(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>)</span>
<span id="cb16-4">])</span></code></pre></div></div>
</div>
<p>Before passing your data through this stack of layers <strong>makes sure you haven’t applied any augmentation already</strong>. So, it’s safe to create a separate TensorFlow dataset without mapping the augmentation function like we previously did -</p>
<div id="cell-30" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create TensorFlow dataset without any augmentation</span></span>
<span id="cb17-2">train_ds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb17-3">    list_ds</span>
<span id="cb17-4">    .<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(parse_images, num_parallel_calls<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>AUTO)</span>
<span id="cb17-5">    .shuffle(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1024</span>)</span>
<span id="cb17-6">    .batch(BATCH_SIZE)</span>
<span id="cb17-7">    .prefetch(AUTO)</span>
<span id="cb17-8">)</span></code></pre></div></div>
</div>
<p>Now, we can see how to examine some of the augmented images that would come out of this mini pipeline -</p>
<div id="cell-32" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:591}}" data-outputid="f8f269a9-8bd3-492d-a445-212fbc6897c0">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">image_batch, label_batch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">iter</span>(train_ds))</span>
<span id="cb18-2"></span>
<span id="cb18-3">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>))</span>
<span id="cb18-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> n <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>):</span>
<span id="cb18-5">    ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplot(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb18-6">    augmented_image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data_augmentation(tf.expand_dims(image_batch[n], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>))</span>
<span id="cb18-7">    plt.imshow(augmented_image[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].numpy())</span>
<span id="cb18-8">    plt.title(label_batch[n].numpy().decode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"utf-8"</span>))</span>
<span id="cb18-9">    plt.axis(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'off'</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://sayak.dev/posts/2020-05-10-augmemtation-recipes_files/figure-html/cell-17-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can also make use of Python <code>lambda</code>s to map <code>data_augmentation</code> directly to our <code>tf.data</code> pipeline like so:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">train_ds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb19-2">    list_ds</span>
<span id="cb19-3">    .<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(parse_images, num_parallel_calls<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>AUTO)</span>
<span id="cb19-4">    .shuffle(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1024</span>)</span>
<span id="cb19-5">    .batch(BATCH_SIZE)</span>
<span id="cb19-6">    .<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x, y: (data_augmentation(x), y),</span>
<span id="cb19-7">        num_parallel_calls<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>AUTO)</span>
<span id="cb19-8">    .prefetch(AUTO)</span>
<span id="cb19-9">)</span></code></pre></div></div>
<p>Note that these layers can be also added as a part of your model allowing them to run on GPUs. Based on your compute budget you should decide if you would want to run these layers on the GPU or you would rather have them executed separately on the CPU.</p>
<p>A functional model definition in Keras using this approach may look like so -</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># You define an input layer with pre-defined shapes</span></span>
<span id="cb20-2">inputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> keras.Input(shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(IMG_SHAPE, IMG_SHAPE, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span>
<span id="cb20-3"></span>
<span id="cb20-4">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data_augmentation(inputs)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Apply random data augmentation</span></span>
<span id="cb20-5">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> feature_extractor_model(x, training<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb20-6">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> GlobalAveragePooling2D()(x)</span>
<span id="cb20-7">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Dropout(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>)(x)  </span>
<span id="cb20-8">outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Dense(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)(x)</span>
<span id="cb20-9"></span>
<span id="cb20-10">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Model(inputs, outputs)</span></code></pre></div></div>
<p>Now, <code>model</code> should be good to go with - <code>model.fit(train_ds, ...)</code>. A fully worked out example is available <a href="https://colab.research.google.com/drive/17vHSAj7no7RMdJ18MJomTf8twqw1suYC#scrollTo=nhSR8l3OX_sM">here</a>. Note that, performance might get slightly affected when going with this approach since the GPUs will be utilized to run the preprocessing layers as well.</p>
<p>Let’s now think about situations where we may need to use a combination of the image ops of TensorFlow and the layers we just saw. What if we need to plug in custom augmentation operations in the augmentation pipeline? Added on top of it, what if we need to fix the probability at which the augmentation operations would get applied? Data augmentation pipelines are quite central behind the success of recent works like <a href="https://arxiv.org/abs/2002.05709">SimCLR</a>, <a href="https://arxiv.org/abs/1912.02781">Augmix</a>, etc.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>These layers have pre-defined inference-time behaviour. So even if you have included them inside your model itself, it’s totally fine. But if you want them during inference, you would need to set its inference-time behaviour.</p>
</div>
</div>
</section>
<section id="towards-more-complex-augmentation-pipelines" class="level1">
<h1>Towards more complex augmentation pipelines</h1>
<p>In this final approach, we will see how to mix and match between the different stock image ops, and stock image processing layers. Let’s first define a class utilizing the stock image ops with a utility function to apply them at random with a pre-defined probability.</p>
<div id="cell-37" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> CustomAugment(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">object</span>):</span>
<span id="cb21-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__call__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, image):        </span>
<span id="cb21-3">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Random flips and grayscale with some stochasticity</span></span>
<span id="cb21-4">        img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._random_apply(tf.image.flip_left_right, image, p<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>)</span>
<span id="cb21-5">        img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._random_apply(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._color_drop, img, p<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>)</span>
<span id="cb21-6">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> img</span>
<span id="cb21-7"></span>
<span id="cb21-8">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _color_drop(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x):</span>
<span id="cb21-9">        image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.image.rgb_to_grayscale(x)</span>
<span id="cb21-10">        image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.tile(x, [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>])</span>
<span id="cb21-11">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> x</span>
<span id="cb21-12">    </span>
<span id="cb21-13">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _random_apply(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, func, x, p):</span>
<span id="cb21-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> tf.cond(</span>
<span id="cb21-15">          tf.less(tf.random.uniform([], minval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, maxval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tf.float32),</span>
<span id="cb21-16">                  tf.cast(p, tf.float32)),</span>
<span id="cb21-17">          <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span>: func(x),</span>
<span id="cb21-18">          <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span>: x)</span></code></pre></div></div>
</div>
<p><code>_random_apply</code> is taken from the <a href="https://github.com/google-research/simclr">official SimCLR repository</a>. Now, in order to tie it together with the stock image processing layers, we can still use the <code>Sequential</code> API with a <code>Lambda</code> layer -</p>
<div id="cell-39" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Build the augmentation pipeline</span></span>
<span id="cb22-2">data_augmentation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.keras.Sequential([</span>
<span id="cb22-3">    tf.keras.layers.Lambda(CustomAugment()),</span>
<span id="cb22-4">    tf.keras.layers.experimental.preprocessing.RandomRotation(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>)</span>
<span id="cb22-5">])</span></code></pre></div></div>
</div>
<p>When we verify if it’s indeed correct, we get desired outputs -</p>
<div id="cell-41" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:591}}" data-outputid="0fafb233-22ff-4446-fbf1-919d77f34613">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">image_batch, label_batch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">iter</span>(train_ds))</span>
<span id="cb23-2"></span>
<span id="cb23-3">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>))</span>
<span id="cb23-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> n <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>):</span>
<span id="cb23-5">    ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplot(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb23-6">    augmented_image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data_augmentation(tf.expand_dims(image_batch[n], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>))</span>
<span id="cb23-7">    plt.imshow(augmented_image[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].numpy())</span>
<span id="cb23-8">    plt.title(label_batch[n].numpy().decode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"utf-8"</span>))</span>
<span id="cb23-9">    plt.axis(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'off'</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://sayak.dev/posts/2020-05-10-augmemtation-recipes_files/figure-html/cell-21-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Training models when using this approach remains the same as the previous one. Keep in mind that performance can get affected when using this approach.</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ul>
<li><a href="https://www.pyimagesearch.com/2019/06/03/fine-tuning-with-keras-and-deep-learning/">Fine-tuning with Keras and Deep Learning</a></li>
<li><a href="https://keras.io/guides/transfer_learning/">Transfer learning &amp; fine-tuning</a></li>
<li><a href="https://keras.io/examples/vision/image_classification_from_scratch/">Image classification from scratch</a></li>
<li><a href="https://www.tensorflow.org/tutorials/images/data_augmentation">Data augmentation</a></li>
</ul>


</section>

 ]]></description>
  <category>tf.keras</category>
  <category>data_augmentation</category>
  <category>image</category>
  <guid>https://sayak.dev/posts/augmentation-recipes.html</guid>
  <pubDate>Sun, 10 May 2020 00:00:00 GMT</pubDate>
  <media:content url="https://sayak.dev/posts/augmentation.png" medium="image" type="image/png" height="145" width="144"/>
</item>
</channel>
</rss>
